{"pages":[{"title":"about","text":"AboutE-mail: 469608976@qq.com","link":"/about/index.html"},{"title":"tags","text":"","link":"/tags/index.html"},{"title":"分类","text":"","link":"/categories/index.html"}],"posts":[{"title":"centos install git","text":"参考文章 https://blog.csdn.net/lqlqlq007/article/details/78983879 prepare12sudo yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel asciidocsudo yum install gcc perl-ExtUtils-MakeMaker 卸载系统自带的底版本git 123 git --versionsudo yum remove git install1cd /usr/local/src/ 1wget https://www.kernel.org/pub/software/scm/git/git-2.23.0.tar.xz 12tar -vxf git-2.23.0.tar.xzcd git-2.23.0 1sudo make prefix=/usr/local/git all 1sudo make prefix=/usr/local/git install 1sudo echo &quot;export PATH=$PATH:/usr/local/git/bin&quot; &gt;&gt; /etc/profile 1source /etc/profile 12 git --versiongit version 2.15.1 如果是非root用户使用git，则需要配置下该用户下的环境变量 123 echo &quot;export PATH=$PATH:/usr/local/git/bin&quot; &gt;&gt; ~/.bashrc source ~/.bashrcgit --version 生成SSH密钥1ssh-keygen -t rsa -C “469608976@qq.com” 添加密钥到GitHub 打开 Github，登录自己的账号后点击自己的头像-&gt;settings-&gt;SSH And GPG Keys-&gt;New SSH key 将本地 id_rsa.pub 中的内容粘贴到 Key 文本框中，随意输入一个 title(不要有中文)，点击 Add Key 即可 centos里测试验证1ssh git@github.com","link":"/2021/09/27/centos/centos-git/"},{"title":"centos install jenkins","text":"prepare: install JDKinstall1wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat/jenkins.repo 1rpm --import https://jenkins-ci.org/redhat/jenkins-ci.org.key 如果不能安装就到官网下载jenkis的rmp包，官网地址（http://pkg.jenkins-ci.org/redhat-stable/）一. 官方下载地址：https://jenkins.io/download/二. 镜像下载地址：http://mirrors.jenkins-ci.org/ 123 wget http://mirror.serverion.com/jenkins/redhat-stable/jenkins-2.235.1-1.1.noarch.rpmrpm -ivh jenkins-2.222.3-1.1.noarch.rpmyum install -y jenkins-2.222.3-1.1.noarch.rpm 1yum install -y jenkins 配置jenkis的端口 1vim /etc/sysconfig/jenkins 12找到修改端口号：JENKINS_PORT=&quot;8080&quot; 此端口不冲突可以不修改 modify mirror of update centerhttps://www.jianshu.com/p/fb1bff7a21a1 vim /var/lib/Jenkins/hudson.model.UpdateCenter.xml 12345678&lt;?xml version='1.1' encoding='UTF-8'?&gt;&lt;sites&gt; &lt;site&gt; &lt;id&gt;default&lt;/id&gt; &lt;url&gt;https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json&lt;/url&gt; &lt;/site&gt;&lt;/sites&gt; config jenkins jdkvim /etc/init.d/jenkins 1/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64/bin/java 启动jenkins 1systemctl start/stop/restart jenkins 开机启动 1systemctl enable jenkins.service su jenkins1su -s /bin/bash jenkins","link":"/2021/09/27/centos/centos-jinkins/"},{"title":"centos install jdk","text":"安装之前先查看一下有无系统自带jdk12345rpm -qa |grep javarpm -qa |grep jdkrpm -qa |grep gcj 如果有就使用批量卸载命令1rpm -qa | grep java | xargs rpm -e --nodeps 直接yum安装1.8.0版本openjdk1yum install java-1.8.0-openjdk* -y 查看版本1java -version 配置JAVA_HOMEA 定位JDK安装路径1. 终端输入：1which java 输出为： 1/usr/bin/java 2. 终端输入：1ls -lr /usr/bin/java 输出为： 1/usr/bin/java -&gt; 3. 终端输入1ls -lrt /etc/alternatives/java 输出： 1/etc/alternatives/java -&gt; /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.181-3.b13.el7_5.x86_64/jre/bin/java 至此，我们确定java的安装目录为： /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.181-3.b13.el7_5.x86_64 B 配置JAVA_HOME1. 打开配置环境变量的文件1vim /etc/profile 2. 添加以下配置：1234export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.181-3.b13.el7_5.x86_64export JRE_HOME=$JAVA_HOME/jreexport CLASSPATH=$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATHexport PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH 1:wq保存退出。 3. 让配置生效1source /etc/profile 4. 测试配置结果1echo $JAVA_HOME","link":"/2021/09/27/centos/centos-jdk/"},{"title":"centos install mysql","text":"https://blog.csdn.net/qq_36582604/article/details/80526287","link":"/2021/09/27/centos/centos-mysql/"},{"title":"centos install maven","text":"1、获取安装包并解压1234cd /usr/localwget https://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gztar -zxvf apache-maven-3.6.3-bin.tar.gz 2、配置环境变量，添加export1vim /etc/profile 12export MAVEN_HOME=/usr/local/mavenexport PATH=${MAVEN_HOME}/bin:${PATH} 1source /etc/profile 1mvn -v 12vim ~/.bash_profile JAVA_HOME 和 M2_HOME 都设置成自己文件的位置 1source ~/.bash_profile 3、添加阿里云镜像1vim $MAVEN_HOME/conf/settings.xm 添加以下镜像配置 123456789101112131415&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;central-repository&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;Central Repository&lt;/name&gt; &lt;url&gt;http://central.maven.org/maven2/&lt;/url&gt; &lt;/mirror&gt; &lt;/mirrors&gt;","link":"/2021/09/27/centos/centos-maven/"},{"title":"1000瓶酒找1毒酒","text":"某酒主人要宴请客人，他共有1000瓶酒，其中1瓶有毒。一旦喝了毒酒后，会在一天后发作，现在如果我们用小白鼠进行检测，问一天内最少需要多少只小白鼠才可以检测出哪瓶有毒？ 此题的常规思路是10只老鼠按从左到右的顺序一字排好，每桶酒也编上号1到1000，并把编号转换成二进制形式(也就是只有0和1的二进制，但是为了方便，每个二进制都写满10位，不够十位数的前面添0补满(比如1100110就写成0001100110)，数位和老鼠的位置一一对应，把酒给相应位置上是1的老鼠喝(每一桶都要喝)。最后按死掉的老鼠是哪几只，然后排成二进制，再转成十进制就是第几桶酒。比如:第70桶酒，70转换成二进制就是0001000110，那么就给第四、八、九只老鼠喝。如果最后死掉第三、七、八只老鼠，那么就是0010001100，转换成十进制就是140，即140桶酒有毒。理论上这10只老鼠可以检测1024桶酒。 并行二分法 简单理解（8瓶为例） 黄色的 取一点混在一起喂给 第一只小白鼠 蓝色的 取一点混在一起喂给 第二只小白鼠 红色的 取一点混在一起喂给 第三只小白鼠 1000瓶 python 1234567891011121314151617181920212223242526272829303132333435363738394041x=int(input('请输入哪瓶酒有毒:'))if x%2 in range(1,2): print(&quot;1dead&quot;)else: print(&quot;1alive&quot;)if x%4 in range(1,3): print(&quot;2dead&quot;)else: print(&quot;2alive&quot;)if x%8 in range(1,5): print(&quot;3dead&quot;)else: print(&quot;3alive&quot;)if x%16 in range(1,9): print(&quot;4dead&quot;)else: print(&quot;4alive&quot;)if x%32 in range(1,17): print(&quot;5dead&quot;)else: print(&quot;5alive&quot;)if x%64 in range(1,33): print(&quot;6dead&quot;)else: print(&quot;6alive&quot;)if x%128 in range(1,65): print(&quot;7dead&quot;)else: print(&quot;7alive&quot;)if x%250 in range(1,126): print(&quot;8dead&quot;)else: print(&quot;8alive&quot;)if x%500 in range(1,251): print(&quot;9dead&quot;)else: print(&quot;9alive&quot;)if x%1000 in range(1,501): print(&quot;10dead&quot;)else: print(&quot;10alive&quot;) 相关文章 8瓶酒一瓶有毒 参考文章 https://zhuanlan.zhihu.com/p/268350907","link":"/2021/11/01/algorithm/algorithm-1/"},{"title":"24小时之内，如何找到有毒的那瓶水？","text":"你带着手表被关在小房子里，找到有毒的瓶子，毒药十小时后发作，只有24消失时间 重要条件： 10 小时后,毒药 发作 每隔一小时 喂一瓶毒药发作时，十小时前喝的那瓶就是有毒的。 参考文章","link":"/2021/11/01/algorithm/algorithm-2/"},{"title":"centos install nginx","text":"prepare1sudo yum install -y gcc gcc-c++ 1sudo yum install -y gcc gcc-c++ pcre pcre-devel zlib zlib-devel openssl openssl-devel root 安装PCRE库123456cd /usr/local/wget http://jaist.dl.sourceforge.net/project/pcre/pcre/8.33/pcre-8.33.tar.gztar -zxvf pcre-8.33.tar.gzcd pcre-8.33./configuremake &amp;&amp; make install root 安装SSL库123456cd /usr/local/wget http://www.openssl.org/source/openssl-1.0.1j.tar.gztar -zxvf openssl-1.0.1j.tar.gzcd openssl-1.0.1j./configmake &amp;&amp; make install root 安装zlib库存123456 cd /usr/local/wget http://www.zlib.net/zlib-1.2.11.tar.gztar -zxvf zlib-1.2.11.tar.gz cd zlib-1.2.11./configuremake &amp;&amp; make install set up nginx123456cd /home/kubemkdir downloadscd downloadswget https://nginx.org/download/nginx-1.16.1.tar.gztar xvf nginx-1.16.1.tar.gz cd /home/kube/downloads/nginx-1.16.1 1./configure --prefix=/home/kube/apps/nginx --user=kube --group=kube --error-log-path=/home/kube/logs/nginx/error.log --http-log-path=/home/kube/logs/nginx/access.log --with-http_ssl_module --with-openssl=/usr/local/openssl-1.0.1j --with-pcre=/usr/local/pcre-8.33 --with-zlib=/usr/local/zlib-1.2.11 --with-http_stub_status_module 12makemake install 1/home/kube/apps/nginx/sbin/nginx -V 3. login root1234567cd ~cd /home/kube/apps/nginx/sbinsudo chown root nginxsudo chmod +s nginxcd ~ kubesudo rm -rf nginx-1.16.1sudo rm nginx-1.16.1.tar.gz nginx.conf1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465user root;worker_processes 2;#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid;events { worker_connections 1024;}http { include mime.types; default_type application/octet-stream; #log_format main '$remote_addr - $remote_user [$time_local] &quot;$request&quot; ' # '$status $body_bytes_sent &quot;$http_referer&quot; ' # '&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;'; #access_log logs/access.log main; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; gzip off; # gzip_min_length 1k # gzip_comp_level 4 # gzip_types text/plain text/css application/json application/javascript server { listen 80; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / { proxy_pass http://localhost:3000; } error_page 404 /404.html; location = /404.html { root html; } # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } }} commands for the Nginx process123./nginx # Start./nginx -s reload ## Reload configure./nginx -s stop ## End","link":"/2021/09/27/centos/centos-nginx/"},{"title":"二分查找","text":"给定一个 n 个元素有序的（升序）整型数组 nums 和一个目标值 target ，写一个函数搜索 nums 中的 target，如果目标值存在返回下标，否则返回 -1。 递归123456789101112131415161718192021class Solution { public int search(int[] nums, int target) { return searchTarget(nums, 0, nums.length - 1, target); } public static int searchTarget(int[] nums, int from, int to, int target) { int mid = (to + from) / 2; if (target &lt; nums[from] || target &gt; nums[to] || to &lt; from) { return -1; } if (nums[mid] == target) { return mid; } if (nums[mid] &lt; target) { return searchTarget(nums, mid + 1, to, target); } else { return searchTarget(nums, from, mid - 1, target); } }} while循环123456789101112131415161718class Solution { public int search(int[] nums, int target) { int low = 0, high = nums.length - 1; while (low &lt;= high) { int mid = (high - low) / 2 + low; int num = nums[mid]; if (num == target) { return mid; } else if (num &gt; target) { high = mid - 1; } else { low = mid + 1; } } return -1; }} 相关题目你是产品经理，目前正在带领一个团队开发新的产品。不幸的是，你的产品的最新版本没有通过质量检测。由于每个版本都是基于之前的版本开发的，所以错误的版本之后的所有版本都是错的。 假设你有 n 个版本 [1, 2, …, n]，你想找出导致之后所有版本出错的第一个错误的版本。 你可以通过调用 bool isBadVersion(version) 接口来判断版本号 version 是否在单元测试中出错。实现一个函数来查找第一个错误的版本。你应该尽量减少对调用 API 的次数。 示例 1： 输入：n = 5, bad = 4输出：4解释：调用 isBadVersion(3) -&gt; false调用 isBadVersion(5) -&gt; true调用 isBadVersion(4) -&gt; true所以，4 是第一个错误的版本。 示例 2： 输入：n = 1, bad = 1输出：1 123456789101112131415161718192021/* The isBadVersion API is defined in the parent class VersionControl. boolean isBadVersion(int version); */public class Solution extends VersionControl { public int firstBadVersion(int n) { int left = 1, right = n; while (left &lt; right) { // 循环直至区间左右端点相同 int mid = left + (right - left) / 2; // 防止计算时溢出 if (isBadVersion(mid)) { right = mid; // 答案在区间 [left, mid] 中 } else { left = mid + 1; // 答案在区间 [mid+1, right] 中 } } // 此时有 left == right，区间缩为一个点，即为答案 return left; } } 相关题目ref: https://leetcode-cn.com/problems/search-insert-position/给定一个排序数组和一个目标值，在数组中找到目标值，并返回其索引。如果目标值不存在于数组中，返回它将会被按顺序插入的位置。 请必须使用时间复杂度为 O(log n) 的算法。 示例 1: 输入: nums = [1,3,5,6], target = 5输出: 2 示例 2: 输入: nums = [1,3,5,6], target = 2输出: 1 示例 3: 输入: nums = [1,3,5,6], target = 7输出: 4 示例 4: 输入: nums = [1,3,5,6], target = 0输出: 0 示例 5: 输入: nums = [1], target = 0输出: 0 1234567891011121314151617class Solution { public int searchInsert(int[] nums, int target) {int low = 0, high = nums.length - 1; while (low &lt;= high) { int mid = (high - low) / 2 + low; int num = nums[mid]; if (num == target) { return mid; } else if (num &gt; target) { high = mid - 1; } else { low = mid + 1; } } return nums[Math.min(low, nums.length - 1)] &lt; target ? Math.min(low, nums.length - 1) + 1 : low; }} 参考文章 https://leetcode-cn.com/problems/binary-search/","link":"/2021/11/04/algorithm/algorithm-3/"},{"title":"有序数组的平方","text":"给你一个按 非递减顺序 排序的整数数组 nums，返回 每个数字的平方 组成的新数组，要求也按 非递减顺序 排序。 示例 1：1234输入：nums = [-4,-1,0,3,10]输出：[0,1,9,16,100]解释：平方后，数组变为 [16,1,0,9,100]排序后，数组变为 [0,1,9,16,100] 示例 2：12输入：nums = [-7,-3,2,3,11]输出：[4,9,9,49,121] 暴力排序最直观的相反，莫过于：每个数平方之后，排个序 12345678910class Solution {public: vector&lt;int&gt; sortedSquares(vector&lt;int&gt;&amp; A) { for (int i = 0; i &lt; A.size(); i++) { A[i] *= A[i]; } sort(A.begin(), A.end()); // 快速排序 return A; }}; 这个时间复杂度是 O(n + nlogn)， 可以说是O(nlogn)的时间复杂度，但为了和下面双指针法算法时间复杂度有鲜明对比，我记为 O(n + nlogn)。 双指针法数组其实是有序的， 只不过负数平方之后可能成为最大数了。 那么数组平方的最大值就在数组的两端，不是最左边就是最右边，不可能是中间。 此时可以考虑双指针法了，i指向起始位置，j指向终止位置。 定义一个新数组result，和A数组一样的大小，让k指向result数组终止位置。 如果A[i] * A[i] &lt; A[j] * A[j] 那么result[k–] = A[j] * A[j]; 。 如果A[i] * A[i] &gt;= A[j] * A[j] 那么result[k–] = A[i] * A[i]; 。 如动画所示： ref 12345678910111213141516171819202122232425262728293031323334class Solution { public int[] sortedSquares(int[] nums) { int right = nums.length - 1; int left = 0; int[] result = new int[nums.length]; int index = result.length - 1; while (left &lt;= right) { if (nums[left] * nums[left] &gt; nums[right] * nums[right]) { result[index--] = nums[left] * nums[left]; ++left; } else { result[index--] = nums[right] * nums[right]; --right; } } return result; }}class Solution { public int[] sortedSquares(int[] nums) { int l = 0; int r = nums.length - 1; int[] res = new int[nums.length]; int j = nums.length - 1; while(l &lt;= r){ if(nums[l] * nums[l] &gt; nums[r] * nums[r]){ res[j--] = nums[l] * nums[l++]; }else{ res[j--] = nums[r] * nums[r--]; } } return res; }} 12345678910111213141516class Solution { public int[] sortedSquares(int[] nums) { int start = 0, end = nums.length - 1; int[] result = new int[nums.length]; for (int i = nums.length - 1; i &gt;= 0; i--) { if (Math.pow(nums[start], 2) &gt; Math.pow(nums[end], 2) ) { result[i] = (int)Math.pow(nums[start], 2) ; start++; } else { result[i] = (int)Math.pow(nums[end], 2) ; end--; } } return result; }} 参考文章 https://leetcode-cn.com/problems/squares-of-a-sorted-array/submissions/","link":"/2021/11/05/algorithm/algorithm-4/"},{"title":"骆驼运输香蕉问题","text":"总共有3000只香蕉，有一只骆驼每一次只能带1000只香蕉，每1公里吃1只香蕉，没有香蕉吃它是不肯走的，A-B 点距离1000公里， 如果这个骆驼要从A点到B点有什么办法可以让更多的香蕉剩下来？如何做到？如何最有效率的运最多的香蕉到B点？ 思路如果直接运送 1000只 香蕉， 从 A 到 B 点， 到达后， 剩余香蕉为 ０.设置一个中转站， 从Ａ到中转站需要消耗５００个香蕉，再回到A点又消耗了５００个， －－＞ 至少需要２个中转站 中转站 把香蕉放到中转站，再回去取香蕉 中转站越少越好 （每多设置一个中转站，消耗越多，骆驼吃的越多） 中转站至少两个 设置两个中转站， 每次从A 到中转站 M， 运送1000只， 需要走3次， 返回A点需要2 次 ，一共走五次。此时M中转站，应该有 2000 只香蕉。为什么？ A 到 M 的距离未知，是变化的， 设为 x. 为了让骆驼物尽其用，每一次都需要运送1000 只香蕉。 M中转站 可能有 1000 或 2000 香蕉 （损耗多少是不知道的） 假设M 有 1000 只 香蕉， 直接从M到B点就可以了。 但是此时只有一个中转站，已分析过，一个中转站是不行的。 所以，M 剩余 2000 是最划算的。 从M 到 N , 需要2次， 返回 1 次, 总共 3次.中转站N,剩余1000 香蕉是最划算的， 可一次从N 到 B. 计算x + y + z = 10005x=1000 –&gt; x=2003y=100 —&gt; x=333.3剩余距离：z=1000-x-y=467 转载https://blog.csdn.net/guanliangliang/article/details/6534761分析这个问题，我们先从初始情况开始，假设走X公里后停下，将剩余香蕉运输过来。 则可以得出剩余香蕉数量为 3000 – 5X，为什么是5呢，因为骆驼往返，总共需要走5次。 同时，由于骆驼一次运输1000只香蕉，所以这个5，只有在剩余香蕉超过2000只的时候才成立，那很容易得出一个不等式，就是3000-5X&lt;2000，求出X=200,。 也就是说，骆驼拖着香蕉，走了200公里，还剩2000只。 等到只剩2000只的时候，骆驼只要一个来回就可以把香蕉全部拖走，所以，走X公里后，剩余香蕉数量为 2000 – 3X，2000-3X &gt; 1000 得出 X = 333 还剩1000只的时候，骆驼一直拖着就行了，目前已经走了200+333=533公里，剩余距离为1000-533=447，所需再消耗447只香蕉，就能到达目的地，剩余香蕉数量为553. 参考文章 https://blog.csdn.net/guanliangliang/article/details/6534761","link":"/2021/10/29/algorithm/algorithm-camel-banana/"},{"title":"有一栋100层高楼,从某一层开始扔下的玻璃球刚好摔坏,现有两个玻璃球,试用最简便的方法确定这个恰好摔坏玻璃球的那层.","text":"二分法第一颗玻璃球： 从50层开始尝试， 75 -&gt; 87 -&gt; 93 -&gt; 96 -&gt; 98 -&gt; 99 -&gt; 100第二颗： 若在50层碎了，需要从第一层开始一层一层的尝试 粗调，细调第一颗玻璃球： 从10层开始尝试， 20 , 30 ,40, 50, 60, 70, 80, 90.第二颗： 第一层开始一层一层的尝试 最优算法：第一颗玻璃球： 14 27 39 50 60 69 77 84 90 95 99 100第二颗： 第一层开始一层一层的尝试 A:这个形象一点就像显微镜的两个调焦螺旋，一个粗调确定大范围，一个微调确定具体楼层。 B：不管什么情况，最终粗调微调的次数总和不变。 我们用①号球确定范围，②号球确定楼层。 (一)假设第一次范围为k层且①号碎了，那么①号球1次，②号球k-1次，合计1+k-1=k次。(二)假设①号球第二次才碎，那么①号球2次，为保证总次数不变②号球要丢k-2次，相当于这个范围有k-1层，情况以此类推。最糟糕情况就是要把100层都覆盖，那么所有次的范围之和加上最后的微调次数要≥98(第一层没高度，无意义，第100层也不用丢，所以是100-2)k+(k-1)+(k-2)+(k-3)+……+3+2+1≥98等差数列求和得k*(k+1)/2≥98，解得k=14，也就是最多14次，也就是第一个范围内有14层。 注意:k在这里即是总次数，也是第一次的层数。①号球具体丢的层数就是15层，28层，40层…… 参考文章 https://www.zhihu.com/question/31855632","link":"/2021/11/15/algorithm/alogrithm-5/"},{"title":"轮转数组","text":"给你一个数组，将数组中的元素向右轮转 k 个位置，其中 k 是非负数。 12345678910111213141516示例 1:输入: nums = [1,2,3,4,5,6,7], k = 3输出: [5,6,7,1,2,3,4]解释:向右轮转 1 步: [7,1,2,3,4,5,6]向右轮转 2 步: [6,7,1,2,3,4,5]向右轮转 3 步: [5,6,7,1,2,3,4]示例 2:输入：nums = [-1,-100,3,99], k = 2输出：[3,99,-1,-100]解释: 向右轮转 1 步: [99,-1,-100,3]向右轮转 2 步: [3,99,-1,-100] 方法一：使用额外的数组我们可以使用额外的数组来将每个元素放至正确的位置。用 nn 表示数组的长度，我们遍历原数组，将原数组下标为 ii 的元素放至新数组下标为 (i+k)\\bmod n(i+k)modn 的位置，最后将新数组拷贝至原数组即可。 1234567891011class Solution { public void rotate(int[] nums, int k) { int n = nums.length; int[] newArr = new int[n]; for (int i = 0; i &lt; n; ++i) { newArr[(i + k) % n] = nums[i]; } System.arraycopy(newArr, 0, nums, 0, n); }} 复杂度分析 时间复杂度： O(n)O(n)，其中 nn 为数组的长度。空间复杂度： O(n)O(n)。 方法二：环状替换方法一中使用额外数组的原因在于如果我们直接将每个数字放至它最后的位置，这样被放置位置的元素会被覆盖从而丢失。因此，从另一个角度，我们可以将被替换的元素保存在变量 \\textit{temp}temp 中，从而避免了额外数组的开销。 我们从位置 00 开始，最初令 \\textit{temp}=\\textit{nums}[0]temp=nums[0]。根据规则，位置 00 的元素会放至 (0+k)\\bmod n(0+k)modn 的位置，令 x=(0+k)\\bmod nx=(0+k)modn，此时交换 \\textit{temp}temp 和 \\textit{nums}[x]nums[x]，完成位置 xx 的更新。然后，我们考察位置 xx，并交换 \\textit{temp}temp 和 \\textit{nums}[(x+k)\\bmod n]nums[(x+k)modn]，从而完成下一个位置的更新。不断进行上述过程，直至回到初始位置 00。 容易发现，当回到初始位置 00 时，有些数字可能还没有遍历到，此时我们应该从下一个数字开始重复的过程，可是这个时候怎么才算遍历结束呢？我们不妨先考虑这样一个问题：从 00 开始不断遍历，最终回到起点 00 的过程中，我们遍历了多少个元素？ 由于最终回到了起点，故该过程恰好走了整数数量的圈，不妨设为 aa 圈；再设该过程总共遍历了 bb 个元素。因此，我们有 an=bkan=bk，即 anan 一定为 n,kn,k 的公倍数。又因为我们在第一次回到起点时就结束，因此 aa 要尽可能小，故 anan 就是 n,kn,k 的最小公倍数 \\text{lcm}(n,k)lcm(n,k)，因此 bb 就为 \\text{lcm}(n,k)/klcm(n,k)/k。 这说明单次遍历会访问到 \\text{lcm}(n,k)/klcm(n,k)/k 个元素。为了访问到所有的元素，我们需要进行遍历的次数为其中 \\text{gcd}gcd 指的是最大公约数。 我们用下面的例子更具体地说明这个过程： 12nums = [1, 2, 3, 4, 5, 6]k = 2 如果读者对上面的数学推导的理解有一定困难，也可以使用另外一种方式完成代码：使用单独的变量 \\textit{count}count 跟踪当前已经访问的元素数量，当 \\textit{count}=ncount=n 时，结束遍历过程。 12345678910111213141516171819202122class Solution { public void rotate(int[] nums, int k) { int n = nums.length; k = k % n; int count = gcd(k, n); for (int start = 0; start &lt; count; ++start) { int current = start; int prev = nums[start]; do { int next = (current + k) % n; int temp = nums[next]; nums[next] = prev; prev = temp; current = next; } while (start != current); } } public int gcd(int x, int y) { return y &gt; 0 ? gcd(y, x % y) : x; }} 复杂度分析 时间复杂度：O(n)O(n)，其中 nn 为数组的长度。每个元素只会被遍历一次。空间复杂度：O(1)O(1)。我们只需常数空间存放若干变量。 方法三：数组翻转该方法基于如下的事实：当我们将数组的元素向右移动 kk 次后，尾部 k\\bmod nkmodn 个元素会移动至数组头部，其余元素向后移动 k\\bmod nkmodn 个位置。 该方法为数组的翻转：我们可以先将所有元素翻转，这样尾部的 k\\bmod nkmodn 个元素就被移至数组头部，然后我们再翻转 [0, k\\bmod n-1][0,kmodn−1] 区间的元素和 [k\\bmod n, n-1][kmodn,n−1] 区间的元素即能得到最后的答案。 我们以 n=7n=7，k=3k=3 为例进行如下展示： 123456789101112131415161718class Solution { public void rotate(int[] nums, int k) { k %= nums.length; reverse(nums, 0, nums.length - 1); reverse(nums, 0, k - 1); reverse(nums, k, nums.length - 1); } public void reverse(int[] nums, int start, int end) { while (start &lt; end) { int temp = nums[start]; nums[start] = nums[end]; nums[end] = temp; start += 1; end -= 1; } }} 复杂度分析 时间复杂度：O(n)O(n)，其中 nn 为数组的长度。每个元素被翻转两次，一共 nn 个元素，因此总时间复杂度为 O(2n)=O(n)O(2n)=O(n)。空间复杂度：O(1)O(1)。 动画演示 ## 参考文章","link":"/2021/11/10/algorithm/algorithm-reverse-array/"},{"title":"16辆摩托车，每辆最多可以跑100km，如果他们可以相互配合，朝同一个方向直线行驶，那么一起最多可以跑多远？","text":"1 辆车 100 km. 2 辆车假设只有两辆车， 最多行驶 100 * 1/2 + 100 = 150 km (两辆车同时出发，行驶50 km后， 将一辆车的油加到另一辆，则这辆车可以在行使100 km) 3 辆车假设有a,b,c三辆车， 行驶 1/3 * 100后， 三辆车都剩下 2/3 的油， 将C的油加到a,b, 则 a,b 满状态，重新出发（问题转化为两辆车）。总路程为： 100 * 1/3 + 100 * 1/2 + 100 4 辆车a, b, c, d 4辆车， 行驶 1/4 后， d 的油加到 a,b,c, 转化为 3辆车。总路程为：100 * 1/4 + 100 * 1/3 + 100 * 1/2 + 100 16 辆车总路程为：100 * （1/16 + 1/15 +…+ 1/3 + 1/2 + 1 ） 转载 https://www.zhihu.com/question/494793411/answer/2192292359这应该是一个程序算法题，按照算法去写逻辑列式子比较费劲。我是这样思考的，一辆车可以用绳子牵引着其余车子行驶，直到这辆车的燃油耗尽，接着是下一辆继续按照这个模式行驶。 这样就出现了，可以每次都用1辆车去牵引剩余的车，也可以同时2辆、3辆。。。去牵引剩余的车，但是只要出现有2辆以上车在同一段路上同时耗费燃油行驶，那必然有一段路程是重复行走的，也就是说有燃油浪费在了“不必要”的路程上，那么从能量转化的角度，路程必然不是最大化的。 所以答案就只能是每次都是1辆车去牵引剩余的车。最多可以行驶：（1/16+1/15+…+1/3+1/2+1）*100=338.07km 参考文章 https://www.zhihu.com/question/494793411/answer/2192292359","link":"/2021/10/29/algorithm/algorithm-motorcycles/"},{"title":"docker_cmd","text":"docker 基本命令1234567891011121314# -a :提交的镜像作者；# -c :使用Dockerfile指令来创建镜像；# -m :提交时的说明文字；# -p :在commit时，将容器暂停。docker commit -a &quot;author&quot; -m &quot;commit msg&quot; {commitId} {name}:{tag}#docker commit -a &quot;author&quot; -m &quot;commit msg&quot; {commitId} author/myubuntu:v1docker logindocker push author/myubuntu:v1docker tag {imageId} {name}:{tag}docker run --name nginx-text -p 9090:90 -d nginx","link":"/2021/09/27/docker/docker-cmd/"},{"title":"burp suit 简单使用","text":"拦截，修改request启动burpsuite–切换到“Proxy”选项卡–选择“Options”菜单–往下看到“Intercept Client Requests”节区在该节区中我们可以看到这个位置可以配置拦截条件，我们以只拦截“www.baidu.com”为例 点击“Add”–布尔运算选择“And”–匹配类型选择“Domain name”–匹配关系选择“Matches”–匹配条件输入“www.baidu.com”--点击“OK” 切换回“Intercept”，此时“Intercept is on”但只会拦截百度的数据包，其他网站的数据包都直接放行了其实在“Intercept Client Requests”节区还可以配置各式各样的过滤条件自己随意发挥。 修改参数后继续请求API 设置 proxy -&gt; HTTP history response:如果要配置Respone的过滤，要到再下边一点的“Intercept Server Respones”节区进行配置。 参考文章 https://blog.csdn.net/weixin_34267123/article/details/86130311","link":"/2021/10/29/fiddler/burp-suit/"},{"title":"hexo icarus 配置 Gitalk 评论系统","text":"登录GitHub并点此注册一个新的OAuth应用 第一个 Application name 是应用名称 第二个 Homepage URL 是主页地址 第三个 是描述 第四个 是回调地址！！这个最重要！！写你博客的地址就可以了 打开对应hexo主题的_config.yml添加如下内容：12345678gitalk:enable: truegithubID: github帐号 # 例：asdfv1929repo: 仓库名称 # 例：blogClientID: Client IDClientSecret: Client SecretadminUser: github帐号 #指定可初始化评论账户distractionFreeMode: true 12345678910111213141516comment: type: gitalk client_id: xxxxxxxxxxxxxxxxxxxx client_secret: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx repo: Some-of-Your-GitHub-Repo owner: you_github_name admin: - you_github_name per_page: 20 # 可选填 distraction_free_mode: false # 可选填 pager_direction: last # 可选填 create_issue_manually: false # 可选填 proxy: # 可选填 flip_move_options: # 可选填 enable_hotkey: true # 可选填 ref: https://blog.csdn.net/qq_36537546/article/details/90730412 https://www.jianshu.com/p/09c1bced3a1f https://blog.csdn.net/qq_23452385/article/details/105936095","link":"/2021/10/14/hexo/hexo-comment/"},{"title":"fiddler简单使用","text":"Fiddler 是位于客户端和服务器端的 HTTP 代理，也是目前最常用的 HTTP 抓包工具之一。（Mac OS 建议采用 Charles） Fiddler 的基本原理 过滤拦截 的 url 修改request1.Fiddler想要抓到数据包，要确保Capture Traffic是开启，在File –&gt; Capture Traffic。开启后再左下角会有显示，当然也可以直接点击左下角的图标来关闭/开启抓包功能。 2. Rules -&gt; Automatic Breakpoints -&gt; Before Requests 3. 修改 request 参考文章 https://zhuanlan.zhihu.com/p/47003094 https://www.cnblogs.com/kristin/p/8445055.html https://www.cnblogs.com/yyhh/p/5140852.html","link":"/2021/10/29/fiddler/fiddler/"},{"title":"git常用命令","text":"1. 常用命令git tag12345678910111213141516# create taggit tag test_tag# delete taggit tag -d test_taggit push origin :refs/tags/test_tag# push taggit push origin test_taggit push origin --tags# show taggit show test_tag# 基于某个commit id create taggit tag -a test_tag {commitId} git 版本回退123456789101112131415161718# git log 命令可以显示所有提交过的版本信息# --pretty=oneline，只会显示版本号和提交时的备注信息git log --pretty=oneline# git reflog 可以查看所有分支的所有操作记录（包括已经被删除的 commit 记录和 reset 的操作）git reflog # git reset –-soft：回退到某个版本，只回退了commit的信息，不会恢复到index file一级。如果还要提交，直接commit即可；# 撤销该commit，但是又不能撤销该提交包含的更改，使用git reset --soft# 可见commit取消了，代码更改并没有取消git reset --soft {commitId}# git reset -–hard：彻底回退到某个版本，本地的源码也会变为上一个版本的内容，撤销的commit中所包含的更改被冲掉；git reset --hard {commitId}git push origin HEAD --force# 用某个commit 创建一个分支git branch {branch_name} {commitId} 2. git 安装配置1、检查git是否已经安装，输入git version命令即可，如果没有显示版本号表示没有安装git2、安装git ubuntusudo apt-get install git 3、配置git全局环境12git config --global user.name &quot;用户名&quot;git config --global user.email &quot;邮箱地址&quot; 4、生成ssh密钥ssh-keygen -t rsa -C &quot;这里换上你的邮箱&quot;会在用户目录~/.ssh/下建立相应的密钥文件。 5、创建完公钥后，需要上传。使用命令cd ~/.ssh进入~/.ssh文件夹，输入cat id_rsa.pub打开id_rsa.pub文件，复制其中所有内容。接着访问git网页，点击SSH公钥，标题栏可以随意输入，公钥栏把刚才复制的内容粘贴进去。 创建一个空的目录，初始化git仓库，添加远程仓库做测试 6、测试连接ssh -T git@github.com 7、git使用命令123456789git clone 项目地址 拉项目git pull 拉代码git push 提交到仓库git init指令初始化一个git仓库git add .添加文件git commit -m &quot;注释&quot;提交至仓库。git remote add origin https://git.oschina.net/你的用户名/项目名.git，git push origin master即可完成推送git checkout master 切换到master分支","link":"/2021/09/27/git/git-cmd/"},{"title":"hexo_guide","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start1hexo clean &amp;&amp; hexo g &amp;&amp; hexo server Create a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo clean &amp;&amp; hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2021/09/23/hexo/hexo-guide/"},{"title":"install flink &amp; IDEA Hello world","text":"1. install flink** 下载和解压 ** 从下载页下载一个二进制的包，你可以选择任何你喜欢的Hadoop/Scala组合包。如果你计划使用文件系统，那么可以使用任何Hadoop版本。进入下载目录解压下载的压缩包 123$ cd ~/Downloads # Go to download directory$ tar xzf flink-*.tgz # Unpack the downloaded archive$ cd flink-1.2.0 2. start flink cluster1$ ./bin/start-local.sh # Start Flink 通过访问http://localhost:8081检查JobManager网页,确保所有组件都已运行。网页会显示一个有效的TaskManager实例。 3. run SocketWindowWordCount现在, 我们可以运行Flink 应用程序。 这个例子将会从一个socket中读一段文本，并且每隔5秒打印每个单词出现的数量。 例如 a tumbling window of processing time, as long as words are floating in. 1$ nc -l 9000 1$ ./bin/flink run examples/streaming/SocketWindowWordCount.jar --port 9000 单词的数量在5秒的时间窗口中进行累加（使用处理时间和tumbling窗口），并打印在stdout。监控JobManager的输出文件，并在nc写一些文本(回车一行就发送一行输入给Flink) : 1234$ nc -l 9000lorem ipsumipsum ipsum ipsumbye 译者注：mac下使用命令nc -l -p 9000来启动监听端口，如果有问题可以telnet localhost 9000看下监听端口是否已经启动，如果启动有问题可以重装netcat ，使用命令brew install netcat。 .out文件将被打印每个时间窗口单词的总数： 1234$ tail -f log/flink-*-jobmanager-*.outlorem : 1bye : 1ipsum : 4 使用以下命令来停止Flink: 1$ ./bin/stop-local.sh 4. idea -&gt; flink maven projectref: https://www.jianshu.com/p/fdc4212422f7 选择Maven 然后选中create from archetype 在列表中找flink-quickstart 找不到的话，点击右边的add archetype 在弹出框中输入： groupId：org.apache.flinkartifactId：flink-quickstart-javaversion：1.11.1repository：https://mirrors.huaweicloud.com/repository/maven/ 从flink.apache.org下载flink-1.11.1-src.tgz，(下载地址：https://www.apache.org/dyn/closer.lua/flink/flink-1.11.1/flink-1.11.1-src.tgz) 也可以从https://github.com/中搜flink，然后点击apache/flink下载源码 将flink-1.11.1-src.tgz解压缩后， 找到子目录\\flink-1.11.1-src\\flink-1.11.1\\flink-examples\\flink-examples-batch\\src\\main\\java\\org\\apache\\flink\\examples\\java\\wordcountcopy文件 将其下的WordCount.java文件及util文件夹复制到FlinkTest工程的以下子目录下 Flink-QuickStart-Java\\src\\main\\java\\org\\example 然后到IDEA,会多出两个文件 修改WordCount.java 把WordCount.java中的 1.将package org.apache.flink.examples.java.wordcount改为 package org.example; 2.将import org.apache.flink.examples.java.wordcount.util.WordCountData; 改为 import org.example.util.WordCountData; 修改util/WordCountData.java 把package org.apache.flink.examples.java.wordcount.util改为 package org.example.util; b)重新编译和打包 运行之：在 {FLINK_HOME}/examples 创建 words.txt. 1./bin/flink run -m localhost:8081 -c org.summer.WordCount examples/myflink-1.0-SNAPSHOT.jar --input examples/words.txt --output examples/result001.txt 5. run flink in Idea下载flink-1.13.0 安装包，这里面有flink网页服务用到的一个jar，地址是：https://flink.apache.org/downloads.html 下载后解压，在lib目录下有个flink-dist_2.11-1.13.0.jar文件，记住此文件的位置，稍后会用到；回到IDEA，在项目上点击右键，点击菜单Open Module Settings：设置工作已经完成，由于StreamingJob的工作是读取本机9000端口的数据，所以我们要把9000端口的服务启动起来，不然StreamingJob运行时是连不上端口的，打开一个控制台，执行命令：nc -l 9000现在可以将StreamingJob运行起来，如下图，右键点击StreamingJob，选择Run ‘StreamingJob.main()’：即可启动flink任务，如果想打断点调试，请选择Debug ‘StreamingJob.main()’ 参考文章 ref: http://ifeve.com/flink-quick-start/ ref: https://www.cnblogs.com/miaoying/p/10341927.html ref: https://www.jianshu.com/p/4a1442da2c4e ref: IDEA上运行Flink任务","link":"/2021/10/25/flink/flink-helloworld/"},{"title":"bind header to controller","text":"12345678910111213141516171819202122232425262728293031import javax.servlet.http.HttpServletRequest;import org.springframework.core.MethodParameter;import org.springframework.web.bind.support.WebDataBinderFactory;import org.springframework.web.context.request.NativeWebRequest;import org.springframework.web.method.support.HandlerMethodArgumentResolver;import org.springframework.web.method.support.ModelAndViewContainer;import lombok.extern.slf4j.Slf4j;import summer.webserver.util.SummerUtils;@Slf4jpublic class SummerArgumentResolver implements HandlerMethodArgumentResolver { public SummerArgumentResolver() { super(); } @Override public boolean supportsParameter(MethodParameter parameter) { return SummerHeader.class.isAssignableFrom(parameter.getParameterType()); } @Override public Object resolveArgument(MethodParameter parameter, ModelAndViewContainer mavContainer, NativeWebRequest webRequest, WebDataBinderFactory binderFactory) { return SummerUtils.getSummerHeader(webRequest.getNativeRequest(HttpServletRequest.class)); }} 12345678910111213141516171819@Slf4jpublic class SummerUtils { public static SummerHeader getSummerHeader(HttpServletRequest request) { User user = (User)request.getAttribute(&quot;user&quot;); String userName = Objects.isNull(user) ? request.getHeader(USER_NAME) : user.getUserName(); String userId = Objects.isNull(user) ? request.getHeader(USER_ID) : user.getUserId(); SummerHeader summerHeader = SummerHeader.builder() .userName(userName) .userId(userId) .build(); if (log.isDebugEnabled()) { log.debug(summerHeader.toString()); } request.setAttribute(&quot;summerHeader&quot;, summerHeader); return summerHeader; }} usage1234567891011121314@Api(tags = &quot;User&quot;)@RestController()@RequestMapping(ROOT_DOMAIN + &quot;/users&quot;)@Slf4jpublic class UserController { @ApiOperation(value = &quot;delete user&quot;) @DeleteMapping(&quot;/{user-id}&quot;) @ResponseStatus(HttpStatus.OK) public void deleteUser(SummerHeader summerHeader, @ApiParam(required = true) @Length(max = 10) @PathVariable(value = &quot;user-id&quot;) String userId) { userMapper.deleteById(userId); }}","link":"/2021/10/25/java/java-bind-header-to-controller/"},{"title":"时间格式 注解 验证","text":"EnumValid.java12345678910111213@Constraint(validatedBy = DateFormatValidator.class)@Target(ElementType.FIELD)@Retention(RetentionPolicy.RUNTIME)public @interface DateFormat { String message() default &quot;&quot;; String pattern() default &quot;yyyy-MM-dd HH:mm:ss&quot;; Class&lt;?&gt;[] groups() default {}; Class&lt;? extends Payload&gt;[] payload() default {};} 校验器EnumValidator.java1234567891011121314151617181920212223242526272829303132333435public class DateFormatValidator implements ConstraintValidator&lt;DateFormat, String&gt; { private String pattern; @Override public void initialize(DateFormat dateFormat) { this.pattern = dateFormat.pattern(); } @Override public boolean isValid(String value, ConstraintValidatorContext context) { boolean isValid = (value == null) || isValid(value); if (!isValid) { throw new RuntimeException(&quot;invalid input parameter.&quot;); } return isValid; } private boolean isValid(String value) { boolean isValid = true; try { String time = URLDecoder.decode(value, &quot;UTF-8&quot;); if (pattern.length() != time.length()) { isValid = false; } else { SimpleDateFormat dateFormat = new SimpleDateFormat(pattern); dateFormat.setLenient(false); dateFormat.parse(time); } } catch (Exception e) { isValid = false; } return isValid; }} 使用的时候，注解作用在DTO字段上：12345public class RequestDto { @DateFormat(pattern = &quot;yyyy-MM-dd&quot;) private String data;}","link":"/2021/10/14/java/java-dataformat-validator/"},{"title":"枚举类型参数验证","text":"EnumValid.java1234567891011121314@Documented@Constraint(validatedBy = EnumValidator.class)@Target({ElementType.TYPE, ElementType.FIELD, ElementType.PARAMETER})@Retention(RetentionPolicy.RUNTIME)public @interface EnumValid { Class&lt;? extends Enum&lt;?&gt;&gt; enumClass(); String message() default &quot;invalid enum item value.&quot;; Class&lt;?&gt;[] groups() default {}; Class&lt;? extends Payload&gt;[] payload() default {};} 校验器EnumValidator.java12345678910111213141516171819202122232425262728293031323334@Slf4jpublic class EnumValidator implements ConstraintValidator&lt;EnumValid, String&gt; { private Class&lt;? extends Enum&lt;?&gt;&gt; enumClass; @Override public void initialize(EnumValid enumValid) { enumClass = enumValid.enumClass(); } @Override public boolean isValid(String value, ConstraintValidatorContext context) { if (!valid(value)) { throw new RuntimeException(&quot;invalid input parameter.&quot;); return false; } return true; } private boolean valid(String value) { try { if (enumClass.isEnum()) { Method method = enumClass.getMethod(&quot;isValidName&quot;, value.getClass()); Boolean result = (Boolean)method.invoke(null, value); return result != null &amp;&amp; result; } } catch (InvocationTargetException | NoSuchMethodException | IllegalAccessException e) { throw new RuntimeException(&quot;Enum values valid error.&quot;); } return false; }} 枚举类123456789public enum SxWhetherIntEnum { YES, NO; public static boolean isValidName(String name) { return Arrays.stream(values()).anyMatch(item -&gt; item.name().equals(name)); }} 使用的时候，注解作用在DTO字段上：123456public class RequestDto { @ApiModelProperty(value = &quot;是否可转定，1-是，2-否（补充）&quot;) @EnumValid(enumClass = SxWhetherIntEnum.class) private Integer payable;} 参考文章 https://www.cnblogs.com/wjqhuaxia/p/12153053.html https://blog.csdn.net/www_tlj/article/details/103950945 https://www.cnblogs.com/wjqhuaxia/p/12153053.html","link":"/2021/10/14/java/java-enum-valid/"},{"title":"GC算法 垃圾收集器","text":"概述jvm 中，程序计数器、虚拟机栈、本地方法栈都是随线程而生随线程而灭，栈帧随着方法的进入和退出做入栈和出栈操作，实现了自动的内存清理。 因此，我们的内存垃圾回收主要集中于 **java 堆和方法区中**，在程序运行期间，这部分内存的分配和使用都是动态的. 对象存活判断判断对象是否存活一般有两种方式： 引用计数：每个对象有一个引用计数属性，新增一个引用时计数加1，引用释放时计数减1，计数为0时可以回收。此方法简单，无法解决对象相互循环引用的问题。 可达性分析（Reachability Analysis）：从GC Roots开始向下搜索，搜索所走过的路径称为引用链。当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的。不可达对象。 在Java语言中，GC Roots包括： 虚拟机栈中引用的对象。 方法区中类静态属性实体引用的对象。 方法区中常量引用的对象。 本地方法栈中JNI引用的对象。 垃圾收集算法标记 -清除算法 “标记-清除”（Mark-Sweep）算法，如它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。之所以说它是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其缺点进行改进而得到的。 它的主要缺点有两个：一个是效率问题，标记和清除过程的效率都不高；另外一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致，当程序在以后的运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 参考文章-","link":"/2021/09/29/java/java-gc/"},{"title":"ConfigurationProperties","text":"在 Spring Boot 项目中，我们将大量的参数配置在 application.properties 或 application.yml 文件中，通过 @ConfigurationProperties 注解，我们可以方便的获取这些参数值 使用 @ConfigurationProperties 配置模块假设我们正在搭建一个发送邮件的模块。在本地测试，我们不想该模块真的发送邮件，所以我们需要一个参数来「开关」 disable 这个功能。另外，我们希望为这些邮件配置一个默认的主题，这样，当我们查看邮件收件箱，通过邮件主题可以快速判断出这是测试邮件 在 application.properties 文件中创建这些参数:我们可以使用 @Value 注解或着使用 Spring Environment bean 访问这些属性，是这种注入配置方式有时显得很笨重。我们将使用更安全的方式(@ConfigurationProperties )来获取这些属性 @ConfigurationProperties 的基本用法非常简单:我们为每个要捕获的外部属性提供一个带有字段的类。请注意以下几点: 前缀定义了哪些外部属性将绑定到类的字段上 根据 Spring Boot 宽松的绑定规则，类的属性名称必须与外部属性的名称匹配 我们可以简单地用一个值初始化一个字段来定义一个默认值 类本身可以是包私有的 类的字段必须有公共 setter 方法 激活 @ConfigurationProperties对于 Spring Boot，创建一个 MailModuleProperties 类型的 bean，我们可以通过下面几种方式将其添加到应用上下文中 首先，我们可以通过添加 @Component 注解让 Component Scan 扫描到很显然，只有当类所在的包被 Spring @ComponentScan 注解扫描到才会生效，默认情况下，该注解会扫描在主应用类下的所有包结构 参考文章 https://blog.csdn.net/yusimiao/article/details/97622666 面试还不知道BeanFactory和ApplicationContext的区别？","link":"/2021/11/03/java/java-ConfigurationProperties/"},{"title":"Java内存模型","text":"Java 内存模型对JVM内存结构的描述中，我们知道了堆和方法区是线程共享的。而局部变量，方法定义参数和异常处理器参数就不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。 Java线程之间的通信由Java内存模型（本文简称为JMM）控制，JMM决定一个线程对共享变量的写入何时对另一个线程可见。从抽象的角度来看，JMM定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读/写共享变量的副本。本地内存是JMM的一个抽象概念，并不真实存在。它涵盖了缓存，写缓冲区，寄存器以及其他的硬件和编译器优化。 Java内存模型的抽象示意图如下： 从上图来看，线程A与线程B之间如要通信的话，必须要经历下面2个步骤： 首先，线程A把本地内存A中更新过的共享变量刷新到主内存中去。 然后，线程B到主内存中去读取线程A之前已更新过的共享变量。 下面通过示意图来说明这两个步骤： 如上图所示，本地内存A和B有主内存中共享变量x的副本。假设初始时，这三个内存中的x值都为0。线程A在执行时，把更新后的x值（假设值为1）临时存放在自己的本地内存A中。当线程A和线程B需要通信时，线程A首先会把自己本地内存中修改后的x值刷新到主内存中，此时主内存中的x值变为了1。随后，线程B到主内存中去读取线程A更新后的x值，此时线程B的本地内存的x值也变为了1。 从整体来看，这两个步骤实质上是线程A在向线程B发送消息，而且这个通信过程必须要经过主内存。JMM通过控制主内存与每个线程的本地内存之间的交互，来为java程序员提供内存可见性保证。 重排序在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三种类型： 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读 / 写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。从 java 源代码到最终实际执行的指令序列，会分别经历下面三种重排序： 上述的 1 属于编译器重排序，2 和 3 属于处理器重排序。这些重排序都可能会导致多线程程序出现内存可见性问题。对于编译器，JMM 的编译器重排序规则会禁止特定类型的编译器重排序（不是所有的编译器重排序都要禁止）。对于处理器重排序，JMM 的处理器重排序规则会要求 java 编译器在生成指令序列时，插入特定类型的内存屏障（memory barriers，intel 称之为 memory fence）指令，通过内存屏障指令来禁止特定类型的处理器重排序（不是所有的处理器重排序都要禁止）。 JMM 属于语言级的内存模型，它确保在不同的编译器和不同的处理器平台之上，通过禁止特定类型的编译器重排序和处理器重排序，为程序员提供一致的内存可见性保证。 处理器重排序与内存屏障指令现代的处理器使用写缓冲区来临时保存向内存写入的数据。写缓冲区可以保证指令流水线持续运行，它可以避免由于处理器停顿下来等待向内存写入数据而产生的延迟。同时，通过以批处理的方式刷新写缓冲区，以及合并写缓冲区中对同一内存地址的多次写，可以减少对内存总线的占用。虽然写缓冲区有这么多好处，但每个处理器上的写缓冲区，仅仅对它所在的处理器可见。这个特性会对内存操作的执行顺序产生重要的影响：处理器对内存的读 / 写操作的执行顺序，不一定与内存实际发生的读 / 写操作顺序一致！为了具体说明，请看下面示例： 1234Processor A Processor Ba = 1; //A1 x = b; //A2 b = 2; //B1 y = a; //B2 初始状态：a = b = 0 处理器允许执行后得到结果：x = y = 0假设处理器 A 和处理器 B 按程序的顺序并行执行内存访问，最终却可能得到 x = y = 0 的结果。具体的原因如下图所示： 这里处理器 A 和处理器 B 可以同时把共享变量写入自己的写缓冲区（A1，B1），然后从内存中读取另一个共享变量（A2，B2），最后才把自己写缓存区中保存的脏数据刷新到内存中（A3，B3）。当以这种时序执行时，程序就可以得到 x = y = 0 的结果。 从内存操作实际发生的顺序来看，直到处理器 A 执行 A3 来刷新自己的写缓存区，写操作 A1 才算真正执行了。虽然处理器 A 执行内存操作的顺序为：A1-&gt;A2，但内存操作实际发生的顺序却是：A2-&gt;A1。此时，处理器 A 的内存操作顺序被重排序了（处理器 B 的情况和处理器 A 一样，这里就不赘述了）。 这里的关键是，由于写缓冲区仅对自己的处理器可见，它会导致处理器执行内存操作的顺序可能会与内存实际的操作执行顺序不一致。由于现代的处理器都会使用写缓冲区，因此现代的处理器都会允许对写 - 读操做重排序。 happens-before从 JDK5 开始，java 使用新的 JSR -133 内存模型（本文除非特别说明，针对的都是 JSR- 133 内存模型）。JSR-133 **提出了 happens-before 的概念，通过这个概念来阐述操作之间的内存可见性**。如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须存在 happens-before 关系。这里提到的两个操作既可以是在一个线程之内，也可以是在不同线程之间。 与程序员密切相关的 happens-before 规则如下： 程序顺序规则：一个线程中的每个操作，happens- before 于该线程中的任意后续操作。 监视器锁规则：对一个监视器锁的解锁，happens- before 于随后对这个监视器锁的加锁。 volatile 变量规则：对一个 volatile 域的写，happens- before 于任意后续对这个 volatile 域的读。 传递性：如果 A happens- before B，且 B happens- before C，那么 A happens- before C。 注意，两个操作之间具有 happens-before 关系，并不意味着前一个操作必须要在后一个操作之前执行！happens-before 仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前（the first is visible to and ordered before the second）。happens- before 的定义很微妙，后文会具体说明 happens-before 为什么要这么定义。 happens-before 与 JMM 的关系如下图所示： 如上图所示，一个 happens-before 规则通常对应于多个编译器重排序规则和处理器重排序规则。对于 java 程序员来说，happens-before 规则简单易懂，它避免程序员为了理解 JMM 提供的内存可见性保证而去学习复杂的重排序规则以及这些规则的具体实现。 参考文章 https://zhuanlan.zhihu.com/p/38348646 https://www.infoq.cn/article/java-memory-model-1/","link":"/2021/09/28/java/java-jmm/"},{"title":"java多线程那点事","text":"java多线程那点事|提升java能力阿里面试官的分享Java面试中需要准备哪些多线程并发的技术要点","link":"/2021/10/25/java/java-multi-thread/"},{"title":"读取txt文件","text":"12345678910111213141516171819202122public class ReadFileByLines { public static void main(String[] args) { try { //1.打开一个file File file = new File(&quot;E:/test.txt&quot;); //2.InputStreamReader&lt;-FileInputStream&lt;-file FileInputStream fis = new FileInputStream(file); InputStreamReader is = new InputStreamReader(fis); //3.用BufferedReader(&lt;-InputStreamReader)的readLine()方法读取 BufferedReader br = new BufferedReader(is); //4.输出 String txtLine = null; while ((txtLine = br.readLine()) != null) { System.out.println(txtLine); } br.close(); } catch (Exception e) { e.printStackTrace(); } }} 123456789101112131415161718public class ReadFileByLines { public static void main(String[] args) { try { File file = new File(&quot;E:/test.txt&quot;); BufferedReader br = new BufferedReader(new FileReader(file)); String txtLine = null; while ((txtLine = br.readLine()) != null) { System.out.println(txtLine); } br.close(); } catch (Exception e) { e.printStackTrace(); } }}","link":"/2021/10/25/java/java-read-txt-file/"},{"title":"some parameters that only one is not null valid","text":"注解12345678910111213141516171819202122@Documented@Target({ElementType.FIELD})@Retention(RetentionPolicy.RUNTIME)public @interface CustomValid {}@Documented@Constraint(validatedBy = OnlyOneNotNullValidator.class)@Target({ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)public @interface OnlyOneNotNull { String message() default &quot;only one filed should not be null&quot;; Class&lt;?&gt;[] groups() default {}; Class&lt;? extends Payload&gt;[] payload() default {}; String[] fieldNames();} OnlyOneNotNullValidator.java12345678910111213141516171819202122232425262728293031323334353637383940414243@Component@Slf4jpublic class OnlyOneNotNullValidator implements ConstraintValidator&lt;OnlyOneNotNull, Object&gt; { private String[] fieldNames; @Override public void initialize(OnlyOneNotNull onlyOneNotNull) { this.fieldNames = onlyOneNotNull.fieldNames(); } @Override public boolean isValid(Object object, ConstraintValidatorContext context) { if (Objects.isNull(object)) { return true; } Class clazz = object.getClass(); try { List&lt;Field&gt; fields = Arrays.stream(fieldNames) .map(fieldName -&gt; ReflectionUtils.findField(clazz, fieldName)) .filter(Objects::nonNull).peek(field -&gt; field.setAccessible(true)) .filter(field -&gt; Objects.nonNull(ReflectionUtils.getField(field, object))) .collect(Collectors.toList()); boolean onlyOneNotNull = fields.size() == 1; if (!onlyOneNotNull) { invalid(context); } else if (fields.get(0).isAnnotationPresent(CustomValid.class)) { validate(fields.get(0).get(object)); } return onlyOneNotNull; } catch (IllegalAccessException e) { log.warn(e.getMessage(), e); return false; } catch (Exception e) { return false; } } private void invalid(ConstraintValidatorContext context) { throw new RuntimeException(String.format(&quot;[%s]&quot;, String.join(&quot;, &quot;, fieldNames)) + &quot; only one filed should not be null&quot;); }} 12345678910111213141516171819202122// valid object's field, for example: valid ClassA.aaa notNull. public static void validate(Object object) { if (object != null) { Class objClass = object.getClass(); try { object = objectMapper.readValue(objectMapper.writeValueAsString(object), Object.class); XssUtils.unescapeObject(object); object = objectMapper.convertValue(object, objClass); } catch (IOException e) { throw new RuntimeException(&quot;QUERY_PARAM_ERROR&quot;); } } javax.validation.Validator validator = (javax.validation.Validator)new ApplicationContextHolder().context.getBean(&quot;validator&quot;); Set&lt;ConstraintViolation&lt;Object&gt;&gt; violations = validator.validate(object); if (!violations.isEmpty()) { StringBuilder msg = new StringBuilder(); for (ConstraintViolation&lt;Object&gt; violation : violations) { msg.append(&quot;[&quot;).append(violation.getPropertyPath()).append(&quot;]&quot;).append(violation.getMessage()); } throw new RuntimeException(&quot;bad request&quot;); } } 使用123456789101112131415161718192021222324252627282930313233@Getter@Setter@Builder@NoArgsConstructor@AllArgsConstructor@OnlyOneNotNull(fieldNames = {&quot;aClass&quot;, &quot;bClass&quot;, &quot;cClass&quot;})public class RequestDto { @CustomValid private AClass aClass; @CustomValid private BClass bClass; @CustomValid private CClass cClass;}@Getter@Setter@Builder@NoArgsConstructor@AllArgsConstructorpublic class AClass { @NotNull private Boolean aaa; @NotNull private BBBB bnbb; @NotBlank @Pattern(regexp = ApiGatewayConst.PatternRegexp.METHOD_HTTP_URL) @Length(max = 1500) private String url;}","link":"/2021/10/14/java/java-onlyOneNotNull-valid/"},{"title":"图 DFS","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102import java.util.Stack;public class Graph { class Vertex{ //顶点 public char lebel; public boolean visited; public Vertex(char lab){ lebel = lab; visited = false; } } private final int maxVertices = 20 ; //最大顶点数 private Vertex vertexList[]; private int adjMatrix[][]; private int vertexCount; private Stack theStack; public Graph(){ vertexList = new Vertex[maxVertices]; adjMatrix = new int[maxVertices][maxVertices]; vertexCount = 0; for(int y = 0 ; y &lt; maxVertices ; y++){ for(int x = 0 ; x &lt; maxVertices ; x ++){ adjMatrix[x][y] = 0; } } theStack = new Stack(); } public void addVertex(char lab){ vertexList[vertexCount++]=new Vertex(lab); } public void addEdge(int start,int end){ adjMatrix[start][end] = 1; adjMatrix[end][start] = 1; } public void displayVertex(int v){ System.out.println(vertexList[v].lebel); } public void dfs(){ vertexList[0].visited = true; displayVertex(0); theStack.push(0); while (!theStack.isEmpty()){ int v = getAdjUnvisitedVertes((int)theStack.peek()); if(v == -1){ theStack.pop(); }else { vertexList[v].visited = true; displayVertex(v); theStack.push(v); } } for(int j = 0 ; j &lt; vertexCount; j++){ vertexList[j].visited = false; } } public int getAdjUnvisitedVertes(int v){ for(int j = 0 ; j &lt; vertexCount ; j ++){ if(adjMatrix[v][j] == 1 &amp;&amp; vertexList[j].visited == false){ return j; } } return -1; } public static void main(String[] args) { int col = 9 ; int[][] a = new int[col][col]; a[0][1] = 1;a[0][5] = 1;a[1][2] = 1;a[1][6]= 1;a[1][8]= 1; a[2][3] = 1; a[2][8] = 1; a[3][8] =1; a[3][6]= 1; a[3][7] = 1;a[3][4] = 1; a[4][5] =1; a[4][7]= 1; a[5][6] = 1;a[6][7] = 1; Graph graph = new Graph(); graph.addVertex('A'); graph.addVertex('B'); graph.addVertex('C'); graph.addVertex('D'); graph.addVertex('E'); graph.addVertex('F'); graph.addVertex('G'); graph.addVertex('H'); graph.addVertex('I'); for(int y = 0 ; y &lt; col; y ++){ for(int x = 0 ; x &lt; col ; x ++ ){ if(a[y][x] == 1){ graph.addEdge(y,x); } } } graph.dfs(); }}","link":"/2021/10/25/java/java-graph-dfs/"},{"title":"常见内存溢出错误","text":"Exception in thread “main”: java.lang.OutOfMemoryError: Java heap space 原因：对象不能被分配到堆内存中。 Exception in thread “main”: java.lang.OutOfMemoryError: PermGen space 原因：类或者方法不能被加载到老年代。它可能出现在一个程序加载很多类的时候，比如引用了很多第三方的库。 Exception in thread “main”: java.lang.OutOfMemoryError: Requested array size exceeds VM limit 原因：创建的数组大于堆内存的空间。 Exception in thread “main”: java.lang.OutOfMemoryError: request bytes for . Out of swap space? 原因：分配本地分配失败。JNI、本地库或者Java虚拟机都会从本地堆中分配内存空间。 Exception in thread “main”: java.lang.OutOfMemoryError: （Native method） 原因：同样是本地方法内存分配失败，只不过是JNI或者本地方法或者Java虚拟机发现。 Troubleshooting Guide for HotSpot VM”, Chapter 3 on “Troubleshooting on memory leaks","link":"/2021/09/28/java/java-oom/"},{"title":"ThreadPoolTaskExecutor","text":"1234567891011121314151617181920212223242526import java.util.concurrent.ThreadPoolExecutor;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.scheduling.annotation.EnableAsync;import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;@Configuration@EnableAsyncpublic class ThreadPool { public static final int DEFAULT_THREADS_NUMS = 2 * Runtime.getRuntime().availableProcessors(); @Bean(&quot;taskExecutor&quot;) public ThreadPoolTaskExecutor taskExecutor() { ThreadPoolTaskExecutor threadPoolTaskExecutor = new ThreadPoolTaskExecutor(); int maxPoolSize = 2 * DEFAULT_THREADS_NUMS; threadPoolTaskExecutor.setCorePoolSize(DEFAULT_THREADS_NUMS); threadPoolTaskExecutor.setMaxPoolSize(maxPoolSize); threadPoolTaskExecutor.setQueueCapacity(maxPoolSize); threadPoolTaskExecutor.setKeepAliveSeconds(60); threadPoolTaskExecutor.setThreadNamePrefix(&quot;Async-Thread-&quot;); threadPoolTaskExecutor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy()); return threadPoolTaskExecutor; }} 参考文章","link":"/2021/11/10/java/java-threadpool/"},{"title":"java 单例模式的写法","text":"饱汉模式饱汉是变种最多的单例模式。我们从饱汉出发，通过其变种逐渐了解实现单例模式时需要关注的问题。 基础的饱汉饱汉，即已经吃饱，不着急再吃，饿的时候再吃。所以他就先不初始化单例，等第一次使用的时候再初始化，即·“懒加载”·。 12345678910111213// 饱汉// UnThreadSafepublic class Singleton1 { private static Singleton1 singleton = null; private Singleton1() { } public static Singleton1 getInstance() { if (singleton == null) { singleton = new Singleton1(); } return singleton; }} 饱汉模式的核心就是懒加载。好处是更启动 速度快、节省资源，一直到实例被第一次访问，才需要初始化单例；小坏处是写起来麻烦，大坏处是线程不安全，if语句存在竞态条件。 写起来麻烦不是大问题，可读性好啊。因此，单线程环境下，基础饱汉是最好。但多线程环境下，基础饱汉就彻底不可用了。下面的几种变种都在试图解决基础饱汉线程不安全的问题。 饱汉 - 变种 1最粗暴的犯法是用synchronized关键字修饰getInstance()方法，这样能达到绝对的线程安全。 12345678910111213// 饱汉// ThreadSafepublic class Singleton1_1 { private static Singleton1_1 singleton = null; private Singleton1_1() { } public synchronized static Singleton1_1 getInstance() { if (singleton == null) { singleton = new Singleton1_1(); } return singleton; }} 变种1的好处是写起来简单，且绝对线程安全；坏处是并发性能极差，事实上完全退化到了串行。单例只需要初始化一次，但就算初始化以后，synchronized的锁也无法避开，从而getInstance()完全变成了串行操作。性能不敏感的场景建议使用。 饱汉 - 变种 2变种2是“臭名昭著”的DCL 1.0。 针对变种1中单例初始化后锁仍然无法避开的问题，变种2在变种1的外层又套了一层check，加上synchronized内层的check，即所谓“双重检查锁”（Double Check Lock，简称DCL）。 123456789101112131415161718192021// 饱汉// UnThreadSafepublic class Singleton1_2 { private static Singleton1_2 singleton = null; public int f1 = 1; // 触发部分初始化问题 public int f2 = 2; private Singleton1_2() { } public static Singleton1_2 getInstance() { // may get half object if (singleton == null) { synchronized (Singleton1_2.class) { if (singleton == null) { singleton = new Singleton1_2(); } } } return singleton; }} 变种2的核心是DCL，看起来变种2似乎已经达到了理想的效果：懒加载+线程安全。可惜的是，正如注释中所说，DCL仍然是线程不安全的，由于指令重排序，你可能会得到“半个对象”，即”部分初始化“问题。 参考：volatile关键字的作用、原理 饱汉 - 变种 3变种3专门针对变种2，可谓DCL 2.0。 针对变种3的“半个对象”问题，变种3在instance上增加了volatile关键字，原理见上述参考。 123456789101112131415161718192021// 饱汉// ThreadSafepublic class Singleton1_3 { private static volatile Singleton1_3 singleton = null; public int f1 = 1; // 触发部分初始化问题 public int f2 = 2; private Singleton1_3() { } public static Singleton1_3 getInstance() { if (singleton == null) { synchronized (Singleton1_3.class) { // must be a complete instance if (singleton == null) { singleton = new Singleton1_3(); } } } return singleton; }} 多线程环境下，变种3更适用于性能敏感的场景。但后面我们将了解到，就算是线程安全的，还有一些办法能破坏单例。 当然，还有很多方式，能通过与volatile类似的方式防止部分初始化。读者可自行阅读内存屏障相关内容，但面试时不建议主动装逼。猴子后面会专门整理一篇文章讨论内存屏障，此处不表。 饿汉模式与饱汉相对，饿汉很饿，只想着尽早吃到。所以他就在最早的时机，即类加载时初始化单例，以后访问时直接返回即可。 12345678910// 饿汉// ThreadSafepublic class Singleton2 { private static final Singleton2 singleton = new Singleton2(); private Singleton2() { } public static Singleton2 getInstance() { return singleton; }} 饿汉的好处是天生的线程安全（得益于类加载机制），写起来超级简单，使用时没有延迟；坏处是有可能造成资源浪费（如果类加载后就一直不使用单例的话）。 值得注意的时，单线程环境下，饿汉与饱汉在性能上没什么差别；但多线程环境下，由于饱汉需要加锁，饿汉的性能反而更优。 Holder模式我们既希望利用饿汉模式中静态变量的方便和线程安全；又希望通过懒加载规避资源浪费。Holder模式满足了这两点要求：核心仍然是静态变量，足够方便和线程安全；通过静态的Holder类持有真正实例，间接实现了懒加载。 123456789101112131415// Holder模式// ThreadSafepublic class Singleton3 { private static class SingletonHolder { private static final Singleton3 singleton = new Singleton3(); private SingletonHolder() { } } private Singleton3() { } public static Singleton3 getInstance() { return SingletonHolder.singleton; }} 相对于饿汉模式，Holder模式仅增加了一个静态内部类的成本，与饱汉的变种3效果相当（略优），都是比较受欢迎的实现方式。同样建议考虑。 枚举模式用枚举实现单例模式，相当好用，但可读性是不存在的。 基础的枚举将枚举的静态成员变量作为单例的实例： 12345// 枚举// ThreadSafepublic enum Singleton4 { SINGLETON;} 代码量比饿汉模式更少。但用户只能直接访问实例Singleton4.SINGLETON——事实上，这样的访问方式作为单例使用也是恰当的，只是牺牲了静态工厂方法的优点，如无法实现懒加载。 丑陋但好用的语法糖Java的枚举是一个“丑陋但好用的语法糖”。 枚举型单例模式的本质通过反编译（jad，源码|String拼接操作”+”的优化？也用到了）打开语法糖，就看到了枚举类型的本质，简化如下： 1234567// 枚举// ThreadSafepublic class Singleton4 extends Enum&lt;Singleton4&gt; { ... public static final Singleton4 SINGLETON = new Singleton4(); ...} 本质上和饿汉模式相同，区别仅在于公有的静态成员变量。 用枚举实现一些trick这一部分与单例没什么关系，可以跳过。如果选择阅读也请认清这样的事实：虽然枚举相当灵活，但如何恰当的使用枚举有一定难度。一个足够简单的典型例子是TimeUnit类，建议有时间耐心阅读。 上面已经看到，枚举型单例的本质仍然是一个普通的类。实际上，我们可以在枚举型型单例上增加任何普通类可以完成的功能。要点在于枚举实例的初始化，可以理解为实例化了一个匿名内部类。为了更明显，我们在Singleton4_1中定义一个普通的私有成员变量，一个普通的公有成员方法，和一个公有的抽象成员方法，如下： 123456789101112131415161718192021// 枚举// ThreadSafepublic enum Singleton4_1 { SINGLETON(&quot;enum is the easiest singleton pattern, but not the most readable&quot;) { public void testAbsMethod() { print(); System.out.println(&quot;enum is ugly, but so flexible to make lots of trick&quot;); } }; private String comment = null; Singleton4_1(String comment) { this.comment = comment; } public void print() { System.out.println(&quot;comment=&quot; + comment); } abstract public void testAbsMethod(); public static Singleton4_1 getInstance() { return SINGLETON; }} 这样，枚举类Singleton4_1中的每一个枚举实例不仅继承了父类Singleton4_1的成员方法print()，还必须实现父类Singleton4_1的抽象成员方法testAbsMethod()。 总结上面的分析都忽略了反射和序列化的问题。通过反射或序列化，我们仍然能够访问到私有构造器，创建新的实例破坏单例模式。此时，只有枚举模式能天然防范这一问题。反射和序列化猴子还不太了解，但基本原理并不难，可以在其他模式上手动实现。 下面继续忽略反射和序列化的问题，做个总结回味一下： 实现方式 关键点 资源浪费 线程安全 多线程环境的性能足够优化 基础饱汉 懒加载 否 否 - 饱汉变种1 懒加载、同步 否 是 否 饱汉变种2 懒加载、DCL 否 否 - 饱汉变种3 懒加载、DCL、volatile 否 是 是 饿汉 静态变量初始化 是 是 是 Holder 静态变量初始化、holder 否 是 是 枚举 枚举本质、静态变量初始化 否 是 是","link":"/2021/09/23/java/java-singleton/"},{"title":"Linux 组调度","text":"参考文章 Linux 组调度浅析 Linux内核之实时进程调度和组调度","link":"/2021/09/24/linux/linux-cgroup/"},{"title":"linux 常用命令","text":"拷贝文件 SCP123456# 拷贝本地文件到远程scp xxx.txt root@{ip / hosts}:/root# 拷贝远程文件到本地scp root@{ip / hosts}:/rootxxx.txt /root top 命令1234567891011# d：指定每两次屏幕信息刷新之间的时间间隔。当然用户可以使用s交互命令来改变之。# p:通过指定监控进程ID来仅仅监控某个进程的状态。# q:该选项将使top没有任何延迟的进行刷新。如果调用程序有超级用户权限，那么top将以尽可能高的优先级运行。# S：指定累计模式。# s：使top命令在安全模式中运行。这将去除交互命令所带来的潜在危险。# i：使top不显示任何闲置或者僵死进程。# c:显示整个命令行而不只是显示命令名。top -H -b -d 1 -n 200 &gt; top.txt# -p : 通过监控进程ID来仅仅监控某个进程的状态top -p {pid}# -i ： 不显示任何闲置或僵死的进程 free12free -mfree -h jmap123# 查看堆内存使用情况jmap -heap {pid} netstat 123456789101112131415# -a (all)显示所有选项，默认不显示LISTEN相关# -t (tcp)仅显示tcp相关选项# -u (udp)仅显示udp相关选项# -n 拒绝显示别名，能显示数字的全部转化成数字。# -l 仅列出有在 Listen (监听) 的服務状态# -p 显示建立相关链接的程序名# -r 显示路由信息，路由表# -e 显示扩展信息，例如uid等# -s 按各个协议进行统计# -c 每隔一个固定时间，执行该netstat命令。netstat -ltnpnetstat -ano | findstr &quot;6379&quot; cat 日志查询123456789# 1. 查询日志中关键字cat -n summer.log | grep &quot;关键字&quot; | wc -l # 2.查询某段时间内的日志sed -n '/start_time/,/end_time/p' summer.log|grep &quot;key words&quot;# set -n '/2021-03-15 01:29:17/,/2021-03-15 02:29:17/p' summer.logwc -l # 统计行数wc -c # 统计字节数wc -l # 统计字数 sed123# sed -n '/start time/,/end time/p' aaa.log | grep &quot;keyword&quot;sed -n '/2021-03-16 01:29:17/,/2021-03-16 02:29:17/p' aa.log | grep &quot;keyword&quot; grep 查看大日志文件123456789101112# A -&gt; After# B -&gt; Before# C -&gt; Contextgrep -A5 'Time: 210607 15:[45-59]' slow3306_9110.log# 匹配多个关键字（且）# 管道符连接 多个条件 实现关键字 且关系 匹配：grep -A5 'Time: 210607 15:[45-59]' slow3306_9110.log | grep 'Query_time: (\\d[2-5])'# 同一行同时满足两个条件（Time、Query_time）才能够匹配。# grep -E 匹配多个关键字（或）grep -E &quot;word1|word2|word3&quot; file.txt# 匹配文件中 同一行包含 word1、word2、word3 之一 tar 压缩、解压123456789# -c: 打包 把 /conf/xxx.* 打包到 xxx.tar.gztar czvf xxx.tar.gz /conf/xxx.*# -x: 解压缩 tar xzvf xxx.tar.gz# -z: gzip 压缩格式# -v: 显示打包guoch# -f：显示打包名称 linux 重启hosts12sudo /etc/init.d/network restart linux 文件属性、权限123456789101112131415# 修改文件属主、组chown -R lch.lch /usr/localchgrp -R lch /usr/local# 修改 u g o 权限chomd g=rwx 1.txtchomd u=rwx 1.txtchomd o=rwx 1.txt# 增加shell 执行权限chmod +x xxx.sh# 修改用户的组usermod -g root lchuserdel -R lch add user123456adduser '用户名'passwd '用户名'chmod -v u+w /etc/sudoersvim /etc/sudoers 找到Allow root to run any commands anywhere之后添加一行 1'用户名' ALL=(ALL) ALL 如需新用户使用sudo时不用输密码，把最后一个ALL改为NOPASSWD:ALL即可 firewall1sudo systemctl status firewalld 1sudo firewall-cmd --zone=public --add-port=3306/tcp --permanent 1sudo firewall-cmd --reload 1sudo firewall-cmd --list-all 1234567891011121314151617181920212223启动一个服务：systemctl start firewalld.service关闭一个服务：systemctl stop firewalld.service重启一个服务：systemctl restart firewalld.service显示一个服务的状态：systemctl status firewalld.service在开机时启用一个服务：systemctl enable firewalld.service在开机时禁用一个服务：systemctl disable firewalld.service查看服务是否开机启动：systemctl is-enabled firewalld.service查看已启动的服务列表：systemctl list-unit-files|grep enabled查看启动失败的服务列表：systemctl --failed配置firewalld-cmd查看版本： firewall-cmd --version查看帮助： firewall-cmd --help显示状态： firewall-cmd --state查看所有打开的端口： firewall-cmd --zone=public --list-ports更新防火墙规则： firewall-cmd --reload查看区域信息: firewall-cmd --get-active-zones查看指定接口所属区域： firewall-cmd --get-zone-of-interface=eth0拒绝所有包：firewall-cmd --panic-on取消拒绝状态： firewall-cmd --panic-off查看是否拒绝： firewall-cmd --query-panic ab testapach AB test1234# -n: 执行的请求数# -c: 并发数ab -c10 -n100 http://www.baidu.com/","link":"/2021/09/27/linux/linux-cmd/"},{"title":"mysql 数据库事务","text":"事务四大属性原子性（Atomicity）事务包含的所有操作要么全部成功，要么全部失败回滚 一致性（Consistency）一致性是指事务必须使数据库从一个一致性状态变换到另一个一致性状态，也就是说一个事务执行之前和执行之后都必须处于一致性状态。举例来说，假设用户A和用户B两者的钱加起来一共是1000，那么不管A和B之间如何转账、转几次账，事务结束后两个用户的钱相加起来应该还得是1000，这就是事务的一致性。 隔离性（Isolation）隔离性是当多个用户并发访问数据库时，比如同时操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。 持久性（Durability）持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。例如我们在使用JDBC操作数据库时，在提交事务方法后，提示用户事务操作完成，当我们程序执行完成直到看到提示后，就可以认定事务已经正确提交，即使这时候数据库出现了问题，也必须要将我们的事务完全执行完成。否则的话就会造成我们虽然看到提示事务处理完毕，但是数据库因为故障而没有执行事务的重大错误。这是不允许的。 事务的隔离级别现在来看看MySQL数据库为我们提供的四种隔离级别： 1234 ① Serializable (串行化)：可避免脏读、不可重复读、幻读的发生。 ② Repeatable read (可重复读)：可避免脏读、不可重复读的发生。 ③ Read committed (读已提交)：可避免脏读的发生。 ④ Read uncommitted (读未提交)：最低级别，任何情况都无法保证。 为什么要设置隔离级别在数据库操作中，在并发的情况下可能出现如下问题： 更新丢失（Lost update）如果多个线程操作，基于同一个查询结构对表中的记录进行修改，那么后修改的记录将会覆盖前面修改的记录，前面的修改就丢失掉了，这就叫做更新丢失。这是因为系统没有执行任何的锁操作，因此并发事务并没有被隔离开来。 第1类丢失更新：事务A撤销时，把已经提交的事务B的更新数据覆盖了。 第2类丢失更新：事务A覆盖事务B已经提交的数据，造成事务B所做的操作丢失。 解决方法：对行加锁，只允许并发一个更新事务。 脏读（Dirty Reads）脏读（Dirty Read）：A事务读取B事务尚未提交的数据并在此基础上操作，而B事务执行回滚，那么A读取到的数据就是脏数据。 解决办法：如果在第一个事务提交前，任何其他事务不可读取其修改过的值，则可以避免该问题。 不可重复读（Non-repeatable Reads）一个事务对同一行数据重复读取两次，但是却得到了不同的结果。事务T1读取某一数据后，事务T2对其做了修改，当事务T1再次读该数据时得到与前一次不同的值。 解决办法：如果只有在修改事务完全提交之后才可以读取数据，则可以避免该问题。 幻象读指两次执行同一条 select 语句会出现不同的结果，第二次读会增加一数据行，并没有说这两次执行是在同一个事务中。一般情况下，幻象读应该正是我们所需要的。但有时候却不是，如果打开的游标，在对游标进行操作时，并不希望新增的记录加到游标命中的数据集中来。隔离级别为 游标稳定性 的，可以阻止幻象读。例如：目前工资为1000的员工有10人。那么事务1中读取所有工资为1000的员工，得到了10条记录；这时事务2向员工表插入了一条员工记录，工资也为1000；那么事务1再次读取所有工资为1000的员工共读取到了11条记录。 解决办法：如果在操作事务完成数据处理之前，任何其他事务都不可以添加新数据，则可避免该问题。 正是为了解决以上情况，数据库提供了几种隔离级别。 事务的隔离级别数据库事务的隔离级别有4个，由低到高依次为Read uncommitted(未授权读取、读未提交)、Read committed（授权读取、读提交）、Repeatable read（可重复读取）、Serializable（序列化），这四个级别可以逐个解决脏读、不可重复读、幻象读这几类问题。 Read uncommitted(未授权读取、读未提交)：如果一个事务已经开始写数据，则另外一个事务则不允许同时进行写操作，但允许其他事务读此行数据。该隔离级别可以通过“排他写锁”实现。这样就避免了更新丢失，却可能出现脏读。也就是说事务B读取到了事务A未提交的数据。Read committed（授权读取、读提交）：读取数据的事务允许其他事务继续访问该行数据，但是未提交的写事务将会禁止其他事务访问该行。该隔离级别避免了脏读，但是却可能出现不可重复读。事务A事先读取了数据，事务B紧接了更新了数据，并提交了事务，而事务A再次读取该数据时，数据已经发生了改变。 Repeatable read（可重复读取）：可重复读是指在一个事务内，多次读同一数据。在这个事务还没有结束时，另外一个事务也访问该同一数据。那么，在第一个事务中的两次读数据之间，即使第二个事务对数据进行修改，第一个事务两次读到的的数据是一样的。这样就发生了在一个事务内两次读到的数据是一样的，因此称为是可重复读。读取数据的事务将会禁止写事务（但允许读事务），写事务则禁止任何其他事务。这样避免了不可重复读取和脏读，但是有时可能出现幻象读。（读取数据的事务）这可以通过“共享读锁”和“排他写锁”实现。 Serializable（序列化）：提供严格的事务隔离。它要求事务序列化执行，事务只能一个接着一个地执行，但不能并发执行。如果仅仅通过“行级锁”是无法实现事务序列化的，必须通过其他机制保证新插入的数据不会被刚执行查询操作的事务访问到。序列化是最高的事务隔离级别，同时代价也花费最高，性能很低，一般很少使用，在该级别下，事务顺序执行，不仅可以避免脏读、不可重复读，还避免了幻像读。 隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大。对于多数应用程序，可以优先考虑把数据库系统的隔离级别设为Read Committed。它能够避免脏读取，而且具有较好的并发性能。尽管它会导致不可重复读、幻读和第二类丢失更新这些并发问题，在可能出现这类问题的个别场合，可以由应用程序采用悲观锁或乐观锁来控制。大多数数据库的默认级别就是Read committed，比如Sql Server , Oracle。MySQL的默认隔离级别就是Repeatable read。 悲观锁和乐观锁虽然数据库的隔离级别可以解决大多数问题，但是灵活度较差，为此又提出了悲观锁和乐观锁的概念。 悲观锁悲观锁，它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度。因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制。也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统的数据访问层中实现了加锁机制，也无法保证外部系统不会修改数据。 使用场景举例：以MySQL InnoDB为例 商品t_items表中有一个字段status，status为1代表商品未被下单，status为2代表商品已经被下单（此时该商品无法再次下单），那么我们对某个商品下单时必须确保该商品status为1。假设商品的id为1。如果不采用锁，那么操作方法如下： 123456//1.查询出商品信息select status from t_items where id=1;//2.根据商品信息生成订单,并插入订单表 t_orders insert into t_orders (id,goods_id) values (null,1);//3.修改商品status为2update t_items set status=2; 但是上面这种场景在高并发访问的情况下很可能会出现问题。例如当第一步操作中，查询出来的商品status为1。但是当我们执行第三步Update操作的时候，有可能出现其他人先一步对商品下单把t_items中的status修改为2了，但是我们并不知道数据已经被修改了，这样就可能造成同一个商品被下单2次，使得数据不一致。所以说这种方式是不安全的。 使用悲观锁来解决问题 在上面的场景中，商品信息从查询出来到修改，中间有一个处理订单的过程，使用悲观锁的原理就是，当我们在查询出t_items信息后就把当前的数据锁定，直到我们修改完毕后再解锁。那么在这个过程中，因为t_items被锁定了，就不会出现有第三者来对其进行修改了。需要注意的是，要使用悲观锁，我们必须关闭mysql数据库的自动提交属性，因为MySQL默认使用autocommit模式，也就是说，当你执行一个更新操作后，MySQL会立刻将结果进行提交。我们可以使用命令设置MySQL为非autocommit模式：set autocommit=0;设置完autocommit后，我们就可以执行我们的正常业务了。具体如下： 12345678910//0.开始事务begin;/begin work;/start transaction; (三者选一就可以)//1.查询出商品信息select status from t_items where id=1 for update;//2.根据商品信息生成订单insert into t_orders (id,goods_id) values (null,1);//3.修改商品status为2update t_items set status=2;//4.提交事务commit;/commit work; 上面的begin/commit为事务的开始和结束，因为在前一步我们关闭了mysql的autocommit，所以需要手动控制事务的提交。上面的第一步我们执行了一次查询操作：select status from t_items where id=1 for update;与普通查询不一样的是，我们使用了select…for update的方式，这样就通过数据库实现了悲观锁。此时在t_items表中，id为1的那条数据就被我们锁定了，其它的事务必须等本次事务提交之后才能执行。这样我们可以保证当前的数据不会被其它事务修改。需要注意的是，在事务中，只有SELECT … FOR UPDATE 或LOCK IN SHARE MODE 操作同一个数据时才会等待其它事务结束后才执行，一般SELECT … 则不受此影响。拿上面的实例来说，当我执行select status from t_items where id=1 for update;后。我在另外的事务中如果再次执行select status from t_items where id=1 for update;则第二个事务会一直等待第一个事务的提交，此时第二个查询处于阻塞的状态，但是如果我是在第二个事务中执行select status from t_items where id=1;则能正常查询出数据，不会受第一个事务的影响。 Row Lock与Table Lock 使用select…for update会把数据给锁住，不过我们需要注意一些锁的级别，MySQL InnoDB默认Row-Level Lock，所以只有「明确」地指定主键或者索引，MySQL 才会执行Row lock (只锁住被选取的数据) ，否则MySQL 将会执行Table Lock (将整个数据表单给锁住)。举例如下：1、select * from t_items where id=1 for update; 这条语句明确指定主键（id=1），并且有此数据（id=1的数据存在），则采用row lock。只锁定当前这条数据。2、select * from t_items where id=3 for update; 这条语句明确指定主键，但是却查无此数据，此时不会产生lock（没有元数据，又去lock谁呢？）。3、select * from t_items where name='手机' for update; 这条语句没有指定数据的主键，那么此时产生table lock，即在当前事务提交前整张数据表的所有字段将无法被查询。4、select * from t_items where id&gt;0 for update; 或者select * from t_items where id&lt;&gt;1 for update;（注：&lt;&gt;在SQL中表示不等于）上述两条语句的主键都不明确，也会产生table lock。5、select * from t_items where status=1 for update;（假设为status字段添加了索引）这条语句明确指定了索引，并且有此数据，则产生row lock。6、select * from t_items where status=3 for update;（假设为status字段添加了索引）这条语句明确指定索引，但是根据索引查无此数据，也就不会产生lock。 悲观锁小结 悲观锁并不是适用于任何场景，它也有它存在的一些不足，因为悲观锁大多数情况下依靠数据库的锁机制实现，以保证操作最大程度的独占性。如果加锁的时间过长，其他用户长时间无法访问，影响了程序的并发访问性，同时这样对数据库性能开销影响也很大，特别是对长事务而言，这样的开销往往无法承受。所以与悲观锁相对的，我们有了乐观锁。 乐观锁乐观锁（ Optimistic Locking ） 相对悲观锁而言，乐观锁假设认为数据一般情况下不会造成冲突，所以只会在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测，如果发现冲突了，则返回用户错误的信息，让用户决定如何去做。实现乐观锁一般来说有以下2种方式： 使用版本号 使用数据版本（Version）记录机制实现，这是乐观锁最常用的一种实现方式。何谓数据版本？即为数据增加一个版本标识，一般是通过为数据库表增加一个数字类型的 “version” 字段来实现。当读取数据时，将version字段的值一同读出，数据每更新一次，对此version值加一。当我们提交更新的时候，判断数据库表对应记录的当前版本信息与第一次取出来的version值进行比对，如果数据库表当前版本号与第一次取出来的version值相等，则予以更新，否则认为是过期数据。 使用时间戳 乐观锁定的第二种实现方式和第一种差不多，同样是在需要乐观锁控制的table中增加一个字段，名称无所谓，字段类型使用时间戳（timestamp）, 和上面的version类似，也是在更新提交的时候检查当前数据库中数据的时间戳和自己更新前取到的时间戳进行对比，如果一致则OK，否则就是版本冲突。 参考文章: https://www.cnblogs.com/limuzi1994/p/9684083.html","link":"/2021/10/19/mysql/mysql-affairs/"},{"title":"SQL语句不走索引时的排查利器 explain extended + show warnings","text":"索引字段出现隐式字符集转换,索引失效如果索引字段出现隐式字符集转换的话，那么索引将失效，进而转为全表扫描，查询效率将大大降低，要避免出现隐式字符集转换； 隐式字符集转换导致索引失效的原因MySQL索引的数据结构是 B+Tree，想要走索引查询必须要满足其 最左前缀原则 ，否则无法通过索引树进行查找，只能进行全表扫描； 例如：下面的这个SQL由于在 索引字段 上使用函数进行运算，导致索引失效 1select * from t_user where SUBSTR(name, 1, 2) = '李彤' 上面的这个SQL怎么改造才能使索引生效呢？如下所示： 1select * from t_user where name like '李彤%' 通过上面的小例子可以知道，如果在索引字段上使用函数运算，则会导致索引失效，而索引字段的 隐式字符集转换 由于MySQL会自动的在索引字段上加上 转换函数 ，进而会导致索引失效；那接下来我们就通过模拟的实际场景来具体看看是不是由于MySQL自动给加上了转换函数而导致索引失效的； explain extended + show warningsEXTENDED关键字的具体查阅资料：https://dev.mysql.com/doc/refman/5.7/en/explain-extended.html 模拟隐式字符集转换的场景：首先创建两个字符集不一样的表： 1234567891011121314151617181920CREATE TABLE `t_department` ( `id` int(11) NOT NULL AUTO_INCREMENT, `de_no` varchar(32) NOT NULL, `info` varchar(200) DEFAULT NULL, `de_name` varchar(200) DEFAULT NULL, PRIMARY KEY (`id`), KEY `index_de_no` (`de_no`) USING BTREE) ENGINE=InnoDB AUTO_INCREMENT=12 DEFAULT CHARSET=utf8mb4;CREATE TABLE `t_employees` ( `id` int(11) NOT NULL AUTO_INCREMENT, `em_no` varchar(32) NOT NULL, `de_no` varchar(32) NOT NULL, `age` int(11) DEFAULT NULL, `info` varchar(200) DEFAULT NULL, `em_name` varchar(200) DEFAULT NULL, PRIMARY KEY (`id`), KEY `index_em_no` (`de_no`) USING BTREE) ENGINE=InnoDB AUTO_INCREMENT=12 DEFAULT CHARSET=utf8; 然后使用存储过程构造数据： 1234567891011121314151617# 如果存储过程存在则删除DROP PROCEDURE IF EXISTS proc_initData;DELIMITER $# 创建存储过程CREATE PROCEDURE proc_initData()BEGINDECLARE i INT DEFAULT 1;WHILE i&lt;=30 DO# 新增数据INSERT INTO t_employees ( em_no, de_no, info, em_name , age) VALUES ( CONCAT('001', i), '003', 'test11', 'test2', i ); #执行的sql语句SET i = i+1;END WHILE;END $# 调用存储过程CALL proc_initData(); 注意：在构造数据时，记得将 t_employees 表中的 de_no 字段值构造的 离散些 ，因为如果索引字段值的 区分度很低 的话，那么MyQSL优化器通过采样统计分析时，发现索引查询和全表扫描性能差不多，就会直接进行全表扫描了； 索引失效的查询SQL语句： 将表和数据构造完后，我们使用SQL语句进行查询下，然后再看看其执行计划； 12explainselect * from t_department a LEFT JOIN t_employees b on a.de_no = b.de_no where a.id = 16 其执行计划如下：发现 t_employees 表中的 de_no 字段有索引，但是没有走索引查询，type=ALL 走的全表扫描. 使用利器快速排查问题： 注意：explain 后面跟的关键字 EXTENDED（扩展信息） 在MySQL5.7及之后的版本中废弃了，但是该语法仍被识别为向后兼容，所以在5.7版本及后续版本中，可以不用在 explain 后面添加 EXTENDED 了； 具体使用方法如下：1 首先在MySQL的可视化工具中打开一个 命令列介面 ：工具 –&gt; 命令列介面2 然后输入下面的SQL并按回车： 12explain EXTENDEDselect * from t_department a LEFT JOIN t_employees b on a.de_no = b.de_no where a.id = 4019; 3 然后紧接着输入命令 show warnings; 并回车，会出现如下图所示内容：通过展示出的执行SQL扩展信息，发现MySQL在字符集不一致时自动添加上字符集转换函数，因为是在 索引字段 de_no 上添加的转换函数，所以就导致了索引失效；而如果我们没看扩展信息的话，那么可能直到我们查看表结构的时候才会发现是由于字符集不一致导致的，这样就会花费很多的时间； 扩展：隐式类型转换咱们聊完上面的隐式字符集转换导致索引失效的情况，再来简单聊聊另一种 隐式类型转换 导致索引失效的情况； 隐式类型转换：简单的说就是字段的类型与其赋值的类型不一致时会进行隐式的转换； 小例如下： 1select * from t_employees where em_name = 123; 上面的SQL中 em_name 为索引字段，字段类型是 varchar，为其赋 int 类型的值时，会发现索引失效，这里也可以通过 explain extended + show warnings 查看，会发现如下图所示内容：","link":"/2021/10/15/mysql/mysql-explain/"},{"title":"查询SQL具体的执行流程","text":"MySql的整体架构描述 Server层各节点描述Server层中主要由 连接器、查询缓存、解析器/分析器、优化器、执行器 连接器客户端想要对数据库进行操作时，前提是与数据库建立好连接；而连接器就是用来负责跟客户端建立连接、获取权限、维持和管理连接的。 连接方式： 短连接就是操作完毕后，马上close关掉。 长连接可以保持打开，减少服务端创建和释放连接的消耗，后面的程序访问的时候还可以使用这个连接。 一般我们会在连接池中使用长连接。 长连接使用时的注意事项：客户端与服务器建立长连接，默认有效时间是 8小时 ，超过8小时MySql服务器就会将连接断开了，那么客户端再次请求的话，就会报 连接已断开的问题 ；并且保持长连接会消耗内存。长时间不活动的连接，MySQL服务器会断开。 查看长连接的超时时间 12345-- 非交互式超时时间，如 JDBC 程序show global variables like 'wait_timeout'; -- 交互式超时时间，如数据库工具show global variables like' interactive_timeout'; 执行后得到下图结果：默认都是28800秒，8小时 。一般项目中使用的连接池中的连接都是长连接的；（例如：druid、c3p0、dbcp等） 长连接超时断的解决方案 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 查询缓存MySQL缓存是默认关闭的，也就是说不推荐使用缓存，为什么呢？ MySql为什么默认不开启缓存呢？主要是由于它的使用场景限制的： 缓存中数据存储格式：key（sql语句）-value（数据值）；所以如果SQL语句（key）只要存在一点不同之处就会直接进行数据库查询了； 由于表中的数据不是一成不变的，大多数是经常变化的，而当数据库中的数据变化了，那么相应的与此表相关的缓存数据就需要移除掉； MySQL 8.0 版本直接将查询缓存的整块功能删掉了。 解析器/分析器分析器的工作主要是对要执行的SQL语句进行解析，最终得到抽象语法书，然后再使用预处理器判断抽象语法树中的表是否存在，如果存在的话，在接着判断select投影列字段是否在表中存在等。 词法分析词法分析用于将SQL拆解为不可再分的原子符号，称为Token。并根据不同数据库方言所提供的字典，将其归类为关键字，表达式，字面量和操作符。 语法分析语法分析就是根据词法分析拆解出来的Token（原子符号）将SQL语句转换为抽象语法树。下面就直接举例说明，看一个SQL它的抽象语法书到底长神魔样： SQL语句： 1SELECT id, name FROM t_user WHERE status = 'ACTIVE' AND age &gt; 18 然后上面的SQL语句经过词法分析、语法分析后得到的抽象语法书如下：注意，为了便于理解，抽象语法树中的关键字的Token用绿色表示，变量的Token用红色表示，灰色表示需要进一步拆分。 预处理器预处理是用来对生成的 抽象语法树 进行语义校验，语义校验就是对查询的表、select投影列字段进行校验，判断表、字段是否存在等； 优化器优化器的作用：主要是将SQL经过词法解析/语法解析后得到的语法树，通过MySQL的数据字典和统计信息的内容，经过 一系列运算 ，从而得出一个 执行计划 。 在优化过程中，经过的一系列运算是什么呢？下面简单说下： 逻辑变换：例如SQL的where条件中存在 8&gt;9，那逻辑转换就是将语法树中存在的这种常量表达式直接进行化简，化简为 false；除了化简还有常量表达式计算等。 代价优化：就是通过付出一些数据统计分析的代价，来得到这个SQL执行是否可以走索引，以及走哪些索引；除此之外，在多表关联查询中，确定最终表join的顺序等； 在分析是否走索引查询时，是通过进行 动态数据采样统计分析 出来；只要是统计分析出来的，那就可能会存在分析错误的情况，所以在SQL执行不走索引时，也要考虑到这方面的因素。 MySql执行计划怎么查看呢？在执行的SQL语句前添加上 explain 关键字即可； 扩展： Oracle怎么查看执行计划？ 参考此文章 Oracle通过执行计划查看查询语句是否使用索引 执行器MySQL 通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶段，开始执行语句。开始执行的时候，要先判断一下建立连接的对象对这个表有没有执行操作的权限，如果没有，就会返回没有权限的错误；如果有，就按照生成的执行计划进行执行。 通过文章最开始的架构图可知，执行器下面连接的就是存储引擎了，执行器就是通过调用存储引擎提供的API接口进行调用操作数据的。 存储引擎描述存储引擎是对底层物理数据执行实际操作的组件，为Server服务器层提供各种操作数据的 API。MySQL 支持插件式的存储引擎，包括 InnoDB 、MyISAM、Memory 等。一般情况下，MySQL默认使用的存储引擎是 InnoDB 。 InnoDB 存储引擎支持的功能总览 扩展其整体架构图：如下图所示，InnoDB存储引擎整体分为内存架构（Memory Structures）和磁盘架构（Disk Structures）。 深入学习，请参考此文章 你居然还不知道Mysql存储引擎InnoDB分为内存架构、磁盘架构？","link":"/2021/10/15/mysql/mysql-execute-process/"},{"title":"BeanFactory和ApplicationContext的区别","text":"接口 BeanFactory 和 ApplicationContext 都是用来从容器中获取 Spring beans 的，但是，他们二者有很大不同 什么是 Spring BeanSpring beans 就是被 Spring 容器所管理的 Java 对象，来看一个简单的例子 12345678910package com.zoltanraffai; public class HelloWorld { private String message; public void setMessage(String message){ this.message = message; } public void getMessage(){ System.out.println(&quot;My Message : &quot; + message); } } 什么是 Spring 容器Spring 容器负责实例化，配置和装配 Spring beans，下面来看如何为 IoC 容器配置我们的 HelloWorld POJO 123456789&lt;?xml version = &quot;1.0&quot; encoding = &quot;UTF-8&quot;?&gt;&lt;beans xmlns = &quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi = &quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation = &quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;bean id = &quot;helloWorld&quot; class = &quot;com.zoltanraffai.HelloWorld&quot;&gt; &lt;property name = &quot;message&quot; value = &quot;Hello World!&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 现在，它已经被 Spring 容器管理了，接下来的问题是：我们怎样获取它？ BeanFactory 和 ApplicationContext 的不同点BeanFactory 接口这是一个用来访问 Spring 容器的 root 接口，要访问 Spring 容器，我们将使用 Spring 依赖注入功能，使用 BeanFactory 接口和它的子接口特性： Bean 的实例化/串联 通常情况，BeanFactory 的实现是使用懒加载的方式，这意味着 beans 只有在我们通过 getBean() 方法直接调用它们时才进行实例化实现 BeanFactory 最常用的 API 是 XMLBeanFactory这里是如何通过 BeanFactory 获取一个 bean 的例子： 1234567891011package com.zoltanraffai; import org.springframework.core.io.ClassPathResource; import org.springframework.beans.factory.InitializingBean; import org.springframework.beans.factory.xml.XmlBeanFactory; public class HelloWorldApp{ public static void main(String[] args) { XmlBeanFactory factory = new XmlBeanFactory (new ClassPathResource(&quot;beans.xml&quot;)); HelloWorld obj = (HelloWorld) factory.getBean(&quot;helloWorld&quot;); obj.getMessage(); }} ApplicationContext 接口ApplicationContext 是 Spring 应用程序中的中央接口，用于向应用程序提供配置信息它继承了 BeanFactory 接口，所以 ApplicationContext 包含 BeanFactory 的所有功能以及更多功能！它的主要功能是支持大型的业务应用的创建特性： Bean instantiation/wiring Bean 的实例化/串联 自动的 BeanPostProcessor 注册 自动的 BeanFactoryPostProcessor 注册 方便的 MessageSource 访问（i18n） ApplicationEvent 的发布 与 BeanFactory 懒加载的方式不同，它是预加载，所以，每一个 bean 都在 ApplicationContext 启动之后实例化这里是 ApplicationContext 的使用例子： 1234567891011package com.zoltanraffai; import org.springframework.core.io.ClassPathResource; import org.springframework.beans.factory.InitializingBean; import org.springframework.beans.factory.xml.XmlBeanFactory; public class HelloWorldApp{ public static void main(String[] args) { ApplicationContext context=new ClassPathXmlApplicationContext(&quot;beans.xml&quot;); HelloWorld obj = (HelloWorld) context.getBean(&quot;helloWorld&quot;); obj.getMessage(); }} 总结ApplicationContext 包含 BeanFactory 的所有特性，通常推荐使用前者。但是也有一些限制情形，比如移动应用内存消耗比较严苛，在那些情景中，使用更轻量级的 BeanFactory 是更合理的。然而，在大多数企业级的应用中，ApplicationContext 是你的首选。 参考文章 面试还不知道BeanFactory和ApplicationContext的区别？","link":"/2021/11/03/spring/spring-diff-of-BeanFactory-and-ApplicationContext/"},{"title":"python-opencv 安装","text":"install opencv-python123456789# prepare pip3 install scikit-build pip3 install cmake ln -s /usr/bin/ninja /usr/bin/ninja-build# install opencvpip3 install opencv-pythonpip3 install opencv-contrib-python 打开 Python IDLE（或 IPython）并在 Python 终端中键入以下命令。 12import cv2 as cvprint(cv.__version__) install python1234567$ sudo yum install python38 -y# 从SCL中使用python3，你需要一行命令来启用Python3：$ scl enable python38 &lt;command&gt;# 您还可以使用Python编译器来调用一个bash shell:$ scl enable python38 bash OpenCV 中文文档 4.0.0https://www.kancloud.cn/aollo/aolloopencv/259610https://www.bookstack.cn/read/opencv-doc-zh-4.0/README.mdhttps://woshicver.com/ 图像入门1. 显示图像123456789import numpy as npimport cv2 as cv＃加载彩色灰度图像img = cv.imread('messi5.jpg'，0)cv.imshow('image', img)cv.waitKey(0)cv.destroyAllWindows() 使用cv.imread()函数读取图像。图像应该在工作目录或图像的完整路径应给出。 第二个参数是一个标志，它指定了读取图像的方式。 123cv.IMREAD_COLOR： 加载彩色图像。任何图像的透明度都会被忽视。它是默认标志。cv.IMREAD_GRAYSCALE：以灰度模式加载图像cv.IMREAD_UNCHANGED：加载图像，包括alpha通道 使用函数 cv.imshow() 在窗口中显示图像。窗口自动适合图像尺寸。 第一个参数是窗口名称，它是一个字符串。第二个参数是我们的对象。你可以根据需要创建任意多个窗口，但可以使用不同的窗口名称。 cv.waitKey()是一个键盘绑定函数。 其参数是以毫秒为单位的时间。该函数等待任何键盘事件指定的毫秒。如果您在这段时间内按下任何键，程序将继续运行。如果0被传递，它将无限期地等待一次敲击键。它也可以设置为检测特定的按键，例如，如果按下键 a 等，我们将在下面讨论。 注意 除了键盘绑定事件外，此功能还处理许多其他GUI事件，因此你必须使用它来实际显示图像。 cv.destroyAllWindows()只会破坏我们创建的所有窗口。如果要销毁任何特定的窗口，请使用函数cv.destroyWindow()在其中传递确切的窗口名称作为参数。 注意 在特殊情况下，你可以创建一个空窗口，然后再将图像加载到该窗口。在这种情况下，你可以指定窗口是否可调整大小。这是通过功能cv.namedWindow()完成的。默认情况下，该标志为cv.WINDOW_AUTOSIZE。但是，如果将标志指定为cv.WINDOW_NORMAL，则可以调整窗口大小。当图像尺寸过大以及向窗口添加跟踪栏时，这将很有帮助。 2. 写入图像使用函数cv.imwrite()保存图像。 第一个参数是文件名，第二个参数是要保存的图像。 cv.imwrite(‘messigray.png’，img) 这会将图像以PNG格式保存在工作目录中。 在下面的程序中，以灰度加载图像，显示图像，按s保存图像并退出，或者按ESC键直接退出而不保存。1234567891011import numpy as npimport cv2 as cvimg = cv.imread('pig.jpg', 0)cv.imshow('image', img)k = cv.waitKey(0) &amp; 0xFFif k == 27: # 等待ESC退出 cv.destroyAllWindows()elif k == ord('s'): # 等待关键字，保存和退出 cv.imwrite('messigray.png', img) cv.destroyAllWindows() 3. 使用Matplotlibref: https://woshicver.com/ThirdSection/2_1_%E5%9B%BE%E5%83%8F%E5%85%A5%E9%97%A8/Matplotlib是Python的绘图库，可为你提供多种绘图方法。你将在接下来的文章中看到它们。在这里，你将学习如何使用Matplotlib显示图像。你可以使用Matplotlib缩放图像，保存图像等。 123456789import numpy as npimport cv2 as cvfrom matplotlib import pyplot as pltimg = cv.imread('pig.jpg', 0)plt.imshow(img, cmap='gray', interpolation='bicubic')plt.xticks([]), plt.yticks([]) # 隐藏 x 轴和 y 轴上的刻度值plt.show()","link":"/2021/10/25/opencv/opencv-install/"},{"title":"select...for update","text":"作用：select for update 是为了在查询时,避免其他用户以该表进行插入,修改或删除等操作,造成表的不一致性.该语句用来锁定特定的行（如果有where子句，就是满足where条件的那些行）。当这些行被锁定后，其他会话可以选择这些行，但不能更改或删除这些行，直到该语句的事务被commit语句或rollback语句结束为止。 for update的使用场景如果遇到存在高并发并且对于数据的准确性很有要求的场景，是需要了解和使用for update的。 比如涉及到金钱、库存等。一般这些操作都是很长一串并且是开启事务的。如果库存刚开始读的时候是1，而立马另一个进程进行了update将库存更新为0了，而事务还没有结束，会将错的数据一直执行下去，就会有问题。所以需要for upate 进行数据加锁防止高并发时候数据出错。 记住一个原则：一锁二判三更新 SELECT…FOR UPDATE 语句的语法如下：12345SELECT ... FOR UPDATE [OF column_list][WAIT n|NOWAIT][SKIP LOCKED];-- 其中：-- OF 子句用于指定即将更新的列，即锁定行上的特定列。-- WAIT 子句指定等待其他用户释放锁的秒数，防止无限期的等待。 使用”FOR UPDATE WAIT”子句的优点如下： 1 防止无限期地等待被锁定的行； 2 允许应用程序中对锁的等待时间进行更多的控制。 3 对于交互式应用程序非常有用，因为这些用户不能等待不确定 4 若使用了skip locked，则可以越过锁定的行，不会报告由wait n 引发的‘资源忙’异常报告 For Example: select * from t for update 会等待行锁释放之后，返回查询结果。 select * from t for update nowait 不等待行锁释放，提示锁冲突，不返回结果 select * from t for update wait 5 等待5秒，若行锁仍未释放，则提示锁冲突，不返回结果 select * from t for update skip locked 查询返回查询结果，但忽略有行锁的记录 排他锁的申请前提没有线程对该结果集中的任何行数据使用排他锁或共享锁，否则申请会阻塞。 for update仅适用于InnoDB，且必须在事务块(BEGIN/COMMIT)中才能生效。在进行事务操作时，通过“for update”语句，MySQL会对查询结果集中每行数据都添加排他锁，其他线程对该记录的更新与删除操作都会阻塞。排他锁包含行锁、表锁。 场景分析假设有一张商品表 goods，它包含 id，商品名称，库存量三个字段，表结构如下： 1234567CREATE TABLE `goods` (`id` int(11) NOT NULL AUTO_INCREMENT,`name` varchar(100) DEFAULT NULL,`stock` int(11) DEFAULT NULL,PRIMARY KEY (`id`),UNIQUE KEY `idx_name` (`name`) USING HASH) ENGINE=InnoDB 插入如下数据： 123456789INSERT INTO `goods` VALUES ('1', 'prod11', '1000');INSERT INTO `goods` VALUES ('2', 'prod12', '1000');INSERT INTO `goods` VALUES ('3', 'prod13', '1000');INSERT INTO `goods` VALUES ('4', 'prod14', '1000');INSERT INTO `goods` VALUES ('5', 'prod15', '1000');INSERT INTO `goods` VALUES ('6', 'prod16', '1000');INSERT INTO `goods` VALUES ('7', 'prod17', '1000');INSERT INTO `goods` VALUES ('8', 'prod18', '1000');INSERT INTO `goods` VALUES ('9', 'prod19', '1000'); 一、数据一致性假设有A、B两个用户同时各购买一件 id=1 的商品，用户A获取到的库存量为 1000，用户B获取到的库存量也为 1000，用户A完成购买后修改该商品的库存量为 999，用户B完成购买后修改该商品的库存量为 999，此时库存量数据产生了不一致。 有两种解决方案： 悲观锁方案： 每次获取商品时，对该商品加排他锁。也就是在用户A获取获取 id=1 的商品信息时对该行记录加锁，期间其他用户阻塞等待访问该记录。悲观锁适合写入频繁的场景。 1234begin;select * from goods where id = 1 for update;update goods set stock = stock - 1 where id = 1;commit; 乐观锁方案： 每次获取商品时，不对该商品加锁。在更新数据的时候需要比较程序中的库存量与数据库中的库存量是否相等，如果相等则进行更新，反之程序重新获取库存量，再次进行比较，直到两个库存量的数值相等才进行数据更新。乐观锁适合读取频繁的场景。 123456#不加锁获取 id=1 的商品对象select * from goods where id = 1begin;#更新 stock 值，这里需要注意 where 条件 “stock = cur_stock”，只有程序中获取到的库存量与数据库中的库存量相等才执行更新update goods set stock = stock - 1 where id = 1 and stock = cur_stock;commit; 如果我们需要设计一个商城系统，该选择以上的哪种方案呢？ 查询商品的频率比下单支付的频次高，基于以上我可能会优先考虑第二种方案（当然还有其他的方案，这里只考虑以上两种方案）。 二、行锁与表锁InnoDB默认是行级别的锁，当有明确指定的主键时候，是行级锁。否则是表级别。 for update的注意点 for update 仅适用于InnoDB，并且必须开启事务，在begin与commit之间才生效。 要测试for update的锁表情况，可以利用MySQL的Command Mode，开启二个视窗来做测试。 1、只根据主键进行查询，并且查询到数据，主键字段产生行锁。 123begin;select * from goods where id = 1 for update;commit; 2、只根据主键进行查询，没有查询到数据，不产生锁。 123begin;select * from goods where id = 1 for update;commit; 3、根据主键、非主键含索引（name）进行查询，并且查询到数据，主键字段产生行锁，name字段产生行锁。 123begin;select * from goods where id = 1 and name='prod11' for update;commit; 4、根据主键、非主键含索引（name）进行查询，没有查询到数据，不产生锁。 123begin;select * from goods where id = 1 and name='prod12' for update;commit; 5、根据主键、非主键不含索引（name）进行查询，并且查询到数据，如果其他线程按主键字段进行再次查询，则主键字段产生行锁，如果其他线程按非主键不含索引字段进行查询，则非主键不含索引字段产生表锁，如果其他线程按非主键含索引字段进行查询，则非主键含索引字段产生行锁，如果索引值是枚举类型，mysql也会进行表锁，这段话有点拗口，大家仔细理解一下。 123begin;select * from goods where id = 1 and name='prod11' for update;commit; 6、根据主键、非主键不含索引（name）进行查询，没有查询到数据，不产生锁。 123begin;select * from goods where id = 1 and name='prod12' for update;commit; 7、根据非主键含索引（name）进行查询，并且查询到数据，name字段产生行锁。 123begin;select * from goods where name='prod11' for update;commit; 8、根据非主键含索引（name）进行查询，没有查询到数据，不产生锁。 123begin;select * from goods where name='prod11' for update;commit; 9、根据非主键不含索引（stock）进行查询，并且查询到数据，stock字段产生表锁。 123begin;select * from goods where stock='1000' for update;commit; 10、根据非主键不含索引（stock）进行查询，没有查询到数据，stock字段产生表锁。 123begin;select * from goods where stock='2000' for update;commit; 11、只根据主键进行查询，查询条件为不等于，并且查询到数据，主键字段产生表锁。 123begin;select * from goods where id &lt;&gt; 1 for update;commit; 12、只根据主键进行查询，查询条件为不等于，没有查询到数据，主键字段产生表锁。 123begin;select * from goods where id &lt;&gt; 1 for update;commit; 13、只根据主键进行查询，查询条件为 like，并且查询到数据，主键字段产生表锁。 123begin;select * from goods where id like '1' for update;commit; 14、只根据主键进行查询，查询条件为 like，没有查询到数据，主键字段产生表锁。 123begin;select * from goods where id like '1' for update;commit; 测试环境数据库版本：5.1.48-community数据库引擎：InnoDB Supports transactions, row-level locking, and foreign keys数据库隔离策略：REPEATABLE-READ（系统、会话） 总结1、InnoDB行锁是通过给索引上的索引项加锁来实现的，只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁。 2、由于MySQL的行锁是针对索引加的锁，不是针对记录加的锁，所以虽然是访问不同行的记录，但是如果是使用相同的索引键，是会出现锁冲突的。应用设计的时候要注意这一点。 3、当表有多个索引的时候，不同的事务可以使用不同的索引锁定不同的行，另外，不论是使用主键索引、唯一索引或普通索引，InnoDB都会使用行锁来对数据加锁。 4、即便在条件中使用了索引字段，但是否使用索引来检索数据是由MySQL通过判断不同执行计划的代价来决定的，如果MySQL认为全表扫描效率更高，比如对一些很小的表，它就不会使用索引，这种情况下InnoDB将使用表锁，而不是行锁。因此，在分析锁冲突时，别忘了检查SQL的执行计划，以确认是否真正使用了索引。 5、检索值的数据类型与索引字段不同，虽然MySQL能够进行数据类型转换，但却不会使用索引，从而导致InnoDB使用表锁。通过用explain检查两条SQL的执行计划，我们可以清楚地看到了这一点。 参考文章 https://blog.csdn.net/ll594317566/article/details/103869619 https://zhuanlan.zhihu.com/p/143866444 https://www.cnblogs.com/wxgblogs/p/6849064.html","link":"/2021/10/26/mysql/mysql-select-for-update/"},{"title":"linux内核分析——CFS（完全公平调度算法）","text":"参考文章 CFS（完全公平调度算法） Linux进程调度策略（CFS调度）详解 [Linux][Power]CFS调度策略 Linux完全公平调度算法原理与实现 Linux进程调度：完全公平调度器CFS 基本介绍 在 2.5 版本之前，Linux 内核采用传统 UNIX 调度算法。 在内核 V2.5 中，调度程序进行了大改，采用了称为 O(1) 的调度算法，它的运行时间为常量，与系统内任务数量无关。 在内核 V2.6 的开发中，调度程序再次修改；在内核 V2.6.23 的发布中，完全公平调度程序（CFS）成为默认的 Linux 调度算法。 Linux 系统的调度基于调度类。每个类都有一个特定优先级。内核针对不同的调度类，采用不同的调度算法，以便满足系统与进程的需要。Linux 标准内核实现两个调度类：采用 CFS 调度算法的 默认调度类和实时调度类。 CFS 调度程序并不采用严格规则来为一个优先级分配某个长度的时间片，而是为每个任务分配一定比例的 CPU 处理时间。每个任务分配的具体比例是根据友好值来计算的。友好值的范围从 -20 到 +19，数值较低的友好值表示较高的相对优先级。具有较低友好值的任务，与具有较高友好值的任务相比，会得到更高比例的处理器处理时间。默认友好值为 0。 友好一词源自如下想法：当一个任务增加了它的友好值，如从 0 至 +10，该任务通过降低优先级，进而对其他任务更加友好。 CFS 没有使用离散的时间片，而是采用目标延迟，这是每个可运行任务应当运行一次的时间间隔。根据目标延迟，按比例分配 CPU 时间。除了默认值和最小值外，随着系统内的活动任务数量超过了一定阈值，目标延迟可以增加。 CFS 调度程序没有直接分配优先级。相反，它通过每个任务的变量 vruntime 以便维护虚拟运行时间，进而记录每个任务运行多久。虚拟运行时间与基于任务优先级的衰减因子有关，更低优先级的任务比更高优先级的任务具有更高衰减速率。对于正常优先级的任务（友好值为 0），虚拟运行时间与实际物理运行时间是相同的。 因此，如果一个默认优先级的任务运行 200ms，则它的虚拟运行时间也为 200ms。然而，如果一个较低优先级的任务运行 200ms，则它的虚拟运行时间将大于 200ms。同样，如果一个更高优先级的任务运行 200ms，则它的虚拟运行时间将小于 200ms。当决定下步运行哪个任务时，调度程序只需选择具有最小虚拟运行时间的任务。此外，一个更高优先级的任务如成为可运行，就会抢占低优先级任务。 下面分析一下 CFS 调度程序是如何工作的。假设有两个任务，它们具有相同的友好值。一个任务是 I/O 密集型而另一个为 CPU 密集型。通常，I/O 密集型任务在运行很短时间后就会阻塞以便等待更多的 I/O；而 CPU 密集型任务只要有在处理器上运行的机会，就会用完它的时间片。 因此，I/O 密集型任务的虚拟运行时间最终将会小于 CPU 密集型任务的，从而使得 I/O 密集型任务具有更高的优先级。这时，如果 CPU 密集型任务在运行，而 I/O 密集型任务变得有资格可以运行（如该任务所等待的 I/O 已成为可用)，那么 I/O 密集型任务就会抢占 CPU 密集型任务。 Linux 也实现了实时调度。采用 SCHED_FIFO 或 SCHED_RR 实时策略来调度的任何任务，与普通（非实时的）任务相比，具有更高的优先级。 Linux 采用两个单独的优先级范围，一个用于实时任务，另一个用于正常任务。实时任务分配的静态优先级为 0〜99，而正常任务分配的优先级为 100〜139。 这两个值域合并成为一个全局的优先级方案，其中较低数值表明较高的优先级。正常任务，根据它们的友好值，分配一个优先级；这里 -20 的友好值映射到优先级 100，而 +19 的友好 1.1 CFS原理cfs定义了一种新的模型，它给cfs_rq（cfs的run queue）中的每一个进程安排一个虚拟时钟，vruntime。如果一个进程得以执行，随着时间的增长（也就是一个个tick的到来），其vruntime将不断增大。没有得到执行的进程vruntime不变。而调度器总是选择vruntime跑得最慢的那个进程来执行。这就是所谓的“完全公平”。为了区别不同优先级的进程，优先级高的进程vruntime增长得慢，以至于它可能得到更多的运行机会。 1.2 CFS基本设计思路CFS思路很简单，就是根据各个进程的权重分配运行时间(权重怎么来的后面再说)。 进程的运行时间计算公式为: 分配给进程的运行时间 = 调度周期 * 进程权重 / 所有进程权重之和 (公式1) 调度周期: 将所有处于TASK_RUNNING态进程都调度一遍的时间,差不多相当于O(1)调度算法中运行队列和过期队列切换一次的时间。 举个例子，比如只有两个进程A, B，权重分别为1和2，调度周期设为30ms，那么分配给A的CPU时间为:30ms * (1/(1+2)) = 10ms；而B的CPU时间为：30ms * (2/(1+2)) = 20ms。那么在这30ms中A将运行10ms，B将运行20ms。 公平怎么体现呢？它们的运行时间并不一样阿？其实公平是体现在另外一个量上面，叫做**virtual runtime(vruntime)**，它记录着进程已经运行的时间，但是并不是直接记录，而是要根据进程的权重将运行时间放大或者缩小一个比例。我们来看下从实际运行时间到vruntime的换算公式 vruntime = 实际运行时间 * 1024 / 进程权重 。 (公式2) 为了不把大家搞晕，这里我直接写1024，实际上它等于nice为0的进程的权重，代码中是NICE_0_LOAD。也就是说，所有进程都以nice为0的进程的权重1024作为基准，计算自己的vruntime增加速度。 还以上面AB两个进程为例，B的权重是A的2倍，那么B的vruntime增加速度只有A的一半。 现在我们把公式2中的实际运行时间用公式1来替换，可以得到这么一个结果： vruntime = (调度周期 * 进程权重 / 所有进程总权重) * 1024 / 进程权重 = 调度周期 * 1024 / 所有进程总权重 看出什么眉目没有？没错，虽然进程的权重不同，但是它们的 vruntime增长速度应该是一样的 ，与权重无关。好，既然所有进程的vruntime增长速度宏观上看应该是同时推进的， 那么就可以用这个vruntime来选择运行的进程，谁的vruntime值较小就说明它以前占用cpu的时间较短，受到了“不公平”对待，因此下一个运行进程就是它。这样既能公平选择进程，又能保证高优先级进程获得较多的运行时间。这就是CFS的主要思想了。 或者可以这么理解：CFS的思想就是让每个调度实体（没有组调度的情形下就是进程，以后就说进程了）的vruntime互相追赶，而每个调度实体的vruntime增加速度不同，权重越大的增加的越慢，这样就能获得更多的cpu执行时间。 再补充一下权重的来源，权重跟进程nice值之间有一一对应的关系，可以通过全局数组prio_to_weight来转换，nice值越大，权重越低。 1.3 CFS数据结构介绍代码之前先介绍一下CFS相关的结构第一个是调度实体sched_entity，它代表一个调度单位，在组调度关闭的时候可以把他等同为进程。每一个task_struct中都有一个sched_entity，进程的vruntime和权重都保存在这个结构中。那么所有的sched_entity怎么组织在一起呢？红黑树。所有的sched_entity以vruntime为key(实际上是以vruntime - min_vruntime为key，是为了防止溢出， 反正结果是一样的)插入到红黑树中，同时缓存树的最左侧节点，也就是vruntime最小的节点，这样可以迅速选中vruntime最小的进程。注意只有等待CPU的就绪态进程在这棵树上，睡眠进程和正在运行的进程都不在树上。 红黑树： 红黑树是自平衡的，没有路径比其它任何路径长两倍以上。树上运行按O(log n)时间发生（n是树中节点的数量），可以快速高效的插入或者删除任务。 任务存储在以时间为顺序的红黑树中（由 sched_entity 对象表示），对处理器需求最多的任务 （最低虚拟运行时）存储在树的左侧，处理器需求最少的任务（最高虚拟运行时）存储在树的右侧。为了公平，调度器然后选取红黑树最左端的节点调度为下一个以便保持公平性。任务通过将其运行时间添加到虚拟运行时，说明其占用 CPU 的时间，然后如果可运行，再插回到树中。这样，树左侧的任务就被给予时间运行了，树的内容从右侧迁移到左侧以保持公平。 因此，每个可运行的任务都会追赶其他任务以维持整个可运行任务集合的执行平衡。 1.4 Vruntime溢出问题之前说过红黑树中实际的作为key的不是vruntime而是vruntime - min_vruntime。min_vruntime是当前红黑树中最小的key。这是为什么呢，我们先看看vruntime的类型，是usigned long类型的，再看看key的类型，是signed long类型的，因为进程的虚拟时间是一个递增的正值，因此它不会是负数，但是它有它的上限，就是unsigned long所能表示的最大值，如果溢出了，那么它就会从0开始回滚，如果这样的话，结果会怎样？结果很严重啊，就是说会本末倒置的，比如以下例子，以unsigned char说明问题： 123unsigned char a = 251, b = 254;b += 5;//到此判断a和b的大小 看看上面的例子，b回滚了，导致a远远大于b，其实真正的结果应该是b比a大8，怎么做到真正的结果呢？改为以下： 12345unsigned char a = 251, b = 254;b += 5;signed char c = a - 250,d = b - 250;//到此判断c和d的大小 结果正确了，要的就是这个效果，可是进程的vruntime怎么用unsigned long类型而不处理溢出问题呢？因为这个vruntime的作用就是推进虚拟时钟，并没有别的用处，它可以不在乎，然而在计算红黑树的key的时候就不能不在乎了，于是减去一个最小的vruntime将所有进程的key围绕在最小vruntime的周围，这样更加容易追踪。运行队列的min_vruntime的作用就是处理溢出问题的。 1.5 组调度引入组调度是为了实现做一件事的一组进程与做另一件事的另一组进程的隔离。每件“事情”各自有自己的权重，而不管它需要使用多少进程来完成。在cfs中，task_group和进程是同等对待的，task_group的优先级也由用户来控制（通过cgroup文件cpu.shares）。实现上，task_group和进程都被抽象成schedule_entity（调度实体，以下简称se），上面说到的vruntime、load、等这些东西都被封装在se里面。而task_group除了有se之外，还有cfs_rq。属于这个task_group的进程就被装在它的cfs_rq中（“组”不仅是一个被调度的实体，也是一个容器）。组调度引入以后，一系列task_group的cfs_rq组成了一个树型结构。树根是cpu所对应的cfs_rq（也就是root group的cfs_rq）、树的中间节点是各个task_group的cfs_rq、 叶子节点是各个进程。在一个task_group的两头，是两个不同的世界，就像《盗梦空间》里不同层次的梦境一样。 1.6 CFS小结CFS还有一个重要特点，即调度粒度小。CFS之前的调度器中，除了进程调用了某些阻塞函数而主动参与调度之外，每个进程都只有在用完了时间片或者属于自己的时间配额之后才被抢占。而CFS则在每次tick都进行检查，如果当前进程不再处于红黑树的左边，就被抢占。在高负载的服务器上，通过调整调度粒度能够获得更好的调度性能。","link":"/2021/09/23/linux/linux_cfs/"},{"title":"spring 事务","text":"spring 事务传播属性 常量 解释 PROPAGATION_REQUIRED 支持当前事务，如果当前没有事务，就新建一个事务。默认的传播属性。 PROPAGATION_REQUIRES_NEW 新建事务，如果当前存在事务，把当前事务挂起。 PROPAGATION_SUPPORTS 支持当前事务，如果当前没有事务，就以非事务方式执行。 PROPAGATION_MANDATORY 支持当前事务，如果当前没有事务，就抛出异常。 PROPAGATION_NOT_SUPPORTED 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER 以非事务方式执行，如果当前存在事务，则抛出异常。 PROPAGATION_NESTED 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则进行与PROPAGATION_REQUIRED类似的操作。 前六个策略类似于EJB CMT，第七个（PROPAGATION_NESTED）是Spring所提供的一个特殊变量。它要求事务管理器或者使用JDBC 3.0 Savepoint API提供嵌套事务行为（如Spring的DataSourceTransactionManager） 事务的传播机制事务的传播性一般用在事务嵌套的场景，比如一个事务方法里面调用了另外一个事务方法，那么两个方法是各自作为独立的方法提交还是内层的事务合并到外层的事务一起提交，这就是需要事务传播机制的配置来确定怎么样执行。常用的事务传播机制如下： PROPAGATION_REQUIRED 支持当前事务，如果当前没有事务，就新建一个事务 Spring默认的传播机制。 如果外层有事务，则当前事务加入到外层事务，一块提交，一块回滚。如果外层没有事务，新建一个事务执行 explain:123456789ServiceA { void methodA() { //外部事务 ServiceB.methodB(); } } ServiceB { void methodB() { //内部事务 } } ServiceB.methodB的事务级别定义为PROPAGATION_REQUIRED, 那么由于执行ServiceA.methodA的时候1、如果ServiceA.methodA已经起了事务，这时调用ServiceB.methodB，ServiceB.methodB看到自己已经运行在ServiceA.methodA的事务内部，就不再起新的事务。这时只有外部事务并且他们是共用的，所以这时ServiceA.methodA或者ServiceB.methodB无论哪个发生异常methodA和methodB作为一个整体都将一起回滚。2、如果ServiceA.methodA没有事务，ServiceB.methodB就会为自己分配一个事务。这样，在ServiceA.methodA中是没有事务控制的。只是在ServiceB.methodB内的任何地方出现异常，ServiceB.methodB将会被回滚，不会引起ServiceA.methodA的回滚 PROPAGATION_REQUES_NEW该事务传播机制是每次都会新开启一个事务，同时把外层事务挂起，当当前事务执行完毕，恢复上层事务的执行。如果外层没有事务，执行当前新开启的事务即可 新建的事务和挂起的事务是两个独立的事务。1.标志REQUIRES_NEW会新开启事务，外层事务不会影响内部事务的提交/回滚2.标志REQUIRES_NEW的内部事务的异常，被外部事务捕获，也可以不处理回滚操作 如果设计ServiceA.methodA事务级别定义为PROPAGATION_REQUIRED，ServiceB.methodB的事务级别定义为PROPAGATION_REQUIRED_NEW,那么，当执行到ServiceB.methodB的时候, ServiceA.methodA所在的事务就会挂起，ServiceB.methodB会发起一个新事务， 等待 ServiceB.methodB的事务完成以后，挂起的事务才会继续执行。 它与PROPAGATION_REQUIRED的区别在于事务的回滚程度。因为ServiceB.methodB新发起一个事务，存在两个不同的事务。如果ServiceB.methodB 已经提交，那么 ServiceA.methodA 回滚失败时 ServiceB.methodB 是不会回滚的。 如果ServiceB.methodB 回滚失败，他抛出的异常被ServiceA.methodA 捕获，ServiceA.methodA的事务任然可能提交（主要看ServiceB.methodB抛出的异常是不是ServiceA.methodA会回滚的异常 ）。 PROPAGATION_SUPPORTS如果当前在事务中，即以事务的形式运行，如果当前不再一个事务中，那么就以非事务的形式运行 PROPAGATION_NOT_SUPPORT该传播机制不支持事务，如果外层存在事务则挂起，执行完当前代码，则恢复外层事务，无论是否异常都不会回滚当前的代码 当前不支持事务。比如ServiceA.methodA的事务级别是PROPAGATION_REQUIRED ，而ServiceB.methodB的事务级别是PROPAGATION_NOT_SUPPORTED ，那么当执行到ServiceB.methodB时，ServiceA.methodA的事务挂起，而他以非事务的状态运行完，再继续ServiceA.methodA的事务。 PROPAGATION_NEVER该传播机制不支持外层事务，即如果外层有事务就抛出异常 不能在事务中运行。假设ServiceA.methodA的事务级别是PROPAGATION_REQUIRED， 而ServiceB.methodB的事务级别是PROPAGATION_NEVER ，那么ServiceB.methodB就要抛出异常了。 PROPAGATION_MANDATORY必须在一个事务中运行。也就是说，他只能被一个父事务调用。否则，他就要抛出异常 PROPAGATION_NESTED如果有活动的事务，则运行在一个嵌套的事务中。如果没有活动事务，则按required属性执行。它使用了一个单独的事务，这个事务拥有多个可以回滚的保存点。内部事务的回滚不会对外部事务造成影响。它只对DataSourceTransactionManager事务管理器有效。 该传播机制的特点是可以保存状态保存点，当前事务回滚到某一个点，从而避免所有的嵌套事务都回滚，即各自回滚各自的，如果子事务没有把异常吃掉，基本还是会引起全部回滚的。 比如我们设计ServiceA.methodA的事务级别为PROPAGATION_REQUIRED，ServiceB.methodB的事务级别为PROPAGATION_NESTED，那么当执行到ServiceB.methodB的时候，ServiceA.methodA所在的事务就会挂起，ServiceB.methodB会起一个新的子事务并设置savepoint，等待ServiceB.methodB的事务完成以后，他才继续执行。。因为ServiceB.methodB是外部事务的子事务，那么1、如果ServiceB.methodB已经提交，那么ServiceA.methodA失败回滚，ServiceB.methodB也将回滚。2、如果ServiceB.methodB失败回滚，如果他抛出的异常被ServiceA.methodA的try..catch捕获并处理，ServiceA.methodA事务仍然可能提交；如果他抛出的异常未被ServiceA.methodA捕获处理，ServiceA.methodA事务将回滚。 理解Nested的关键是savepoint。他与PROPAGATION_REQUIRES_NEW的区别是：PROPAGATION_REQUIRES_NEW 完全是一个新的事务,它与外部事务相互独立；而 PROPAGATION_NESTED 则是外部事务的子事务, 如果外部事务 commit, 嵌套事务也会被 commit, 这个规则同样适用于 roll back. 在 spring 中使用 PROPAGATION_NESTED的前提： 我们要设置 transactionManager 的 nestedTransactionAllowed 属性为 true, 注意, 此属性默认为 false!!! java.sql.Savepoint 必须存在, 即 jdk 版本要 1.4+ Connection.getMetaData().supportsSavepoints() 必须为 true, 即 jdbc drive 必须支持 JDBC 3.0 确保以上条件都满足后, 你就可以尝试使用 PROPAGATION_NESTED 了. 事务的隔离级别事务的隔离级别定义一个事务可能受其他并发务活动活动影响的程度，可以把事务的隔离级别想象为这个事务对于事物处理数据的自私程度。 在一个典型的应用程序中，多个事务同时运行，经常会为了完成他们的工作而操作同一个数据。并发虽然是必需的，但是会导致以下问题： 脏读（Dirty read）脏读发生在一个事务读取了被另一个事务改写但尚未提交的数据时。如果这些改变在稍后被回滚了，那么第一个事务读取的数据就会是无效的。 不可重复读（Nonrepeatable read）不可重复读发生在一个事务执行相同的查询两次或两次以上，但每次查询结果都不相同时。这通常是由于另一个并发事务在两次查询之间更新了数据。不可重复读重点在修改。 幻读（Phantom reads）幻读和不可重复读相似。当一个事务（T1）读取几行记录后，另一个并发事务（T2）插入了一些记录时，幻读就发生了。在后来的查询中，第一个事务（T1）就会发现一些原来没有的额外记录。幻读重点在新增或删除。 在理想状态下，事务之间将完全隔离，从而可以防止这些问题发生。然而，完全隔离会影响性能，因为隔离经常涉及到锁定在数据库中的记录（甚至有时是锁表）。完全隔离要求事务相互等待来完成工作，会阻碍并发。因此，可以根据业务场景选择不同的隔离级别。 隔离级别 含义 ISOLATION_DEFAULT 使用后端数据库默认的隔离级别 ISOLATION_READ_UNCOMMITTED 允许读取尚未提交的更改。可能导致脏读、幻读或不可重复读。 ISOLATION_READ_COMMITTED （Oracle 默认级别） 允许从已经提交的并发事务读取。可防止脏读，但幻读和不可重复读仍可能会发生。 ISOLATION_REPEATABLE_READ （MYSQL默认级别） 对相同字段的多次读取的结果是一致的，除非数据被当前事务本身改变。可防止脏读和不可重复读，但幻读仍可能发生。 ISOLATION_SERIALIZABLE 完全服从ACID的隔离级别，确保不发生脏读、不可重复读和幻影读。这在所有隔离级别中也是最慢的，因为它通常是通过完全锁定当前事务所涉及的数据表来完成的。 只读如果一个事务只对数据库执行读操作，那么该数据库就可能利用那个事务的只读特性，采取某些优化措施。通过把一个事务声明为只读，可以给后端数据库一个机会来应用那些它认为合适的优化措施。由于只读的优化措施是在一个事务启动时由后端数据库实施的， 因此，只有对于那些具有可能启动一个新事务的传播行为（PROPAGATION_REQUIRES_NEW、PROPAGATION_REQUIRED、 ROPAGATION_NESTED）的方法来说，将事务声明为只读才有意义。 事务超时为了使一个应用程序很好地执行，它的事务不能运行太长时间。因此，声明式事务的下一个特性就是它的超时。 假设事务的运行时间变得格外的长，由于事务可能涉及对数据库的锁定，所以长时间运行的事务会不必要地占用数据库资源。这时就可以声明一个事务在特定秒数后自动回滚，不必等它自己结束。 由于超时时钟在一个事务启动的时候开始的，因此，只有对于那些具有可能启动一个新事务的传播行为（PROPAGATION_REQUIRES_NEW、PROPAGATION_REQUIRED、ROPAGATION_NESTED）的方法来说，声明事务超时才有意义。 回滚规则在默认设置下，事务只在出现运行时异常（runtime exception）时回滚，而在出现受检查异常（checked exception）时不回滚（这一行为和EJB中的回滚行为是一致的）。不过，可以声明在出现特定受检查异常时像运行时异常一样回滚。同样，也可以声明一个事务在出现特定的异常时不回滚，即使特定的异常是运行时异常。 Spring声明式事务配置参考事物配置中有哪些属性可以配置?以下只是简单的使用参考 事务的传播性：@Transactional(propagation=Propagation.REQUIRED) 事务的隔离级别：@Transactional(isolation = Isolation.READ_UNCOMMITTED) 读取未提交数据(会出现脏读, 不可重复读) 基本不使用 只读：@Transactional(readOnly=true) 该属性用于设置当前事务是否为只读事务，设置为true表示只读，false则表示可读写，默认值为false。 事务的超时性：@Transactional(timeout=30) 回滚： 指定单一异常类：@Transactional(rollbackFor=RuntimeException.class) 指定多个异常类：@Transactional(rollbackFor={RuntimeException.class, Exception.class})","link":"/2021/10/19/spring/spring-affairs/"},{"title":"spring中的@PostConstruct注解的用法","text":"@PostConstruct是java5的时候引入的注解，指的是在项目启动的时候执行这个方法，也可以理解为在spring容器启动的时候执行，可作为一些数据的常规化加载，比如数据字典之类的。 @PostConstruct注解使用简介简单起见，我们准备一个springboot项目快速启动。项目目录结构如下： 下面我们在cn.lay.postconstruct目录下创建一个类，并添加一个@PostConstruct的方法，如 最后，我们执行PostConstructApplication的main方法，启动项目。在控制台里，我们会看到 到这里，我们可以知道@PostConstruct注解的用途了。当一个class被注解为一个Bean，那么class上被@PostConstruct注解的方法将会在程序启动的时候执行。 知道了如何使用@PostConstruct以后，我们会产生疑问。为什么@PostConstruct注解的方法会在程序启动的时候执行呢？后续的内容将为你解开疑惑。 @PostConstruct原理 ref: https://www.cnblogs.com/lay2017/p/11735802.html 转载自：https://www.cnblogs.com/mark5/p/12767120.html被@PostConstruct修饰的方法会在服务器加载Servle的时候运行，并且只会被服务器执行一次。PostConstruct在构造函数之后执行也就是加载顺序 服务器加载Servlet -&gt; servlet 构造函数的加载 -&gt; postConstruct -&gt;init（init是在service 中的初始化方法. 创建service 时发生的事件.） -&gt;Service-&gt;destory-&gt;predestory-&gt;服务器卸载serlvet 那么问题：spring中Constructor、@Autowired、@PostConstruct的顺序 Constructor &gt;&gt; @Autowired &gt;&gt; @PostConstruct 依赖注入的字面意思就可以知道，要将对象p注入到对象a，那么首先就必须得生成对象p与对象a，才能执行注入。所以，如果一个类A中有个成员变量p被@Autowired注解，那么@Autowired注入是发生在A的构造方法执行完之后的。 @PostConstruct应用场景：如果想在生成对象时候完成某些初始化操作，而偏偏这些初始化操作又依赖于依赖注入，那么就无法在构造函数中实现。为此，可以使用@PostConstruct注解一个方法来完成初始化，@PostConstruct注解的方法将会在依赖注入完成后被自动调用。 参考文章 https://www.cnblogs.com/mark5/p/12767120.html https://www.cnblogs.com/lay2017/p/11735802.html","link":"/2021/11/03/spring/spring-postconstruct/"},{"title":"opencv 练习","text":"OpenCV中的绘图功能12345678910111213141516171819202122232425import cv2 as cvimport numpy as np# 创建黑色的图像img = np.zeros((512, 512, 3), np.uint8)# 绘制一条厚度为5的蓝色对角线cv.line(img, (0, 0), (511, 511), (255, 0, 0), 5)cv.rectangle(img, (384, 0), (510, 128), (0, 255, 0), 3)cv.circle(img, (447, 63), 63, (0, 0, 255), -1)cv.ellipse(img, (256, 256), (100, 50), 0, 0, 180, 255, -1)pts = np.array([[10, 5], [20, 30], [70, 20], [50, 10]], np.int32)pts = pts.reshape((-1, 1, 2))cv.polylines(img, [pts], True, (0, 255, 255))font = cv.FONT_HERSHEY_SIMPLEXcv.putText(img, 'OpenCV', (10, 500), font, 4, (255, 255, 255), 2, cv.LINE_AA)cv.imshow('image', img)cv.waitKey(0)cv.destroyAllWindows()","link":"/2021/10/25/opencv/opencv-practice/"},{"title":"Spring — Dynamically register beans in 4 ways At Run-Time","text":"使用 GenericBeanDefinition 进行动态 Bean 注册GenericBeanDefinition 是用于标准 bean 定义目的的一站式商店。像任何 bean 定义一样，它允许指定一个类以及可选的构造函数参数值和属性值。此外，可以通过“ parentName ”属性灵活地配置从父 bean 定义派生。通常，使用这个GenericBeanDefinition类来注册用户可见的 bean 定义（后处理器可能会对其进行操作，甚至可能重新配置父名称）。使用RootBeanDefinition / ChildBeanDefinition，其中父/子关系恰好是预先确定的。 123456789public class MyBean { private Date date; public void doSomething () { System.out.println(&quot;from my bean, date: &quot; + date); } public void setDate (Date date) { this.date = date; }} 注册上面的，使用GenericBeanDefinition动态创建的 bean 。 12345678910111213141516public class GenericBeanDefinitionExample { public static void main (String[] args) { DefaultListableBeanFactory context = new DefaultListableBeanFactory(); GenericBeanDefinition gbd = new GenericBeanDefinition(); gbd.setBeanClass(MyBean.class); MutablePropertyValues mpv = new MutablePropertyValues(); mpv.add(&quot;date&quot;, new Date()); //alternatively we can use: // gbd.getPropertyValues().addPropertyValue(&quot;date&quot;, new Date()); gbd.setPropertyValues(mpv); context.registerBeanDefinition(&quot;myBeanName&quot;, gbd); MyBean bean = context.getBean(MyBean.class); bean.doSomething(); }} Output: 1from my bean, date: Wed Jult 23 12:20:58 EDT 2019 使用 BeanDefinitionBuilder 动态注册 Bean使用构建器模式构建BeanDefinitions 的编程方法。主要用于在实现 Spring 2.0 NamespaceHandlers 时使用。这里唯一的区别是， BeanDefinitionBuilder 使用Builder Pattern。 创建另一个 bean 类。 123456789public class MyBean { private String str; public void setStr (String str) { this.str = str; } public void doSomething () { System.out.println(&quot;from MyBean &quot; + str); }} 使用BeanDefinitionBuilder动态注册 bean 的示例。 123456789101112public class BeanDefinitionBuilderExample { public static void main (String[] args) { DefaultListableBeanFactory beanFactory = new DefaultListableBeanFactory(); BeanDefinitionBuilder b = BeanDefinitionBuilder.rootBeanDefinition(MyBean.class) .addPropertyValue(&quot;str&quot;, &quot;myStringValue&quot;); beanFactory.registerBeanDefinition(&quot;myBean&quot;, b.getBeanDefinition()); MyBean bean = beanFactory.getBean(MyBean.class); bean.doSomething(); }} Output: 12from MyBean myStringValue 使用 BeanDefinitionBuilder 注入其他 bean 引用Creating Bean 1 123456789public class Bean1 { private Bean2 otherBean; public void setOtherBean (Bean2 otherBean) { this.otherBean = otherBean; } public void doSomething () { otherBean.doSomething(); }} Creating Bean 2 12345public class Bean2 { public void doSomething () { System.out.println(&quot;from other bean &quot;); }} Setting the Bean2 in Bean1 1234567891011121314151617181920public class InjectingOtherBeans { public static void main (String[] args) { DefaultListableBeanFactory context = new DefaultListableBeanFactory(); //define and register MyOtherBean GenericBeanDefinition beanOtherDef = new GenericBeanDefinition(); beanOtherDef.setBeanClass(Bean2.class); context.registerBeanDefinition(&quot;other&quot;, beanOtherDef); //definine and register myBean GenericBeanDefinition beanDef = new GenericBeanDefinition(); beanDef.setBeanClass(Bean1.class); MutablePropertyValues mpv = new MutablePropertyValues(); mpv.addPropertyValue(&quot;otherBean&quot;, context.getBean(&quot;other&quot;)); beanDef.setPropertyValues(mpv); context.registerBeanDefinition(&quot;myBean&quot;, beanDef); //using MyBean instance MyBean bean = context.getBean(MyBean.class); bean.doSomething(); }} Output: 1from other bean Dynamic Bean Registration With BeanFactoryPostProcessor一个BeanFactoryPostProcessor的可交互和修改bean定义，但从来没有bean实例。允许自定义修改应用程序上下文的 bean 定义，调整上下文底层 bean 工厂的 bean 属性值。应用程序上下文可以在它们的 bean 定义中自动检测BeanFactoryPostProcessor bean，并在创建任何其他 bean 之前应用它们。 创建配置1234567@Configuration public class MyConfig { @Bean MyConfigBean myConfigBean () { return new MyConfigBean(); } } BeanFactoryPostProcessor允许客户端代码自定义 bean 定义。方法BeanFactoryPostProcessor.postProcessBeanFactory在所有 bean 定义加载后由 Spring 启动过程调用，但尚未实例化任何 bean。 123456789101112public class MyConfigBean implements BeanFactoryPostProcessor { @Override public void postProcessBeanFactory ( ConfigurableListableBeanFactory beanFactory) throws BeansException { GenericBeanDefinition bd = new GenericBeanDefinition(); bd.setBeanClass(MyBean.class); bd.getPropertyValues().add(&quot;strProp&quot;, &quot;my string property&quot;); ((DefaultListableBeanFactory) beanFactory) .registerBeanDefinition(&quot;myBeanName&quot;, bd); }} BeanFactoryPostProcessor 示例的主类。 123456789public class MyBean { private String strProp; public void setStrProp (String strProp) { this.strProp = strProp; } public void doSomething () { System.out.println(&quot;from MyBean: &quot; + strProp); }} Main class for BeanFactoryPostProcessor Example. 12345678public class BeanFactoryPostProcessorExample { public static void main (String[] args) { AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(MyConfig.class); MyBean bean = context.getBean(MyBean.class); bean.doSomething(); }} Output: 123from MyBean: my string propertyWARNING: @Bean method MyConfig.myConfigBean is non-static and returns an object assignable to Spring's BeanFactoryPostProcessor interface. This will result in a failure to process annotations such as @Autowired, @Resource and @PostConstruct within the method's declaring @Configuration class. Add the 'static' modifier to this method to avoid these container lifecycle issues; see @Bean javadoc for complete details. 使用 BeanFactoryPostProcessor 动态注册 Bean对标准BeanFactoryPostProcessor SPI 的扩展，允许在常规 BeanFactoryPostProcessor 检测开始之前注册进一步的 bean 定义。特别是，BeanDefinitionRegistryPostProcessor可以注册进一步的 bean 定义，进而定义BeanFactoryPostProcessor实例。 Creating another config class. 1234567@Configurationpublic class MyConfig { @Bean MyConfigBean myConfigBean () { return new MyConfigBean(); }} 这是BeanFactoryPostProcessor的子接口（最后一个例子）。它允许注册 bean 定义。它的方法postProcessBeanDefinitionRegistry在BeanFactoryPostProcessor#postProcessBeanFactory之前被调用。该接口更侧重于 BeanDefinition 注册而不是通用BeanFactoryPostProcessor。为 BeanDefinitionRegistryPostProcessor 创建实现类。 123456789101112131415public class MyConfigBean implements BeanDefinitionRegistryPostProcessor { @Override public void postProcessBeanDefinitionRegistry (BeanDefinitionRegistry registry) throws BeansException { GenericBeanDefinition bd = new GenericBeanDefinition(); bd.setBeanClass(MyBean.class); bd.getPropertyValues().add(&quot;strProp&quot;, &quot;my string property&quot;); registry.registerBeanDefinition(&quot;myBeanName&quot;, bd); } @Override public void postProcessBeanFactory (ConfigurableListableBeanFactory beanFactory) throws BeansException { //no op }} Creating a brand new Bean class. 123456789public class MyBean { private String strProp; public void setStrProp (String strProp) { this.strProp = strProp; } public void doSomething () { System.out.println(&quot;from MyBean: &quot; + strProp); }} Main class for BeanDefinitionRegistryPostProcessor Example. 12345678public class BeanDefinitionRegistryPostProcessorExample { public static void main (String[] args) { AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(MyConfig.class); MyBean bean = (MyBean) context.getBean(&quot;myBeanName&quot;); bean.doSomething(); }} Output: 123from MyBean: my string propertyWARNING: Cannot enhance @Configuration bean definition 'beanDefinitionRegistryPostProcessorExample.MyConfig' since its singleton instance has been created too early. The typical cause is a non-static @Bean method with a BeanDefinitionRegistryPostProcessor return type: Consider declaring such methods as 'static'. Unregistering the Bean at run timeIf we need to remove registered beans at runtime, we can do the same as below.To remove or unregister the bean from spring context. 1beanRegistry.removeBeanDefinition(&quot;bean&quot;) To delete/clear the singleton bean from context. 1beanRegistry.destroySingleton(&quot;bean&quot;) 参考文章 https://medium.com/@venkivenki4b6/spring-dynamically-register-beans-in-4-ways-at-run-time-c1dc45dcbeb9 https://www.jianshu.com/p/25939d9ce832 https://www.cnblogs.com/monument/p/12933915.html SpringBoot基础篇Bean之动态注册","link":"/2021/11/03/spring/spring-register-bean-at-run-time/"},{"title":"wsl常用命令","text":"参考文章 https://docs.microsoft.com/zh-cn/windows/wsl/reference 设置默认版本wsl --set-default-version 2 检查分配给每个已安装的 Linux 分发版的 WSL 版本wsl -l -vwsl --list --verbose 将分发版设置为受某一 WSL 版本支持wsl --set-version &lt;distribution name&gt; &lt;versionNumber&gt; 运行/停止Ubuntu子系统wsl -l 列出了系统中安装的子系统名称，可以是一个或多个，本文中的子系统名称是Ubuntu-18.04-20190707，接下来针对这个默认子系统进行操作： 运行子系统wsl --distribution Ubuntu-18.04-20190707或者wsl -d Ubuntu-18.04-20190707 查看运行中的子系统-l --running12 适用于 Linux 的 Windows 子系统:Ubuntu-18.04-20190707 (默认) 停止子系统wsl -t Ubuntu-18.04-20190707或者wsl --terminate Ubuntu-18.04-20190707 备份/删除/还原子系统备份子系统非常简单，但一定要先停止子系统之后再备份wsl --export Ubuntu-18.04-20190707 c:\\temp\\Ubuntu-18.04-20190707.tar等待完成即可。备份成功后，子系统会被打包成命令中指定的tar文件。 删除子系统也是一个命令即可：wsl --unregister Ubuntu-18.04-20190707这样WSL子系统就从Windows中删除的干干净净了。 还原子系统删除了没关系，刚才做了备份，也是一个命令还原：wsl --import Ubuntu-18.04-20190707 c:\\WSL c:\\temp\\Ubuntu-18.04-20190707.tar这里注意指定还原的路径。成功后，子系统又回来了，可以用wsl -l确认一下。 用于运行 Linux 命令的参数 不带参数 如果未提供命令行，wsl.exe 将启动默认 shell。 –exec, -e 执行指定的命令，但不使用默认的 Linux shell。 按原样传递剩余的命令行。 上述命令也接受以下选项： –distribution, -d 运行指定的分发版。 –user, -u 以指定用户的身份运行。 用于管理适用于 Linux 的 Windows 子系统的参数 –export 将分发版导出到 tar 文件。 在标准输出中，文件名可以是 -。 –import 导入指定的 tar 文件作为新的分发版。 在标准输入中，文件名可以是 -。 –list、-l [选项] 列出分发版。 选项： –all 列出所有分发版，包括当前正在安装或卸载的分发版。 –verbose, -v 显示命令的附加信息或展开的详细信息。 –running 仅列出当前正在运行的分发版。 –set-default, -s 将分发版设置为默认值。 –terminate, -t 终止指定的分发版。 –unregister 注销分发版。 –help 显示用法信息。","link":"/2021/09/27/wsl/wsl-cmd/"},{"title":"wsl install ubuntu","text":"参考文章 https://blog.csdn.net/weixin_45883933/article/details/106085184 安装前配置ref: 适用于 Linux 的 Windows 子系统安装指南 (Windows 10) 1. 启用 Windows 功能搜索并打开“启用或关闭 Windows 功能”，然后选择“适用于Linux的Windows子系统”复选框。 在windows功能中重新勾选hyper-v然后开启hyper-v模式在管理员powershell中执行 1bcdedit /set hypervisorlaunchtype auto 如果禁用了组策略里面的Device Guard虚拟化安全设置，需要打开组策略管理，本地计算机策略 &gt; 计算机配置 &gt; 管理模板&gt;系统 &gt; Device Guard打开 基于虚拟化的安全设置为“已开启”或者“未设置”随后重新开启wsl2，若不行，重启计算机。 启用虚拟机平台可选组件在 powerShell 中以管理员身份运行下面命令 1dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart 1dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart 运行完成之后，请重启电脑完成安装. 设置WSL发行版如果想要将默认的WSL发行版设置成 WSL 2，在 powerShell 中使用下面命令 wsl --set-default-version 2如果想要设置某一个发行版为WSL2，在 powerShell 中使用下面命令，将 换成你想要设置的发行版即可，例如 Ubuntu-18.04 wsl --set-version &lt;Distro&gt; 2 wsl --set-version Ubuntu-20.04 2验证使用的WSL版本 wsl -l -v 下载安装 Ubuntu-20.04 (Windows 应用商店里)更新包目录，并使用分发版的首选包管理器升级已安装的包sudo apt update &amp;&amp; sudo apt upgrade Windows不会自动更新或升级Linux发行版：Linux用户经常意外自行控制此任务。123456789101112131415161718192021222324252627ubuntu@kylin：〜$ wslfetch ./+o+- Windows 10 Linux Subsystem yyyyy. 'yyyyyy+ root@kylin .;//+/h yyyyyyo BUILD: 19624 .++ .:/++++++/-.`sss/` BRANCH: rs_prerelease .:++o: `\\++++++++/:---:/- RELEASE: Ubuntu 20.04 LTS o:+o+:++. `````'-/ooo+++++\\ KERNEL: Linux 4.19.104-microsoft-standard .:+o:+o/. `+sssooo+\\ UPTIME: 0d 0h 2m .++/+ +oo+o:` \\sssooo; /+++//+: oo+o \\+/+o+++ o++o ydddhh+ .++.o+ +oo+:` /dddhhh; .+.o+oo:. oddhhhh+ \\+.++o+o` -,,,,.:ohdhhhhh+ `:o+++ ohhhhhhhhyo++os: .o: .syhhhhhhh'.oo++o. /osyyyyyyy.oooo+++\\ `````+oo+++o:/ `oo++'`root@kylin:~# lsb_release -a | lolcatNo LSB modules are available.Distributor ID: UbuntuDescription: Ubuntu 20.04 LTSRelease: 20.04Codename: focalubuntu @ kylin:/ $ sudo apt install lolcat sudo apt install lolcat lsb_release -a | lolcat wsl ubuntu config1 修改 默认的源 (更换国内源)cp /etc/apt/sources.list /etc/apt/sourses.list.bak 更换默认源为阿里源, 使用 sudo vim /etc/apt/sources.list 命令编辑，删除原来的内容，添加下面的阿里源信息 12345678910deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse 更换源之后，使用下面的命令更新一下 12sudo apt-get updatesudo apt-get upgrade 2.ssh 连接 配置在WSL Ubuntu系统中安装ssh server当对Linux实现文件操作时，使用WinScp更为方便。因此需要使用ssh远程登陆 安装ssh serversudo apt-get install openssh-server 配置ssh使用 cp 命令将 SSH 相关配置备份sudo cp /etc/ssh/sshd_config /etc/ssh/sshd_config.bak 使用 vim 编辑 sshd_config 文件 sudo vim /etc/ssh/sshd_config调整一下设置： 12345Port 22ListenAddress 0.0.0.0PermitRootLogin yesStrictModes yesPasswordAuthentication yes 12345678910111213141516171819root@summer:/# service ssh status * sshd is not runningroot@summer:/# service ssh start * Starting OpenBSD Secure Shell server sshd sshd: no hostkeys available -- exiting.root@summer:/# sshd -Tsshd: no hostkeys available -- exiting.root@summer:/# ssh-keygen -t dsa -f /etc/ssh/ssh_host_dsa_keyroot@summer:/# ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_keyroot@summer:/etc/ssh# sshd -Troot@summer:/etc/ssh# service ssh start * Starting OpenBSD Secure Shell server sshd [ OK ] ＃Ubuntu的防火墙状态检测，防火墙可能限制SSH端口22root@summer:~# service ufw status * Firewall is not running... [fail]root@kylin:~# 重启ssh service 1sudo service ssh restart sshd: no hostkeys available — exiting在开启SSHD服务时报错.sshd re-exec requires execution with an absolute path用绝对路径启动,也报错如下:Could not load host key: /etc/ssh/ssh_host_keyCould not load host key: /etc/ssh/ssh_host_rsa_keyCould not load host key: /etc/ssh/ssh_host_dsa_keyDisabling protocol version 1. Could not load host keyDisabling protocol version 2. Could not load host keysshd: no hostkeys available — exiting解决过程: 123ssh-keygen -t dsa -f /etc/ssh/ssh_host_dsa_keyssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key/usr/sbin/sshd 如果上述两个文件存在，仍然出现这个错误，那么试试 chmod 600 上述两个文件。之后应该可以解决。 ssh login登陆 SSH使用 SSH 指令登陆 ssh root@127.0.0.1 -p 22 运行/停止Ubuntu子系统wsl -l 列出了系统中安装的子系统名称，可以是一个或多个，本文中的子系统名称是Ubuntu-18.04-20190707，接下来针对这个默认子系统进行操作： 运行子系统wsl --distribution Ubuntu-18.04-20190707或者wsl -d Ubuntu-18.04-20190707 查看运行中的子系统-l --running12 适用于 Linux 的 Windows 子系统:Ubuntu-18.04-20190707 (默认) 停止子系统wsl -t Ubuntu-18.04-20190707或者wsl --terminate Ubuntu-18.04-20190707 备份/删除/还原子系统备份子系统非常简单，但一定要先停止子系统之后再备份wsl --export Ubuntu-18.04-20190707 c:\\temp\\Ubuntu-18.04-20190707.tar等待完成即可。备份成功后，子系统会被打包成命令中指定的tar文件。 删除子系统也是一个命令即可：wsl --unregister Ubuntu-18.04-20190707这样WSL子系统就从Windows中删除的干干净净了。 还原子系统删除了没关系，刚才做了备份，也是一个命令还原：wsl --import Ubuntu-18.04-20190707 c:\\WSL c:\\temp\\Ubuntu-18.04-20190707.tar这里注意指定还原的路径。成功后，子系统又回来了，可以用wsl -l确认一下。 install docker-engines under ubuntu.refer to: https://docs.docker.com/engine/install/ubuntu/ 12345#启动docker sudo service docker startservice --status-allsudo service docker start #WSL2下能使用 systemctl, 参考：https://www.cnblogs.com/a5idc/p/13752839.htmlsudo usermod -aG docker {$USER}，$user是linux os你创建的用户，参考：https://docs.docker.com/engine/install/linux-postinstall/docker run hello-world #检查是否安装成功 install docker-compose under ubuntu.参考： https://docs.docker.com/compose/install/ 1sudo chmod +x /usr/local/bin/docker-compose 在ubuntu下git checkout docker-compose目录，运行mysql等服务。在windows下，可直接使用localhost连接123docker-compose up #启动并运行docker-compose up -d #在后台运行docker-compose down apt-get install telnetapt-get -y install netcat-traditional","link":"/2021/09/27/wsl/wsl-ubuntu/"},{"title":"wsl install centos","text":"参考文章 https://zhuanlan.zhihu.com/p/347461016 install centosWindows的应用商店中有一些不错的linux发行版，包括很多同学都很喜欢的ubuntu，但是个人比较熟悉使用centos，而应用商店中的centos是要收费的，不过好在github上面有CENTOS官方开源的安装包，我们这里使用github上的安装包进行安装。 如果使用应用商店中的发行版直接点击安装即可。随后便可以跳过下面的centos的安装部分。 首先我们去centos的GitHub页面下载对应的安装包：https://github.com/CentOS/sig-cloud-instance-images/blob/CentOS-8-x86_64/docker/centos-8-x86_64.tar.xz 接着我们以管理员身份打开一个powershell窗口： 1234# 安装 ChocolateySet-ExecutionPolicy Bypass -Scope Process -Force; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))# 安装 LxRunOfflinechoco install lxrunoffline 注意这里安装完成之后需要重启powershell来进行下一步的安装 LxRunOffline.exe install -n centos -d D:\\wsl\\centos -f D:\\wsl\\centos-8-x86_64.tar.xz 12345LxRunOffline install -n 自定义系统名称 -d 安装目录路径 -f tar.xz安装包路径# 注意windows系统命令行中的文件路径和linux系统差别很大# 比如我这里的安装命令就是LxRunOffline.exe install -n centos -d D:/centos -f .\\centos-7-x86_64-docker.tar.xz# 将centos安装到D盘的centos文件夹下，并且命名为centos 接下来就可以使用下述两种方式尝试启动 12LxRunOffline run -n 自定义系统名称wsl -d 自定义系统名称 升级centos为wsl2123456# 列出已经安装的wsl的信息wsl -l -v# 将对应的wsl设为wsl2，注意&lt;Distro&gt;要和上面查询到的信息一致wsl --set-version &lt;Distro&gt; 2# 设置默认使用的发行版wsl -s &lt;Distro&gt;","link":"/2021/09/27/wsl/wsl-centos/"},{"title":"CompletableFuture基本用法","text":"对比 Future：我们的目的都是获取异步任务的结果，但是对于Future来说，只能通过get方法或者死循环判断isDone来获取。异常情况就更是难办。 CompletableFuture：只要我们设置好回调函数即可实现： 只要任务完成，即执行我们设置的函数（不用再去考虑什么时候任务完成） 如果发生异常，同样会执行我们处理异常的函数，甚至连默认返回值都有（异常情况处理更加省力） 如果有复杂任务，比如依赖问题，组合问题等，同样可以写好处理函数来处理（能应付复杂任务的处理） FutureJDK5新增了Future接口，用于描述一个异步计算的结果。虽然 Future 以及相关使用方法提供了异步执行任务的能力，但是对于结果的获取却是很不方便，只能通过阻塞或者轮询的方式得到任务的结果。阻塞的方式显然和我们的异步编程的初衷相违背，轮询的方式又会耗费无谓的 CPU 资源，而且也不能及时地得到计算结果。 以前我们获取一个异步任务的结果可能是这样写的 Future 接口的局限性Future接口可以构建异步应用，但依然有其局限性。它很难直接表述多个Future 结果之间的依赖性。实际开发中，我们经常需要达成以下目的： 将多个异步计算的结果合并成一个 等待Future集合中的所有任务都完成 Future完成事件（即，任务完成以后触发执行动作） … CompletionStage CompletionStage代表异步计算过程中的某一个阶段，一个阶段完成以后可能会触发另外一个阶段 一个阶段的计算执行可以是一个Function，Consumer或者Runnable。比如：stage.thenApply(x -&gt; square(x)).thenAccept(x -&gt; System.out.print(x)).thenRun(() -&gt; System.out.println()) 一个阶段的执行可能是被单个阶段的完成触发，也可能是由多个阶段一起触发1public class CompletableFuture&lt;T&gt; implements Future&lt;T&gt;, CompletionStage&lt;T&gt; CompletableFuture 在Java8中，CompletableFuture提供了非常强大的Future的扩展功能，可以帮助我们简化异步编程的复杂性，并且提供了函数式编程的能力，可以通过回调的方式处理计算结果，也提供了转换和组合 CompletableFuture 的方法。 它可能代表一个明确完成的Future，也有可能代表一个完成阶段（ CompletionStage ），它支持在计算完成以后触发一些函数或执行某些动作。 它实现了Future和CompletionStage接口 对于CompletableFuture有四个执行异步任务的方法： 1234public static &lt;U&gt; CompletableFuture&lt;U&gt; supplyAsync(Supplier&lt;U&gt; supplier)public static &lt;U&gt; CompletableFuture&lt;U&gt; supplyAsync(Supplier&lt;U&gt; supplier, Executor executor)public static CompletableFuture&lt;Void&gt; runAsync(Runnable runnable)public static CompletableFuture&lt;Void&gt; runAsync(Runnable runnable, Executor executor) 121. 如果我们指定线程池，则会使用我么指定的线程池；如果没有指定线程池，默认使用ForkJoinPool.commonPool()作为线程池。2. supply开头的带有返回值，run开头的无返回值。 执行异步任务（supplyAsync / runAsync）1234567891011121314151617181920import java.util.Random;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.CompletableFuture;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;public class Main { public static void main(String[] args) throws Exception { ThreadPoolExecutor executor = new ThreadPoolExecutor(3, 6, 60L, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(10)); CompletableFuture&lt;Integer&gt; future = CompletableFuture.supplyAsync(() -&gt; { return new Random().nextInt(100); }, executor); System.out.println(future.get()); executor.shutdown(); }} 以上仅仅返回个随机数，如果我们要利用计算结果进一步处理呢？ 结果转换（thenApply / thenApplyAsync）123456// 同步转换public &lt;U&gt; CompletableFuture&lt;U&gt; thenApply(Function&lt;? super T,? extends U&gt; fn)// 异步转换，使用默认线程池public &lt;U&gt; CompletableFuture&lt;U&gt; thenApplyAsync(Function&lt;? super T,? extends U&gt; fn)// 异步转换，使用指定线程池public &lt;U&gt; CompletableFuture&lt;U&gt; thenApplyAsync(Function&lt;? super T,? extends U&gt; fn, Executor executor) 123456789101112131415161718192021222324252627282930313233343536import java.util.Random;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.CompletableFuture;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;public class Main { public static void main(String[] args) throws Exception { ThreadPoolExecutor executor = new ThreadPoolExecutor(3, 6, 60L, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(10)); CompletableFuture&lt;Integer&gt; future = CompletableFuture // 执行异步任务 .supplyAsync(() -&gt; { return new Random().nextInt(100); }, executor) // 对上一步的结果进行处理 .thenApply(n -&gt; { try { Thread.sleep(2000); } catch (InterruptedException e) { e.printStackTrace(); } int res = new Random().nextInt(100); System.out.println(String.format(&quot;如果是同步的，这条消息应该先输出。上一步结果：%s，新加：%s&quot;, n, res)); return n + res; }); System.out.println(&quot;我等了你2秒&quot;); System.out.println(future.get()); executor.shutdown(); }} 输出：如果把thenApply换成thenApplyAsync，则会输出：处理完任务以及结果，该去消费了 消费而不影响最终结果（thenAccept / thenRun / thenAcceptBoth）1234567891011public CompletableFuture&lt;Void&gt; thenAccept(Consumer&lt;? super T&gt; action)public CompletableFuture&lt;Void&gt; thenAcceptAsync(Consumer&lt;? super T&gt; action)public CompletableFuture&lt;Void&gt; thenAcceptAsync(Consumer&lt;? super T&gt; action, Executor executor)public CompletableFuture&lt;Void&gt; thenRun(Runnable action)public CompletableFuture&lt;Void&gt; thenRunAsync(Runnable action)public CompletableFuture&lt;Void&gt; thenRunAsync(Runnable action, Executor executor)public &lt;U&gt; CompletableFuture&lt;Void&gt; thenAcceptBoth(CompletionStage&lt;? extends U&gt; other, BiConsumer&lt;? super T, ? super U&gt; action)public &lt;U&gt; CompletableFuture&lt;Void&gt; thenAcceptBothAsync(CompletionStage&lt;? extends U&gt; other, BiConsumer&lt;? super T, ? super U&gt; action)public &lt;U&gt; CompletableFuture&lt;Void&gt; thenAcceptBothAsync(CompletionStage&lt;? extends U&gt; other, BiConsumer&lt;? super T, ? super U&gt; action, Executor executor) 123456789这三种的区别是：thenAccept：能够拿到并利用执行结果thenRun：不能够拿到并利用执行结果，只是单纯的执行其它任务thenAcceptBoth：能传入另一个stage，然后把另一个stage的结果和当前stage的结果作为参数去消费。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import java.util.Random;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.CompletableFuture;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;public class Main { public static void main(String[] args) throws Exception { ThreadPoolExecutor executor = new ThreadPoolExecutor(3, 6, 60L, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(10)); CompletableFuture&lt;Integer&gt; future = CompletableFuture // 执行异步任务 .supplyAsync(() -&gt; { return new Random().nextInt(100); }, executor) // 对上一步的结果进行处理 .thenApplyAsync(n -&gt; { try { Thread.sleep(2000); } catch (InterruptedException e) { e.printStackTrace(); } int res = new Random().nextInt(100); System.out.println(String.format(&quot;如果是同步的，这条消息应该先输出。上一步结果：%s，新加：%s&quot;, n, res)); return n + res; }); // 单纯的消费执行结果，注意这个方法是不会返回计算结果的——CompletableFuture&lt;Void&gt; CompletableFuture&lt;Void&gt; voidCompletableFuture = future.thenAcceptAsync(n -&gt; { System.out.println(&quot;单纯消费任务执行结果：&quot; + n); }); // 这个无法消费执行结果，没有传入的入口，只是在当前任务执行完毕后执行其它不相干的任务 future.thenRunAsync(() -&gt; { System.out.println(&quot;我只能执行其它工作，我得不到任务执行结果&quot;); }, executor); // 这个方法会接受其它CompletableFuture返回值和当前返回值 future.thenAcceptBothAsync(CompletableFuture.supplyAsync(() -&gt; { return &quot;I'm Other Result&quot;; }), (current, other) -&gt; { System.out.println(String.format(&quot;Current：%s，Other:%s&quot;, current, other)); }); System.out.println(&quot;我等了你2秒&quot;); System.out.println(future.get()); executor.shutdown(); }} 结果： 组合任务（thenCombine / thenCompose）1234567public &lt;U,V&gt; CompletionStage&lt;V&gt; thenCombine(CompletionStage&lt;? extends U&gt; other, BiFunction&lt;? super T,? super U,? extends V&gt; fn)public &lt;U,V&gt; CompletionStage&lt;V&gt; thenCombineAsync(CompletionStage&lt;? extends U&gt; other, BiFunction&lt;? super T,? super U,? extends V&gt; fn)public &lt;U,V&gt; CompletionStage&lt;V&gt; thenCombineAsync(CompletionStage&lt;? extends U&gt; other, BiFunction&lt;? super T,? super U,? extends V&gt; fn, Executor executor)public &lt;U&gt; CompletionStage&lt;U&gt; thenCompose(Function&lt;? super T, ? extends CompletionStage&lt;U&gt;&gt; fn)public &lt;U&gt; CompletionStage&lt;U&gt; thenComposeAsync(Function&lt;? super T, ? extends CompletionStage&lt;U&gt;&gt; fn)public &lt;U&gt; CompletionStage&lt;U&gt; thenComposeAsync(Function&lt;? super T, ? extends CompletionStage&lt;U&gt;&gt; fn, Executor executor) 12345这两种区别：主要是返回类型不一样。thenCombine：至少两个方法参数，一个为其它stage，一个为用户自定义的处理函数，函数返回值为结果类型。thenCompose：至少一个方法参数即处理函数，函数返回值为stage类型。 先看thenCombine 123456789101112131415161718192021222324252627282930313233343536373839import java.util.Random;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.CompletableFuture;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;public class Main { public static void main(String[] args) throws Exception { ThreadPoolExecutor executor = new ThreadPoolExecutor(3, 6, 60L, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(10)); CompletableFuture&lt;Integer&gt; otherFuture = CompletableFuture // 执行异步任务 .supplyAsync(() -&gt; { int result = new Random().nextInt(100); System.out.println(&quot;任务A：&quot; + result); return result; }, executor); CompletableFuture&lt;Integer&gt; future = CompletableFuture // 执行异步任务 .supplyAsync(() -&gt; { int result = new Random().nextInt(100); System.out.println(&quot;任务B：&quot; + result); return result; }, executor) .thenCombineAsync(otherFuture, (current, other) -&gt; { int result = other + current; System.out.println(&quot;组合两个任务的结果：&quot; + result); return result; }); System.out.println(future.get()); executor.shutdown(); }} 执行结果：再来看thenCompose 1234567891011121314151617181920212223242526272829303132333435import java.util.Random;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.CompletableFuture;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;public class Main { public static void main(String[] args) throws Exception { ThreadPoolExecutor executor = new ThreadPoolExecutor(3, 6, 60L, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(10)); CompletableFuture&lt;Integer&gt; future = CompletableFuture // 执行异步任务 .supplyAsync(() -&gt; { int result = new Random().nextInt(100); System.out.println(&quot;任务A：&quot; + result); return result; }, executor) .thenComposeAsync((current) -&gt; { return CompletableFuture.supplyAsync(() -&gt; { int b = new Random().nextInt(100); System.out.println(&quot;任务B：&quot; + b); int result = b + current; System.out.println(&quot;组合两个任务的结果：&quot; + result); return result; }, executor); }); System.out.println(future.get()); executor.shutdown(); }} 输出： 快者优先（applyToEither / acceptEither）1有个场景，如果我们有多条渠道去完成同一种任务，那么我们肯定选择最快的那个。 1234567public &lt;U&gt; CompletionStage&lt;U&gt; applyToEither(CompletionStage&lt;? extends T&gt; other, Function&lt;? super T, U&gt; fn)public &lt;U&gt; CompletionStage&lt;U&gt; applyToEitherAsync(CompletionStage&lt;? extends T&gt; other, Function&lt;? super T, U&gt; fn)public &lt;U&gt; CompletionStage&lt;U&gt; applyToEitherAsync(CompletionStage&lt;? extends T&gt; other, Function&lt;? super T, U&gt; fn, Executor executor)public CompletionStage&lt;Void&gt; acceptEither(CompletionStage&lt;? extends T&gt; other, Consumer&lt;? super T&gt; action)public CompletionStage&lt;Void&gt; acceptEitherAsync(CompletionStage&lt;? extends T&gt; other, Consumer&lt;? super T&gt; action)public CompletionStage&lt;Void&gt; acceptEitherAsync(CompletionStage&lt;? extends T&gt; other, Consumer&lt;? super T&gt; action, Executor executor) 1这两种区别：仅仅是一个有返回值，一个没有（Void） 先看applyToEither 123456789101112131415161718192021222324252627282930313233343536373839404142import java.util.Random;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.CompletableFuture;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;public class Main { public static void main(String[] args) throws Exception { ThreadPoolExecutor executor = new ThreadPoolExecutor(3, 6, 60L, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(10)); CompletableFuture&lt;String&gt; otherFuture = CompletableFuture .supplyAsync(() -&gt; { int result = new Random().nextInt(100); System.out.println(&quot;执行者A：&quot; + result); try { // 故意A慢了一些 Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } return &quot;执行者A【&quot; + result + &quot;】&quot;; }, executor); CompletableFuture&lt;String&gt; future = CompletableFuture .supplyAsync(() -&gt; { int result = new Random().nextInt(100); System.out.println(&quot;执行者B：&quot; + result); return &quot;执行者B【&quot; + result + &quot;】&quot;; }, executor) .applyToEither(otherFuture, (faster) -&gt; { System.out.println(&quot;谁最快：&quot; + faster); return faster; }); System.out.println(future.get()); executor.shutdown(); }} 输出：再看acceptEither 1234567891011121314151617181920212223242526272829303132333435363738394041import java.util.Random;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.CompletableFuture;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;public class Main { public static void main(String[] args) throws Exception { ThreadPoolExecutor executor = new ThreadPoolExecutor(3, 6, 60L, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(10)); CompletableFuture&lt;String&gt; otherFuture = CompletableFuture .supplyAsync(() -&gt; { int result = new Random().nextInt(100); System.out.println(&quot;执行者A：&quot; + result); try { // 故意A慢了一些 Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } return &quot;执行者A【&quot; + result + &quot;】&quot;; }, executor); CompletableFuture&lt;Void&gt; future = CompletableFuture .supplyAsync(() -&gt; { int result = new Random().nextInt(100); System.out.println(&quot;执行者B：&quot; + result); return &quot;执行者B【&quot; + result + &quot;】&quot;; }, executor) .acceptEither(otherFuture, (faster) -&gt; { System.out.println(&quot;谁最快：&quot; + faster); }); System.out.println(future.get()); executor.shutdown(); }} 输出： 异常处理（exceptionally / whenComplete / handle）123456789public CompletionStage&lt;T&gt; exceptionally(Function&lt;Throwable, ? extends T&gt; fn);public CompletionStage&lt;T&gt; whenComplete(BiConsumer&lt;? super T, ? super Throwable&gt; action);public CompletionStage&lt;T&gt; whenCompleteAsync(BiConsumer&lt;? super T, ? super Throwable&gt; action);public CompletionStage&lt;T&gt; whenCompleteAsync(BiConsumer&lt;? super T, ? super Throwable&gt; action, Executor executor);public &lt;U&gt; CompletionStage&lt;U&gt; handle(BiFunction&lt;? super T, Throwable, ? extends U&gt; fn);public &lt;U&gt; CompletionStage&lt;U&gt; handleAsync(BiFunction&lt;? super T, Throwable, ? extends U&gt; fn);public &lt;U&gt; CompletionStage&lt;U&gt; handleAsync(BiFunction&lt;? super T, Throwable, ? extends U&gt; fn,Executor executor); exceptionally 123456789101112131415161718192021222324252627282930import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.CompletableFuture;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;public class Main { public static void main(String[] args) throws Exception { ThreadPoolExecutor executor = new ThreadPoolExecutor(3, 6, 60L, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(10)); CompletableFuture&lt;String&gt; future = CompletableFuture .supplyAsync(() -&gt; { if (true){ throw new RuntimeException(&quot;Error!!!&quot;); } return &quot;Hello&quot;; }, executor) // 处理上一步发生的异常 .exceptionally(e -&gt; { System.out.println(&quot;处理异常：&quot; + e.getMessage()); return &quot;处理完毕!&quot;; }); System.out.println(future.get()); executor.shutdown(); }} 输出：whenComplete 123456789101112131415161718192021222324252627282930313233343536import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.CompletableFuture;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;public class Main { public static void main(String[] args) throws Exception { ThreadPoolExecutor executor = new ThreadPoolExecutor(3, 6, 60L, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(10)); CompletableFuture&lt;String&gt; future = CompletableFuture .supplyAsync(() -&gt; { if (true){ throw new RuntimeException(&quot;Error!!!&quot;); } return &quot;Hello&quot;; }, executor) // 处理上一步发生的异常 .whenComplete((result,ex) -&gt; { // 这里等待为了上一步的异常输出完毕 try { Thread.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(&quot;上一步结果：&quot; + result); System.out.println(&quot;处理异常：&quot; + ex.getMessage()); }); System.out.println(future.get()); executor.shutdown(); }} 输出结果：可以看见，用whenComplete对异常情况不是特别友好。 handle 12345678910111213141516171819202122232425262728293031import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.CompletableFuture;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;public class Main { public static void main(String[] args) throws Exception { ThreadPoolExecutor executor = new ThreadPoolExecutor(3, 6, 60L, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(10)); CompletableFuture&lt;String&gt; future = CompletableFuture .supplyAsync(() -&gt; { if (true){ throw new RuntimeException(&quot;Error!!!&quot;); } return &quot;Hello&quot;; }, executor) // 处理上一步发生的异常 .handle((result,ex) -&gt; { System.out.println(&quot;上一步结果：&quot; + result); System.out.println(&quot;处理异常：&quot; + ex.getMessage()); return &quot;Value When Exception Occurs&quot;; }); System.out.println(future.get()); executor.shutdown(); }} 输出： 综上，如果单纯要处理异常，那就用exceptionally；如果还想处理结果（没有异常的情况），那就用handle，比whenComplete友好一些，handle不仅能处理异常还能返回一个异常情况的默认值。 参考文章 https://www.cnblogs.com/cjsblog/p/9267163.html https://www.cnblogs.com/LUA123/p/12050255.html https://www.liaoxuefeng.com/wiki/1252599548343744/1306581182447650 https://blog.csdn.net/qq_31865983/article/details/106137777 https://blog.csdn.net/finalheart/article/details/87615546 https://www.jianshu.com/p/6bac52527ca4","link":"/2021/11/10/java/java-CompletableFuture/"},{"title":"zookeeper windows install","text":"下载zookeeperhttps://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/ 配置zookeeper解压到三个目录我们想要在单机上搭建3个server的伪集群，需要将下载好的zookeeper压缩包解压到三个目录下。 123server1 : D:\\zookeeper\\server1server2 : D:\\zookeeper\\server3server3 : D:\\zookeeper\\server3 创建配置文件（cfg文件）解压之后，分别进入conf目录，可以看到zoo_sample.cfg，log4j.properties和configuration.xsl三个文件。 在该目录下创建一个zoo.cfg文件（也可以直接使用zoo_sample.cfg），配置如下： 123456789101112131415161718192021222324252627282930313233343536# The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial # synchronization phase can takeinitLimit=10# The number of ticks that can pass between # sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just # example sakes.dataDir=D:\\\\zookeeper\\\\server1\\\\datadataLogDir=D:\\\\zookeeper\\\\server1\\\\logs# the port at which the clients will connectclientPort=2181# the maximum number of client connections.# increase this if you need to handle more clients#maxClientCnxns=60## Be sure to read the maintenance section of the # administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to &quot;0&quot; to disable auto purge feature#autopurge.purgeInterval=1# （伪集群zookeeper的server1标识）server.1=localhost:2887:3887 # （伪集群zookeeper的server2标识）server.2=localhost:2888:3888 # （伪集群zookeeper的server3标识）server.3=localhost:2889:3889 以上就是zookeeper伪集群中server1的配置文件。同理在其他两个解压路径的conf目录下创建server2和server3的配置文件zoo.cfg。参数区别仅在于dataDir、dataLogDir和clientPort server2的zoo.cfg 123456789101112131415161718192021222324252627282930313233343536# The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial # synchronization phase can takeinitLimit=10# The number of ticks that can pass between # sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just # example sakes.dataDir=D:\\\\zookeeper\\\\server2\\\\datadataLogDir=D:\\\\zookeeper\\\\server2\\\\logs# the port at which the clients will connectclientPort=2182# the maximum number of client connections.# increase this if you need to handle more clients#maxClientCnxns=60## Be sure to read the maintenance section of the # administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to &quot;0&quot; to disable auto purge feature#autopurge.purgeInterval=1# （伪集群zookeeper的server1标识）server.1=localhost:2887:3887 # （伪集群zookeeper的server2标识）server.2=localhost:2888:3888 # （伪集群zookeeper的server3标识）server.3=localhost:2889:3889 server3的zoo.cfg 123456789101112131415161718192021222324252627282930313233343536# The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial # synchronization phase can takeinitLimit=10# The number of ticks that can pass between # sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just # example sakes.dataDir=D:\\\\zookeeper\\\\server3\\\\datadataLogDir=D:\\\\zookeeper\\\\server3\\\\logs# the port at which the clients will connectclientPort=2183# the maximum number of client connections.# increase this if you need to handle more clients#maxClientCnxns=60## Be sure to read the maintenance section of the # administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to &quot;0&quot; to disable auto purge feature#autopurge.purgeInterval=1# （伪集群zookeeper的server1标识）server.1=localhost:2887:3887 # （伪集群zookeeper的server2标识）server.2=localhost:2888:3888 # （伪集群zookeeper的server3标识）server.3=localhost:2889:3889 创建myid文件在上个步骤中，我们在dataDir中指定了快照存放目录，切换到各目录下，分别创建一个文件名为myid的文件（没有后缀名）。文件内容为一个整型数。 在server1的data目录下的myid文件，其内容为1。在server2的data目录下的myid文件，其内容为2。在server3的data目录下的myid文件，其内容为3。 启动zookeeper分别切换到三个解压路径下的bin目录，在cmd上输入zkServer.cmd启动服务，可以同时用三个cmd窗口分别启动三个server，启动顺序是server1 -&gt; server2 -&gt; server3。启动的过程中是会报错的，信息如下： 进入cmd，切换目录到 /server1/bin/，执行命令 zkServer.cmd（此时会打印错误日志，别急，这是心跳检查连接其他zk服务，等启动集群数量一半以上的zk服务后，就不报错了）进入cmd，切换目录到 /server2/bin/，执行命令 zkServer.cmd进入cmd，切换目录到 /server3/bin/，执行命令 zkServer.cmd 验证zookeeper服务是否启动cmd，切换目录到 /server1/bin，执行命令 zkCli.cmd -server localhost:2181 参考文章 https://www.cnblogs.com/yangzhenlong/p/8270835.html https://blog.csdn.net/sinat_34596644/article/details/78289842","link":"/2021/10/28/zookeeper/zookeeper-windows-install/"},{"title":"JVM内存结构","text":"概述 JVM内存结构主要有三大块：·堆内存、方法区和栈·。堆内存是JVM中最大的一块由年轻代和老年代组成，而年轻代内存又被分成三部分，Eden空间、From Survivor空间、To Survivor空间,默认情况下年轻代按照8:1:1的比例来分配； 方法区存储类信息、常量、静态变量等数据，是线程共享的区域，为与Java堆区分，方法区还有一个别名Non-Heap(非堆)；栈又分为java虚拟机栈和本地方法栈主要用于方法的执行。 xx xxxxxxxx desc 堆（Heap） 线程共享 所有的对象实例以及数组都要在堆上分配。 回收器主要管理的对象。 方法区（Method Area） 线程共享 Non-Heap（非堆） 存储类信息、常量、静态变量、即时编译器编译后的代码。 方法栈（JVM Stack） 线程私有 存储局部变量表、操作栈、动态链接、方法出口，对象指针。 本地方法栈（Native Method Stack） 线程私有 为虚拟机使用到的Native 方法服务。如Java使用c或者c++编写的接口服务时，代码在此区运行。 程序计数器（Program Counter Register） 线程私有 PC寄存器（PC Register） 当前线程所执行的字节码的行号指示器。指向下一条要执行的指令。 在通过一张图来了解如何通过参数来控制各区域的内存大小 1234567-Xms设置堆的最小空间大小-Xmx设置堆的最大空间大小-XX:NewSize设置新生代最小空间大小-XX:MaxNewSize设置新生代最大空间大小-XX:PermSize设置永久代最小空间大小-XX:MaxPermSize设置永久代最大空间大小-Xss设置每个线程的堆栈大小 JVMJVM = 类加载器(classloader) + 执行引擎(execution engine) + 运行时数据区域(runtime data area) 运行时数据区域Java虚拟机在执行Java程序的过程中会把它所管理的内存划分为若干个不同的数据区域。这些区域都有各自的用途，以及创建和销毁的时间，有的区域随着虚拟机进程的启动而存在，有些区域则是依赖用户线程的启动和结束而建立和销毁。 JVM 内存结构 程序计数器 线程私有 程序计数器（Program Counter Register）是一块较小的内存空间，**它的作用可以看做是当前线程所执行的字节码的行号指示器**。在虚拟机的概念模型里，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 由于Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，**每条线程都需要有一个独立的程序计数器，各条线程之间的计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。** 如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Natvie方法，这个计数器值则为空（Undefined）。 异常情况此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。 JVM栈/方法栈线程私有 与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，**它的生命周期与线程相同。**虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 局部变量表存放了编译期可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它不等同于对象本身，根据不同的虚拟机实现，它可能是一个指向对象起始地址的引用指针，也可能指向一个代表对象的句柄或者其他与此对象相关的位置）和returnAddress类型（指向了一条字节码指令的地址）。其中64位长度的long和double类型的数据会占用2个局部变量空间（Slot），其余的数据类型只占用1个。 局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。 控制参数 1-Xss控制每个线程栈的大小 异常情况 在Java虚拟机规范中，对这个区域规定了两种异常状况： 如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常 如果虚拟机栈可以动态扩展（当前大部分的Java虚拟机都可动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈）， 当扩展时无法申请到足够的内存时会抛出OutOfMemoryError异常。 本地方法栈线程私有 本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而**本地方法栈则是为虚拟机使用到的Native方法服务。**虚拟机规范中对本地方法栈中的方法使用的语言、使用方式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它。甚至有的虚拟机（譬如Sun HotSpot虚拟机）直接就把本地方法栈和虚拟机栈合二为一。 控制参数在Sun JDK中本地方法栈和方法栈是同一个，因此也可以用-Xss控制每个线程的大小。 1-Xss控制每个线程栈的大小 异常情况与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。 Java堆线程共享对于大多数应用来说，Java堆（Java Heap）是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。 Java堆是垃圾收集器管理的主要区域，因此很多时候也被称做“GC堆”。如果从内存回收的角度看，由于现在收集器基本都是采用的分代收集算法，所以Java堆中还可以细分为：新生代和老年代；再细致一点的有Eden空间、From Survivor空间、To Survivor空间等。所有新生成的对象首先都是放在新生代的。需要注意，Survivor的两个区是对称的，没先后关系，所以同一个区中可能同时存在从Eden复制过来的对象，和从前一个Survivor复制过来的对象，而复制到老年代的只有从第一个Survivor区过来的对象。而且，Survivor区总有一个是空的。 控制参数 1234-Xms设置堆的最小空间大小-Xmx设置堆的最大空间大小-XX:NewSize设置新生代最小空间大小-XX:MaxNewSize设置新生代最小空间大小 垃圾回收此区域是垃圾回收的主要操作区域。 异常情况如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError 异常 根据Java虚拟机规范的规定，Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可，就像我们的磁盘空间一样。在实现时，既可以实现成固定大小的，也可以是可扩展的，不过当前主流的虚拟机都是按照可扩展来实现的（通过-Xmx和-Xms控制）。 方法区线程共享 方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做Non-Heap（非堆），目的应该是与Java堆区分开来。 对于习惯在HotSpot虚拟机上开发和部署程序的开发者来说，很多人愿意把方法区称为“永久代”（Permanent Generation），本质上两者并不等价，仅仅是因为HotSpot虚拟机的设计团队选择把GC分代收集扩展至方法区，或者说使用永久代来实现方法区而已。 控制参数12-XX:PermSize 设置最小空间 -XX:MaxPermSize 设置最大空间 垃圾回收 Java虚拟机规范对这个区域的限制非常宽松，除了和Java堆一样不需要连续的内存和可以选择固定大小或者可扩展外，还可以选择不实现垃圾收集。相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入了方法区就如永久代的名字一样“永久”存在了。这个区域的内存回收目标主要是针对常量池的回收和对类型的卸载，一般来说这个区域的回收“成绩”比较难以令人满意，尤其是类型的卸载，条件相当苛刻，但是这部分区域的回收确实是有必要的。 异常情况根据Java虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。 HotSpot中方法区的变化 jdk版本 区别 jdk1.6及之前 有永久代，静态变量存放在永久代上 jdk1.7 有永久代，但已经逐步去“永久代”，字符串常量池、静态变量移除，保存着堆中 jdk1.8及之后 无永久代，类型信息、字段、方法、常量保存在本地内存的元空间，但字符串常量池、静态变量仍在堆中 运行时常量池 运行时常量池是在方法区的一部分； Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池(Constant Pool Table)，用于存放编译期生成的各种字面量和符号引用，这部分内容将类在加载后进入方法区的运行时常量池中存放； Java虚拟机对Class文件每一部分(自然也包括常量池)的格式都有严格规定，每一个字节用于存储哪种数据都必须符合规范上的要求才会被虚拟机认可、加载和执行，但是对于运行时常量池，《Java虚拟机规范》没有做任何细节的要求，不用的提供上实现的虚拟机可以按照自己的需要来实现这个内存区域，不过一般来说，除了保存Class文件中描述的符号引用外,还会把翻译出来的直接引用也存储在运行时常量池中；运行时常量池相对于Class文件常量池的另外一个重要特征就是具备动态性，Java语言并不要求常量一定只有在编译期才能产生，也就是说，并非预置入Class文件中常量池的内容才能进入方法区运行时常量池，运行期间也可能将新的常量放入池中，这种特性被开发人员利用得比较多的便是String类的intern()方法。 既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出OutOfMemoryError异常。 永久代和元空间java7及以前版本JVM内存结构图 堆和方法区连在了一起，但这并不能说堆和方法区是一起的，它们在逻辑上依旧是分开的。但在物理上来说，它们又是连续的一块内存。 也就是说，方法区和前面讲到的Eden和老年代是连续的。 永久代（PermGen）对于习惯了在HotSpot虚拟机上开发、部署的程序员来说，很多都愿意将方法区称作永久代。本质上来讲两者并不等价，仅因为Hotspot将GC分代扩展至方法区，或者说使用永久代来实现方法区。在其他虚拟机上是没有永久代的概念的。也就是说方法区是规范，永久代是Hotspot针对该规范进行的实现。 理解上面的概念之后，我们对Java7及以前版本的堆和方法区的构造再进行一下变动。 对Java7及以前版本的Hotspot中方法区位于永久代中。同时，永久代和堆是相互隔离的，但它们使用的物理内存是连续的。永久代的垃圾收集是和老年代捆绑在一起的，因此无论谁满了，都会触发永久代和老年代的垃圾收集。 但在Java7中永久代中存储的部分数据已经开始转移到Java Heap或Native Memory中了。比如，符号引用(Symbols)转移到了Native Memory；字符串常量池(interned strings)转移到了Java Heap；类的静态变量(class statics)转移到了Java Heap。然后，在Java8中，时代变了，Hotspot取消了永久代。永久代真的成了永久的记忆。永久代的参数-XX:PermSize和-XX：MaxPermSize也随之失效。 元空间（Metaspace）对于Java8，HotSpots取消了永久代，那么是不是就没有方法区了呢？当然不是，方法区只是一个规范，只不过它的实现变了。在Java8中，元空间(Metaspace)登上舞台，方法区存在于元空间(Metaspace)。同时，元空间不再与堆连续，而且是存在于本地内存（Native memory）。 本地内存（Native memory），也称为C-Heap，是供JVM自身进程使用的。当Java Heap空间不足时会触发GC，但Native memory空间不够却不会触发GC。 针对Java8的调整，我们再次对内存结构图进行调整。 元空间存在于本地内存，意味着只要本地内存足够，它不会出现像永久代中java.lang.OutOfMemoryError: PermGen space这种错误。默认情况下元空间是可以无限使用本地内存的，但为了不让它如此膨胀，JVM同样提供了参数来限制它使用的使用。 -XX:MetaspaceSize class metadata的初始空间配额，以bytes为单位，达到该值就会触发垃圾收集进行类型卸载，同时GC会对该值进行调整：如果释放了大量的空间，就适当的降低该值；如果释放了很少的空间，那么在不超过MaxMetaspaceSize（如果设置了的话），适当的提高该值。 -XX：MaxMetaspaceSize 可以为class metadata分配的最大空间。默认是没有限制的。 -XX：MinMetaspaceFreeRatio 在GC之后，最小的Metaspace剩余空间容量的百分比，减少为class metadata分配空间导致的垃圾收集。 -XX:MaxMetaspaceFreeRatio 在GC之后，最大的Metaspace剩余空间容量的百分比，减少为class metadata释放空间导致的垃圾收集。 面试官 | JVM 为什么使用元空间替换了永久代？ 表面上看是为了避免OOM异常。因为通常使用PermSize和MaxPermSize设置永久代的大小就决定了永久代的上限，但是不是总能知道应该设置为多大合适, 如果使用默认值很容易遇到OOM错误。 当使用元空间时，可以加载多少类的元数据就不再由MaxPermSize控制, 而由系统的实际可用空间来控制。 更深层的原因还是要合并HotSpot和JRockit的代码，JRockit从来没有所谓的永久代，也不需要开发运维人员设置永久代的大小，但是运行良好。同时也不用担心运行性能问题了,在覆盖到的测试中, 程序启动和运行速度降低不超过1%，但是这点性能损失换来了更大的安全保障。 直接内存在JVM的内存模型，里面并不包含直接内存，也就是说这块内存区域并不是JVM运行时数据区的一部分，但它却会被频繁的使用，原因是NIO这个包；NIO（New input/output）是JDK1.4中新加入的类，引入了一种基于通道（channel）和缓冲区（buffer）的I/O方式，它可以使用Native函数库直接分配堆外内存，然后通过堆上的DirectByteBuffer对象对这块内存进行引用和操作。 可以看出，直接内存的大小并不受到java堆大小的限制，甚至不受到JVM进程内存大小的限制。它只受限于本机总内存（RAM及SWAP区或者分页文件）大小以及处理器寻址空间的限制（最常见的就是32位/64位CPU的最大寻址空间限制不同）。 参考文章 https://juejin.cn/post/6970606107442020360 https://juejin.cn/post/6844903592374042637 https://www.cnblogs.com/ityouknow/p/5610232.html https://zhuanlan.zhihu.com/p/38348646","link":"/2021/09/28/java/java-jvm-memory/"},{"title":"AbstractQueuedSynchronizer","text":"AQS (AbstractQueuedSynchronizer) 抽象类的队列式同步器原理概览AQS核心思想是，如果被请求的共享资源空闲，那么就将当前请求资源的线程设置为有效的工作线程，将共享资源设置为锁定状态；如果共享资源被占用，就需要一定的阻塞等待唤醒机制来保证锁分配。这个机制主要用的是CLH队列的变体实现的，将暂时获取不到锁的线程加入到队列中。 CLH：Craig、Landin and Hagersten队列，是单向链表，AQS中的队列是CLH变体的虚拟双向队列（FIFO），AQS是通过将每条请求共享资源的线程封装成一个节点来实现锁的分配。 基础定义 AQS 是一个用于构建锁和同步器的框架，许多同步器都可以通过AQS很容易并且高效的构造出来。 为线程的同步和等待等操作提供一个基础模板类。尽可能多的实现可重入锁，读写锁同步器所有需要的功能。队列同步器内部实现了线程的同步队列，独占或是共享的获取方式等，使其只需要少量的代码便可以实现目标功能。 一般来说，AQS的子类应以其他类的内部类的形式存在，然后使用代理模式调用子类和AQS本身的方法实现线程的同步。也就是说，使用ReentrantLock举例，外界调用ReentrantLock，ReentrantLock内部定义Sync，Sync是AQS的子类，在ReentrantLock的内部实现中调用Sync的方法，最后完成最终的功能，当然ReentrantLock内部稍复杂，又加入和公平锁和非公平锁。 抽象的队列式的同步器，AQS定义了一套多线程访问共享资源的同步器框架，许多同步类实现都依赖于它，如常用的ReentrantLock/Semaphore/CountDownLatch...。AbstractQueuedSynchronizer，这个类也是在java.util.concurrent.locks下面。 AQS，全名AbstractQueuedSynchronizer，是一个抽象类的队列式同步器，它的内部通过维护一个状态volatile int state(共享资源)，一个FIFO线程等待队列来实现同步功能。 state 所有通过AQS实现功能的类都是通过修改state的状态来操作线程的同步状态。比如在ReentrantLock中，一个锁中只有一个state状态，当state为0时，代表所有线程没有获取锁，当state为1时，代表有线程获取到了锁。通过是否能把state从0设置成1，当然，设置的方式是使用CAS设置，代表一个线程是否获取锁成功。 state用关键字volatile修饰，代表着该共享资源的状态一更改就能被所有线程可见，而AQS的加锁方式本质上就是多个线程在竞争state，当state为0时代表线程可以竞争锁，不为0时代表当前对象锁已经被占有，其他线程来加锁时则会失败，加锁失败的线程会被放入一个FIFO的等待队列中，这些线程会被UNSAFE.park()操作挂起，等待其他获取锁的线程释放锁才能够被唤醒。 AQS内部维护一个线程的队列。队列由内部的节点组成。 队列的节点为Node,节点分为SHARED和EXCLUSIVE分别时共享模式的节点和独占模式的节点。 而这个等待队列其实就相当于一个CLH队列，用一张原理图来表示大致如下： AQS支持两种资源分享的方式： Exclusive（独占，只有一个线程能执行，如ReentrantLock） Share（共享，多个线程可同时执行，如Semaphore/CountDownLatch）。 自定义的同步器继承AQS后，只需要实现·共享资源state的获取和释放方式·即可，其他如线程队列的维护（如获取资源失败入队/唤醒出队等）等操作，AQS在顶层已经实现了， AQS代码内部提供了一系列操作锁和线程队列的方法，主要操作锁的方法包含以下几个： compareAndSetState()：利用CAS的操作来设置state的值- tryAcquire(int)：独占方式获取锁。成功则返回true，失败则返回false。 tryRelease(int)：独占方式释放锁。成功则返回true，失败则返回false。 tryAcquireShared(int)：共享方式释放锁。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。 tryReleaseShared(int)：共享方式释放锁。如果释放后允许唤醒后续等待结点返回true，否则返回false。 像ReentrantLock就是实现了自定义的tryAcquire-tryRelease，从而操作state的值来实现同步效果。 除此之外，AQS内部还定义了一个静态类Node，表示CLH队列的每一个结点，该结点的作用是对每一个等待获取资源做了封装，包含了需要同步的线程本身、线程等待状态….. 我们可以看下该类的一些重点变量： 1234567891011121314151617181920static final class Node { /** 表示共享模式下等待的Node */ static final Node SHARED = new Node(); /** 表示独占模式下等待的mode */ static final Node EXCLUSIVE = null; /** 下面几个为waitStatus的具体值 */ static final int CANCELLED = 1; static final int SIGNAL = -1; static final int CONDITION = -2; static final int PROPAGATE = -3; volatile int waitStatus; /** 表示前面的结点 */ volatile Node prev; /** 表示后面的结点 */ volatile Node next; /**当前结点装载的线程，初始化时被创建，使用后会置空*/ volatile Thread thread; /**链接到下一个节点的等待条件，用到Condition的时候会使用到*/ Node nextWaiter;} 代码里面定义了一个表示当前Node结点等待状态的字段waitStatus，该字段的取值包含了CANCELLED(1)、SIGNAL(-1)、CONDITION(-2)、PROPAGATE(-3)、0，这五个值代表了不同的特定场景： CANCELLED：表示当前结点已取消调度。当timeout或被中断（响应中断的情况下），会触发变更为此状态，进入该状态后的结点将不会再变化。 SIGNAL：表示后继结点在等待当前结点唤醒。后继结点入队时，会将前继结点的状态更新为SIGNAL（记住这个-1的值，因为后面我们讲的时候经常会提到） CONDITION：表示结点等待在Condition上，当其他线程调用了Condition的SIGNAL()方法后，CONDITION状态的结点将从等待队列转移到同步队列中，等待获取同步锁。(注：Condition是AQS的一个组件，后面会细说) PROPAGATE：共享模式下，前继结点不仅会唤醒其后继结点，同时也可能会唤醒后继的后继结点。 0：新结点入队时的默认状态。 也就是说，当waitStatus为负值表示结点处于有效等待状态，为正值的时候表示结点已被取消。 在AQS内部中还维护了两个Node对象head和tail，一开始默认都为null 12private transient volatile Node head; private transient volatile Node tail; 讲完了AQS的一些基础定义，我们就可以开始学习同步的具体运行机制了，为了更好的演示，我们用ReentrantLock作为使用入口，一步步跟进源码探究AQS底层是如何运作的，这里说明一下，因为ReentrantLock底层调用的AQS是独占模式，所以下文讲解的AQS源码也是针对独占模式的操作 独占模式 ReentrantLock和synchronized功能类似，使用AQS的独占模式，只有一个线程可以获取锁。 加锁过程我们都知道，ReentrantLock的加锁和解锁方法分别为lock()和unLock()，我们先来看获取锁的方法， 123456final void lock(){ if(compareAndSetState(0,1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); } 其中compareAndSetState(0, 1)如果返回true就代表着之前state是0，也就是当前无线程获取锁，同时当前线程获取锁成功了，将独占线程设置为当前线程。 如果是false就代表当前有线程占用，当前占用的线程有2个可能 当前线程在占用，因为是可重入锁，之后同样会获取锁 其他线程在占用，在其他线程占用期间，当前线程需要等待 逻辑很简单，线程进来后直接利用CAS尝试抢占锁，如果抢占成功state值回被改为1，且设置对象独占锁线程为当前线程，否则就调用acquire(1)再次尝试获取锁。 我们假定有两个线程A和B同时竞争锁，A进来先抢占到锁，此时的AQS模型图就类似这样： acquire acquire是一种以独占方式获取资源，如果获取到资源，线程直接返回，否则进入等待队列，直到获取到资源为止，且整个过程忽略中断的影响。该方法是独占模式下线程获取共享资源的顶层入口。获取到资源后，线程就可以去执行其临界区代码了。 acquire方法是一种互斥模式，且忽略中断。该方法至少执行一次tryAcquire(int)方法，如果tryAcquire(int)方法返回true，则acquire直接返回，否则当前线程需要进入队列进行排队。 1234public final void acquire(int arg){ if(!tryAcquire(arg)&amp;&amp;acquireQueued(addWaiter(Node.EXCLUSIVE),arg)) selfInterrupt(); } acquire(1)包含整个获取锁，如果获取不到就等待的操作 acquire包含了几个函数的调用， tryAcquire：尝试直接获取锁，如果成功就直接返回； addWaiter：获取不到锁时,说明有其他线程目前正在占用锁, 将该线程加入等待队列FIFO的尾部，并标记为独占模式； acquireQueued：线程阻塞在等待队列中获取锁，一直获取到资源后才返回。如果在整个等待过程中被中断过，则返回true，否则返回false。 selfInterrupt：自我中断，就是既拿不到锁，又在等待时被中断了，线程就会进行自我中断selfInterrupt()，将中断补上。 我们一个个来看源码，并结合上面的两个线程来做场景分析。 tryAcquire 在tryAcquire(arg)中是尝试获取锁,是由ReentrantLock提供的,逻辑比较简单当前无线程占有锁时,即state为0时,获取锁 当前有线程占有锁,但当前占有锁的线程是当前线程时,因为ReentrantLock是可重入锁,获取锁,并把state+1 1234567891011121314151617181920212223protected final boolean tryAcquire(int acquires){ return nonfairTryAcquire(acquires); }final boolean nonfairTryAcquire(int acquires){final Thread current=Thread.currentThread(); int c=getState(); if(c==0){ if(compareAndSetState(0,acquires)){ setExclusiveOwnerThread(current); return true; } }else if(current==getExclusiveOwnerThread()){ //当前占有锁的线程是当前线程时,因为ReentrantLock是可重入锁,获取锁,并把state+1 int nextc=c+acquires; if(nextc&lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false; } 当线程B进来后，nonfairTryAcquire方法首先会获取state的值，如果为0，则正常获取该锁，不为0的话判断是否是当前线程占用了，是的话就累加state的值，这里的累加也是为了配合释放锁时候的次数，从而实现可重入锁的效果。 当然，因为之前锁已经被线程A占领了，所以这时候tryAcquire会返回false，继续下面的流程。 addWaiter获取不到锁时,说明有其他线程目前正在占用锁,将当前线程包装成节点放入同步队列 将该线程加入等待队列FIFO的尾部，并标记为独占模式； 123456789101112131415161718/** 先尝试快速入队，如果入队成功直接返回，如果失败（存在竞态）就使用cas反复入队直到成功为止 **/private Node addWaiter(Node mode){ Node node=new Node(Thread.currentThread(),mode); // Try the fast path of enq; backup to full enq on failure // //快速入队 Node pred=tail; if(pred!=null){ node.prev=pred; if(compareAndSetTail(pred,node)){ pred.next=node; return node; } } enq(node); return node; } 这段代码首先会创建一个和当前线程绑定的Node节点，Node为双向链表。此时等待队列中的tail指针为空，直接调用enq(node)方法将当前线程加入等待队列尾部，然后返回当前结点的前驱结点， enq用于将当前节点插入等待队列，如果队列为空，则初始化当前队列。整个过程以CAS自旋的方式进行，直到成功加入队尾为止。 1234567891011121314151617private Node enq(final Node node){ // CAS&quot;自旋&quot;，直到成功加入队尾 for(;;){ Node t=tail; if(t==null){ // 队列为空，初始化一个Node结点作为Head结点，并将tail结点也指向它 if(compareAndSetHead(new Node())) tail=head; }else{ // 把当前结点插入队列尾部 node.prev=t; if(compareAndSetTail(t,node)){ t.next=node; return t; } } } } 第一遍循环时，tail指针为空，初始化一个Node结点，并把head和tail结点都指向它，然后第二次循环进来之后，tail结点不为空了，就将当前的结点加入到tail结点后面，也就是这样： 如果此时有另一个线程C进来的话，发现锁已经被A拿走了，然后队列里已经有了线程B，那么线程C就只能乖乖排到线程B的后面去， 入队完成之后再判断一次当前是否有可能获得锁，也就是前一个节点是head的话， 前一个线程有可能已经释放了，再获取一次，如果获取成功，设置当前节点为头节点，整个获取过程完成。 acquireQueued接着解读方法，通过tryAcquire()和addWaiter()，我们的线程还是没有拿到资源，并且还被排到了队列的尾部，如果让你来设计的话，这个时候你会怎么处理线程呢？其实答案也很简单，能做的事无非两个： 1、循环让线程再抢资源。但仔细一推敲就知道不合理，因为如果有多个线程都参与的话，你抢我也抢只会降低系统性能 2、进入等待状态休息，直到其他线程彻底释放资源后唤醒自己，自己再拿到资源 毫无疑问，选择2更加靠谱，acquireQueued方法做的也是这样的处理： acquireQueued()用于队列中的线程自旋地以独占且不可中断的方式获取同步状态（acquire），直到拿到锁之后再返回。该方法的实现分成两部分：如果当前节点已经成为头结点，尝试获取锁（tryAcquire）成功，然后返回；否则检查当前节点是否应该被park，然后将该线程park并且检查当前线程是否被可以被中断。 12345678910111213141516171819202122 final boolean acquireQueued(final Node node,int arg){ boolean failed=true; try{ // 标记是否会被中断 boolean interrupted=false; // CAS自旋 for(;;){// 获取当前结点的前结点 final Node p=node.predecessor(); if(p==head&amp;&amp;tryAcquire(arg)){ setHead(node); p.next=null; // help GC failed=false; return interrupted; } if(shouldParkAfterFailedAcquire(p,node)&amp;&amp;parkAndCheckInterrupt()) interrupted=true; } }finally{ if(failed) // 获取锁失败，则将此线程对应的node的waitStatus改为CANCEL cancelAcquire(node); } } shouldParkAfterFailedAcquire(Node, Node)shouldParkAfterFailedAcquire方法通过对当前节点的前一个节点的状态进行判断，对当前节点做出不同的操作，至于每个Node的状态表示，可以参考接口文档。 12345678910111213141516171819 private static boolean shouldParkAfterFailedAcquire(Node pred,Node node){ int ws=pred.waitStatus; if(ws==Node.SIGNAL) // 前驱结点等待状态为&quot;SIGNAL&quot;，那么自己就可以安心等待被唤醒了 return true; if(ws&gt;0){ /* * 前驱结点被取消了，通过循环一直往前找，直到找到等待状态有效的结点(等待状态值小于等于0) ， * 然后排在他们的后边，至于那些被当前Node强制&quot;靠后&quot;的结点，因为已经被取消了，也没有引用链， * 就等着被GC了 */ do{ node.prev=pred=pred.prev; }while(pred.waitStatus&gt;0); pred.next=node; }else{ // 如果前驱正常，那就把前驱的状态设置成SIGNAL compareAndSetWaitStatus(pred,ws,Node.SIGNAL); } return false; }private final boolean parkAndCheckInterrupt(){ LockSupport.park(this); return Thread.interrupted(); } acquireQueued方法的流程是这样的： 1、CAS自旋，先判断当前传入的Node的前结点是否为head结点，是的话就尝试获取锁，获取锁成功的话就把当前结点置为head，之前的head置为null(方便GC)，然后返回 2、如果前驱结点不是head或者加锁失败的话，就调用 shouldParkAfterFailedAcquire，将前驱节点的waitStatus变为了SIGNAL=-1，最后执行 parkAndChecknIterrupt 方法，调用 LockSupport.park()挂起当前线程，parkAndCheckInterrupt在挂起线程后会判断线程是否被中断，如果被中断的话，就会重新跑acquireQueued方法的CAS自旋操作，直到获取资源。 ps：LockSupport.park方法会让当前线程进入waitting状态，在这种状态下，线程被唤醒的情况有两种，一是被unpark()，二是被interrupt()，所以，如果是第二种情况的话，需要返回被中断的标志，然后在acquire顶层方法的窗口那里自我中断补上 此时，因为线程A还未释放锁，所以线程B状态都是被挂起的， 到这里，加锁的流程就分析完了. 获取锁并等待的过程:当lock()执行的时候： 先快速获取锁，当前没有线程执行的时候直接获取锁 尝试获取锁，当没有线程执行或是当前线程占用锁，可以直接获取锁 将当前线程包装为node放入同步队列，设置为尾节点 前一个节点如果为头节点，再次尝试获取一次锁 将前一个有效节点设置为SIGNAL 然后阻塞直到被唤醒 为了方便你们更加清晰理解，我加多一张流程图 释放锁说完了加锁，我们来看看释放锁是怎么做的，AQS中释放锁的方法是release()，当调用该方法时会释放指定量的资源 (也就是锁) ，如果彻底释放了（即state=0）,它会唤醒等待队列里的其他线程来获取资源。 release当ReentrantLock进行释放锁操作时，调用的是AQS的release(1)操作 123456789public final boolean release(int arg){ if(tryRelease(arg)){ Node h=head; if(h!=null&amp;&amp;h.waitStatus!=0) unparkSuccessor(h); return true; } return false; } tryRelease代码上可以看出，核心的逻辑都在tryRelease方法中，该方法的作用是释放资源，AQS里该方法没有具体的实现，需要由自定义的同步器去实现，我们看下ReentrantLock代码中对应方法的源码： 在tryRelease(arg)中会将锁释放一次，如果当前state是1，且当前线程是正在占用的线程，释放锁成功，返回true，否则因为是可重入锁，释放一次可能还在占用，应一直释放直到state为0为止 123456789101112protected final boolean tryRelease(int releases){ int c=getState()-releases; if(Thread.currentThread()!=getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free=false; if(c==0){ free=true; setExclusiveOwnerThread(null); } setState(c); return free; } tryRelease方法会减去state对应的值，如果state为0，也就是已经彻底释放资源，就返回true，并且把独占的线程置为null，否则返回false。 此时AQS中的数据就会变成这样： unparkSuccessor完全释放资源后，当前线程要做的就是唤醒CLH队列中第一个在等待资源的线程，也就是head结点后面的线程，此时调用的方法是unparkSuccessor()， 然后优先找下一个节点，如果取消了就从尾节点开始找，找到最前面一个可用的节点 将其取消阻塞状态。 123456789101112131415161718 private void unparkSuccessor(Node node){int ws=node.waitStatus;if(ws&lt; 0) //将head结点的状态置为0 compareAndSetWaitStatus(node,ws,0);//找到下一个需要唤醒的结点s Node s=node.next;//如果为空或已取消 if(s==null||s.waitStatus&gt;0){s=null;// 从后向前，直到找到等待状态小于0的结点，前面说了，结点waitStatus小于0时才有效 for(Node t=tail;t!=null&amp;&amp;t!=node;t=t.prev)if(t.waitStatus&lt;=0)s=t;}// 找到有效的结点，直接唤醒 if(s!=null)LockSupport.unpark(s.thread);//唤醒} 方法的逻辑很简单，就是先将head的结点状态置为0，避免下面找结点的时候再找到head，然后找到队列中最前面的有效结点，然后唤醒，我们假设这个时候线程A已经释放锁，那么此时队列中排最前边竞争锁的线程B就会被唤醒。然后被唤醒的线程B就会尝试用CAS获取锁，回到acquireQueued方法的逻辑， 阻塞在acquireQueued的地方在唤醒之后开始继续执行，当前节点已经是最前面的一个可用（未取消）节点了,经过不断的for循环以及在shouldParkAfterFailedAcquire中不断向前寻找可用节点，因此这个被唤醒的节点一定可以使其之前的节点为head。然后获取锁成功。 但是此时节点会与新加入的节点竞争，也就是不公平锁的由来。 在公平锁中，在tryAcquire时会判断之前是否有等待的节点hasQueuedPredecessors(),如果有就不会再去获取锁了,因此能保证顺序执行。 12345678910111213for(;;){// 获取当前结点的前结点 final Node p=node.predecessor(); if(p==head&amp;&amp;tryAcquire(arg)){ setHead(node); p.next=null; // help GC failed=false; return interrupted; } if(shouldParkAfterFailedAcquire(p,node)&amp;&amp;parkAndCheckInterrupt()) interrupted=true; } 当线程B获取锁之后，会把当前结点赋值给head，然后原先的前驱结点 (也就是原来的head结点) 去掉引用链，方便回收，这样一来，线程B获取锁的整个过程就完成了，此时AQS的数据就会变成这样：到这里，我们已经分析完了AQS独占模式下加锁和释放锁的过程，也就是tryAccquire-&gt;tryRelease这一链条的逻辑，除此之外，AQS中还支持共享模式的同步，这种模式下关于锁的操作核心其实就是tryAcquireShared-&gt;tryReleaseShared这两个方法，我们可以简单看下 共享模式 ReentrantReadWriteLock是Java中读写锁的实现，写写互斥，读写互斥，读读共享。读写锁在内部分为读锁和写锁，因为我们要探索共享模式，因此更关注读锁。 获取锁AQS中，共享模式获取锁的顶层入口方法是acquireShared，该方法会获取指定数量的资源，成功的话就直接返回，失败的话就进入等待队列，直到获取资源， 1234public final void acquireShared(int arg){ if(tryAcquireShared(arg)&lt; 0) doAcquireShared(arg); } 该方法里包含了两个方法的调用， tryAcquireShared：尝试获取一定资源的锁，返回的值代表获取锁的状态。 doAcquireShared：进入等待队列，并循环尝试获取锁，直到成功。 tryAcquireSharedtryAcquireShared在AQS里没有实现，同样由自定义的同步器去完成具体的逻辑，像一些较为常见的并发工具Semaphore、CountDownLatch里就有对该方法的自定义实现，虽然实现的逻辑不同，但方法的作用是一样的，就是获取一定资源的资源，然后根据返回值判断是否还有剩余资源，从而决定下一步的操作。 返回值有三种定义： 负值代表获取失败； (当前有写锁，返回-1，即未获取共享锁，需要执行下一步doAcquireShared) 0代表获取成功，但没有剩余的资源，也就是state已经为0； 正值代表获取成功，而且state还有剩余，其他线程可以继续领取 当返回值小于0时，证明此次获取一定数量的锁失败了，然后就会走doAcquireShared方法 123456789101112131415protected final int tryAcquireShared(int unused){ Thread current=Thread.currentThread(); int c=getState(); if(exclusiveCount(c)!=0&amp;&amp; getExclusiveOwnerThread()!=current) return-1; int r=sharedCount(c); if(!readerShouldBlock()&amp;&amp; r&lt;MAX_COUNT &amp;&amp; compareAndSetState(c,c+SHARED_UNIT)){ //设置firstReader，计算数量，略 return 1; } return fullTryAcquireShared(current); } 设置共享锁需要修改state的数量，表示获取共享锁的线程的数量，当共享锁的获取存在竞争时，即compareAndSetState(c, c + SHARED_UNIT))可能设置失败，此时进入fullTryAcquireShared(current)进行获取共享锁的完整版操作。 也就是说共享锁获取时：如果当前没有独占锁在占用，AQS根据其实现类的tryAcquireShared来实现让一个共享锁直接获取到锁(可以直接执行)当有独占锁在占用是，让共享锁去等待直到独占锁解锁为止，也就是doAcquireShared(arg)的逻辑 doAcquireShared此方法的作用是将当前线程加入等待队列尾部休息，直到其他线程释放资源唤醒自己，自己成功拿到相应量的资源后才返回，这是它的源码： 12345678910111213141516171819202122232425262728293031323334 private void doAcquireShared(int arg){// 加入队列尾部final Node node=addWaiter(Node.SHARED); boolean failed=true; try{ boolean interrupted=false; // CAS自旋 for(;;){final Node p=node.predecessor(); // 判断前驱结点是否是head if(p==head){ // 尝试获取一定数量的锁 int r=tryAcquireShared(arg); if(r&gt;=0){ // 获取锁成功，而且还有剩余资源，就设置当前结点为head，并继续唤醒下一个线程 setHeadAndPropagate(node,r); // 让前驱结点去掉引用链，方便被GC p.next=null; // help GC if(interrupted) selfInterrupt(); failed=false; return; } } // 跟独占模式一样，改前驱结点waitStatus为-1，并且当前线程挂起，等待被唤醒 if(shouldParkAfterFailedAcquire(p,node)&amp;&amp;parkAndCheckInterrupt()) interrupted=true; } }finally{ if(failed) cancelAcquire(node); } } doAcquireShared(arg) 除了将线程封装成节点入队外还表达了3个思想： 什么时候该执行 什么时候该传播 什么时候该等待（阻塞） 其中入队、执行和等待的逻辑基本和独占锁一样， 入队：都是加入等待队列的末尾，成为tail节点； 执行：判断当前节点的前一个节点是不是头节点，如果是的话尝试获取锁，如果获取到了就执行； 等待：获取不到或前一个节点不是头节点就代表该线程需要暂时等待，直到被叫醒为止。设置前一个节点为SIGNAL状态，然后进入等待。 其中不同的就是共享锁的传播逻辑： 想象一下，当前有一个写锁正在占用，有多个读锁在等待，当写锁释放时，第二个线程也就是想要获取读锁的线程就可以获取锁了。获取到之后当前正在用的锁就是读锁了，那后面的读锁呢，因为读锁是共享的，后面的读锁应该也能够依次获取读锁，也就是读锁的传播机制。 1234567891011 private void setHeadAndPropagate(Node node,int propagate){Node h=head;// head指向自己 setHead(node);// 如果还有剩余量，继续唤醒下一个邻居线程 if(propagate&gt;0||h==null||h.waitStatus&lt; 0){Node s=node.next;if(s==null||s.isShared())doReleaseShared();}} 将当前的节点设置为头节点，判断如果是共享锁，执行doReleaseShared()，唤醒当前节点 123456789101112131415161718private void doReleaseShared(){for(;;){Node h=head;if(h!=null&amp;&amp;h!=tail){int ws=h.waitStatus;if(ws==Node.SIGNAL){if(!compareAndSetWaitStatus(h,Node.SIGNAL,0))continue;unparkSuccessor(h);}else if(ws==0&amp;&amp;!compareAndSetWaitStatus(h,0,Node.PROPAGATE))continue;}if(h==head)break;}} 当前节点唤醒之后doAcquireShared(int arg)会继续执行,因为之前的节点被设置为头节点,如果后续是获取共享锁的节点会继续执行setHeadAndPropagate,一直传播下去直到遇到获取独占锁的节点。 看到这里，你会不会一点熟悉的感觉，这个方法的逻辑怎么跟上面那个acquireQueued() 那么类似啊？对的，其实两个流程并没有太大的差别。只是doAcquireShared()比起独占模式下的获取锁上多了一步唤醒后继线程的操作，当获取完一定的资源后，发现还有剩余的资源，就继续唤醒下一个邻居线程，这才符合”共享”的思想嘛。 这里我们可以提出一个疑问，共享模式下，当前线程释放了一定数量的资源，但这部分资源满足不了下一个等待结点的需要的话，那么会怎么样？ 按照正常的思维，共享模式是可以多个线程同时执行的才对，所以，多个线程的情况下，如果老大释放完资源，但这部分资源满足不了老二，但能满足老三，那么老三就可以拿到资源。可事实是，从源码设计中可以看出，如果真的发生了这种情况，老三是拿不到资源的，因为等待队列是按顺序排列的，老二的资源需求量大，会把后面量小的老三以及老四、老五等都给卡住。从这一个角度来看，虽然AQS严格保证了顺序，但也降低了并发能力 接着往下说吧，唤醒下一个邻居线程的逻辑在doReleaseShared()中，我们放到下面的释放锁来解析。 共享锁的获取总结如下： 尝试获取共享锁，如果当前是共享锁或无锁，设置共享锁的state,获取锁 如果当前是写锁，进入等待流程 入队，加入等待队列的末尾，成为tail节点 判断当前节点的前一个节点是不是头节点，如果是的话尝试获取锁，如果获取到了就执行 获取不到或前一个节点不是头节点就代表该线程需要暂时等待，直到被叫醒为止。设置前一个节点为SIGNAL状态，然后进入等待 如果可以获取到锁，设置头节点并进入共享锁节点传播流程 释放锁共享模式释放锁的顶层方法是releaseShared，它会释放指定量的资源，如果成功释放且允许唤醒等待线程，它会唤醒等待队列里的其他线程来获取资源。下面是releaseShared()的源码： 1234567public final boolean releaseShared(int arg){ if(tryReleaseShared(arg)){ doReleaseShared(); return true; } return false; } 该方法同样包含两部分的逻辑： tryReleaseShared：释放资源。 doAcquireShared：唤醒后继结点。 跟tryAcquireShared方法一样，tryReleaseShared在AQS中没有具体的实现，由子同步器自己去定义，但功能都一样，就是释放一定数量的资源。 释放完资源后，线程不会马上就收工，而是唤醒等待队列里最前排的等待结点。 tryReleaseShared在tryReleaseShared(arg)，基本就是tryAcquireShared(int unused)的反向操作 将设置的HoldCounter减少，firstReader设置null，state减少,将tryAcquireShared(int unused)添加的状态全部反向还原回去 当共享锁全部释放完毕，返回true，否则返回false doAcquireShared唤醒后继结点的工作在doReleaseShared()方法中完成，我们可以看下它的源码： 123456789101112131415161718192021 private void doReleaseShared(){for(;;){// 获取等待队列中的head结点 Node h=head;if(h!=null&amp;&amp;h!=tail){int ws=h.waitStatus;// head结点waitStatus = -1,唤醒下一个结点对应的线程 if(ws==Node.SIGNAL){if(!compareAndSetWaitStatus(h,Node.SIGNAL,0))continue;// loop to recheck cases// 唤醒后继结点 unparkSuccessor(h);}else if(ws==0&amp;&amp;!compareAndSetWaitStatus(h,0,Node.PROPAGATE))continue;// loop on failed CAS }if(h==head)// loop if head changed break; }} 代码没什么特别的，就是如果等待队列head结点的waitStatus为-1的话，就直接唤醒后继结点，唤醒的方法unparkSuccessor()在上面已经讲过了，这里也没必要再复述。 总的来看，AQS共享模式的运作流程和独占模式很相似。 2. Condition介绍完了AQS的核心功能，我们再扩展一个知识点，在AQS中，除了提供独占/共享模式的加锁/解锁功能，它还对外提供了关于Condition的一些操作方法。 Condition是个接口，在jdk1.5版本后设计的，基本的方法就是await()和SIGNAL()方法，功能大概就对应Object的wait()和notify()，Condition必须要配合锁一起使用，因为对共享状态变量的访问发生在多线程环境下。一个Condition的实例必须与一个Lock绑定，因此Condition一般都是作为Lock的内部实现，AQS中就定义了一个类ConditionObject来实现了这个接口，那么它应该怎么用呢？我们可以简单写个demo来看下效果 123456789101112131415161718192021222324252627282930313233public class ConditionDemo { public static void main(String[] args) { ReentrantLock lock = new ReentrantLock(); Condition condition = lock.newCondition(); Thread tA = new Thread(() -&gt; { lock.lock(); try { System.out.println(&quot;线程A加锁成功&quot;); System.out.println(&quot;线程A执行await被挂起&quot;); condition.await(); System.out.println(&quot;线程A被唤醒成功&quot;); } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); System.out.println(&quot;线程A释放锁成功&quot;); } }); Thread tB = new Thread(() -&gt; { lock.lock(); try { System.out.println(&quot;线程B加锁成功&quot;); condition.SIGNAL(); System.out.println(&quot;线程B唤醒线程A&quot;); } finally { lock.unlock(); System.out.println(&quot;线程B释放锁成功&quot;); } }); tA.start(); tB.start(); }} 执行main函数后结果输出为： 1234567线程A加锁成功 线程A执行await被挂起 线程B加锁成功 线程B唤醒线程A 线程B释放锁成功 线程A被唤醒成功 线程A释放锁成功 代码执行的结果很容易理解，线程A先获取锁，然后调用await()方法挂起当前线程并释放锁，线程B这时候拿到锁，然后调用SIGNAL唤醒线程A。 毫无疑问，这两个方法让线程的状态发生了变化，我们仔细来研究一下， 翻看AQS的源码，我们会发现Condition中定义了两个属性firstWaiter和lastWaiter，前面说了，AQS中包含了一个FIFO的CLH等待队列，每个Conditon对象就包含这样一个等待队列，而这两个属性分别表示的是等待队列中的首尾结点， 1234/** First node of condition queue. */private transient Node firstWaiter;/** Last node of condition queue. */private transient Node lastWaiter; 注意：Condition当中的等待队列和AQS主体的同步等待队列是分开的，两个队列虽然结构体相同，但是作用域是分开的 await先看await()的源码： 12345678910111213141516171819202122 public final void await()throws InterruptedException{if(Thread.interrupted())throw new InterruptedException();// 将当前线程加入到等待队列中 Node node=addConditionWaiter();// 完全释放占有的资源，并返回资源数 int savedState=fullyRelease(node);int interruptMode=0;// 循环判断当前结点是不是在Condition的队列中，是的话挂起 while(!isOnSyncQueue(node)){LockSupport.park(this);if((interruptMode=checkInterruptWhileWaiting(node))!=0)break;}if(acquireQueued(node,savedState)&amp;&amp;interruptMode!=THROW_IE)interruptMode=REINTERRUPT;if(node.nextWaiter!=null)// clean up if cancelled unlinkCancelledWaiters();if(interruptMode!=0)reportInterruptAfterWait(interruptMode);} 当一个线程调用 Condition.await()方法，将会以当前线程构造结点，这个结点的waitStatus赋值为Node.CONDITION， 也就是-2，并将结点从尾部加入等待队列，然后尾部结点就会指向这个新增的结点， 123456789101112131415 private Node addConditionWaiter(){Node t=lastWaiter;// If lastWaiter is cancelled, clean out. if(t!=null&amp;&amp;t.waitStatus!=Node.CONDITION){unlinkCancelledWaiters();t=lastWaiter;}Node node=new Node(Thread.currentThread(),Node.CONDITION);if(t==null)firstWaiter=node;elset.nextWaiter=node;lastWaiter=node;return node;} 我们依然用上面的demo来演示，此时，线程A获取锁并调用Condition.await()方法后，AQS内部的数据结构会变成这样：在Condition队列中插入对应的结点后，线程A会释放所持有的资源，走到while循环那层逻辑， 12345while(!isOnSyncQueue(node)){ LockSupport.park(this); if((interruptMode=checkInterruptWhileWaiting(node))!=0) break; } isOnSyncQueue方法的会判断当前的线程节点是不是在同步队列中，这个时候此结点还在Condition队列中，所以该方法返回false，这样的话循环会一直持续下去，线程被挂起，等待被唤醒，此时，线程A的流程暂时停止了。 当线程A调用await()方法挂起的时候，线程B获取到了线程A释放的资源，然后执行SIGNAL()方法： SIGNAL1234567public final void SIGNAL(){ if(!isHeldExclusively()) throw new IllegalMonitorStateException(); Node first=firstWaiter; if(first!=null) doSIGNAL(first); } 先判断当前线程是否为获取锁的线程，如果不是则直接抛出异常。接着调用doSIGNAL()方法来唤醒线程。 12345678910111213141516171819 private void doSIGNAL(Node first){ // 循环，从队列一直往后找不为空的首结点 do{ if((firstWaiter=first.nextWaiter)==null) lastWaiter=null; first.nextWaiter=null; }while(!transferForSIGNAL(first)&amp;&amp;(first=firstWaiter)!=null); }final boolean transferForSIGNAL(Node node){ // CAS循环，将结点的waitStatus改为0 if(!compareAndSetWaitStatus(node,Node.CONDITION,0)) return false; // 上面已经分析过，此方法会把当前结点加入到等待队列中，并返回前驱结点 Node p=enq(node); int ws=p.waitStatus; if(ws&gt;0||!compareAndSetWaitStatus(p,ws,Node.SIGNAL)) LockSupport.unpark(node.thread); return true; } 从doSIGNAL的代码中可以看出，这时候程序寻找的是Condition等待队列中首结点firstWaiter的结点，此时该结点指向的是线程A的结点，所以之后的流程作用的都是线程A的结点。 这里分析下transferForSIGNAL方法，先通过CAS自旋将结点waitStatus改为0，然后就把结点放入到同步队列 (此队列不是Condition的等待队列) 中，然后再用CAS将同步队列中该结点的前驱结点waitStatus改为Node.SIGNAL，也就是-1，此时AQS的数据结构大概如下(额…..少画了个箭头，大家就当head结点是线程A结点的前驱结点就好)：回到await()方法，当线程A的结点被加入同步队列中时，isOnSyncQueue()会返回true，跳出循环， 123456789101112 while(!isOnSyncQueue(node)){LockSupport.park(this);if((interruptMode=checkInterruptWhileWaiting(node))!=0)break;}if(acquireQueued(node,savedState)&amp;&amp;interruptMode!=THROW_IE)interruptMode=REINTERRUPT;if(node.nextWaiter!=null)// clean up if cancelled unlinkCancelledWaiters();if(interruptMode!=0)reportInterruptAfterWait(interruptMode); 接着执行acquireQueued()方法，这里就不用多说了吧，尝试重新获取锁，如果获取锁失败继续会被挂起，直到另外线程释放锁才被唤醒。 所以，当线程B释放完锁后，线程A被唤醒，继续尝试获取锁，至此流程结束。 对于这整个通信过程，我们可以画一张流程图展示下： 总结说完了Condition的使用和底层运行机制，我们再来总结下它跟普通 wait/notify 的比较，一般这也是问的比较多的，Condition大概有以下两点优势： Condition 需要结合 Lock 进行控制，使用的时候要注意一定要对应的unlock()，可以对多个不同条件进行控制，只要new 多个 Condition对象就可以为多个线程控制通信，wait/notify 只能和 synchronized 关键字一起使用，并且只能唤醒一个或者全部的等待队列； Condition 有类似于 await 的机制，因此不会产生加锁方式而产生的死锁出现，同时底层实现的是 park/unpark 的机制，因此也不会产生先唤醒再挂起的死锁，一句话就是不会产生死锁，但是 wait/notify 会产生先唤醒再挂起的死锁。 无论是独占还是共享模式，或者结合是Condition工具使用，AQS本质上的同步功能都是通过对锁和队列中结点的操作来实现的， 3. ReentrantLockReentrantLock意思为可重入锁，指的是一个线程能够对一个临界资源重复加锁。为了帮助大家更好地理解ReentrantLock的特性，我们先将ReentrantLock跟常用的Synchronized进行比较，其特性如下：下面通过伪代码，进行更加直观的比较： 1234567891011121314151617181920212223242526272829// **************************Synchronized的使用方式**************************// 1.用于代码块synchronized (this){}// 2.用于对象synchronized (object){}// 3.用于方法public synchronized void test(){} // 4.可重入 for(int i=0;i&lt; 100;i++){synchronized (this){} }// **************************ReentrantLock的使用方式**************************public void test()throw Exception{ // 1.初始化选择公平锁、非公平锁 ReentrantLock lock=new ReentrantLock(true); // 2.可用于代码块 lock.lock(); try{ try{ // 3.支持多种加锁方式，比较灵活; 具有可重入特性 if(lock.tryLock(100,TimeUnit.MILLISECONDS)){} }finally{ // 4.手动释放锁 lock.unlock() } }finally{ lock.unlock(); } } ReentrantLock 最基本的使用方式123456789101112class X { private final ReentrantLock lock = new ReentrantLock(); public void m() { lock.lock(); try { doSomething(); } finally { lock.unlock() } }} 当创建ReentrantLock时默认使用非公平锁，效率高于公平锁，暂不讨论公平锁。 ReentrantReadWriteLock的读锁的最基本的使用方式如下123456789101112class X { private final ReentrantReadWriteLock rwl = new ReentrantReadWriteLock(); public void m() { rwl.readLock().lock(); try { read(); } finally { rwl.readLock().unlock(); } }} 2. synchronizesynchronized可以保证方法或者代码块在运行时，同一时刻只有一个方法可以进入到临界区，同时它还可以保证共享变量的内存可见性。Synchronized主要有以下三个作用：保证互斥性、保证可见性、保证顺序性。 synchronize与lock的区别参考文章 图文并茂：AQS 是怎么运行的？ AQS 简介 Java技术之AQS详解 Java并发之AQS详解 从ReentrantLock的实现看AQS的原理及应用","link":"/2021/09/23/java/java_aqs/"}],"tags":[{"name":"git","slug":"git","link":"/tags/git/"},{"name":"centos","slug":"centos","link":"/tags/centos/"},{"name":"jdk","slug":"jdk","link":"/tags/jdk/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"拦截&#x2F;抓包工具","slug":"拦截-抓包工具","link":"/tags/%E6%8B%A6%E6%88%AA-%E6%8A%93%E5%8C%85%E5%B7%A5%E5%85%B7/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"flink","slug":"flink","link":"/tags/flink/"},{"name":"gc","slug":"gc","link":"/tags/gc/"},{"name":"数据结构","slug":"数据结构","link":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"Spring","slug":"Spring","link":"/tags/Spring/"},{"name":"opencv","slug":"opencv","link":"/tags/opencv/"},{"name":"spring","slug":"spring","link":"/tags/spring/"},{"name":"wsl","slug":"wsl","link":"/tags/wsl/"},{"name":"zookeeper","slug":"zookeeper","link":"/tags/zookeeper/"},{"name":"jvm","slug":"jvm","link":"/tags/jvm/"},{"name":"并发","slug":"并发","link":"/tags/%E5%B9%B6%E5%8F%91/"},{"name":"java锁","slug":"java锁","link":"/tags/java%E9%94%81/"}],"categories":[{"name":"git","slug":"git","link":"/categories/git/"},{"name":"centos","slug":"centos","link":"/categories/centos/"},{"name":"jdk","slug":"jdk","link":"/categories/jdk/"},{"name":"mysql","slug":"mysql","link":"/categories/mysql/"},{"name":"算法","slug":"算法","link":"/categories/%E7%AE%97%E6%B3%95/"},{"name":"nginx","slug":"nginx","link":"/categories/nginx/"},{"name":"java","slug":"java","link":"/categories/java/"},{"name":"docker","slug":"docker","link":"/categories/docker/"},{"name":"拦截&#x2F;抓包工具","slug":"拦截-抓包工具","link":"/categories/%E6%8B%A6%E6%88%AA-%E6%8A%93%E5%8C%85%E5%B7%A5%E5%85%B7/"},{"name":"hexo","slug":"hexo","link":"/categories/hexo/"},{"name":"flink","slug":"flink","link":"/categories/flink/"},{"name":"gc","slug":"gc","link":"/categories/gc/"},{"name":"数据结构","slug":"数据结构","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"linux","slug":"linux","link":"/categories/linux/"},{"name":"Spring","slug":"Spring","link":"/categories/Spring/"},{"name":"opencv","slug":"opencv","link":"/categories/opencv/"},{"name":"spring","slug":"spring","link":"/categories/spring/"},{"name":"wsl","slug":"wsl","link":"/categories/wsl/"},{"name":"zookeeper","slug":"zookeeper","link":"/categories/zookeeper/"},{"name":"jvm","slug":"jvm","link":"/categories/jvm/"}]}